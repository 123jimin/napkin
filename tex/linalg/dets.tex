\chapter{Trace and Determinant}
You may have learned in high school that given a matrix
\[ \left(
	\begin{array}{cc}
		a & c \\
		b & d
	\end{array}
	\right)
\]
the trace is the sum of the diagonals $a+d$ and the determinant is $ad-bc$.
But we know that a matrix is somehow just encoding a linear map using a choice of basis.
Why would these random formulas somehow not depend on the choice of a basis?

The goal of this chapter is to give the basis-free definition of a determinant and trace of a matrix: that is, we're going to define $\det T$ and $\Tr T$ for $T : V \to V$ without making reference to the encoding for $T$.
This will make it obvious the determinant and trace of a matrix do not depend on the choice of basis,
and that several properties are vacuously true (e.g.\ that the determinant is multiplicative).

Along the way, we'll meet the tensor product, the dual module, and the wedge product. Fair warning that this makes this one of the harder chapters.

\section{Tensor Product}
Note that $\dim (V \oplus W) = \dim V + \dim W$, even though as sets $V \oplus W$ looks like $V \times W$.
What if we wanted a real ``product'' of spaces,
with multiplication of dimensions?

For example, let's pull out (for hopefully the last time)
my favorite example of a real vector space, namely
\[ V = \left\{ ax^2 + bx + c \mid a,b,c \in \RR \right\}. \]
Here's another space, a little smaller:
\[ W = \left\{ dy + e \mid d,e \in \RR \right\}. \]
If we take the direct sum, then we would get some rather unnatural
vector space of dimension five
(whose elements can be thought of as pairs $(ax^2+bx+c,dy+e)$).
But suppose we want a vector space
whose elements are \emph{products} of polynomials in $V$ and $W$;
it would contain elements like $4x^2y + 5xy + y + 3$.
In particular, the basis would be
\[ \left\{ x^2y, x^2, xy, x, y, 1 \right\} \]
and thus have dimension six.

For this we resort to the \emph{tensor product}.
It does exactly this, except that the ``multiplication''
is done by a scary\footnote{%
	Seriously, $\otimes$ looks \emph{terrifying} to non-mathematicians,
	and even to many math undergraduates.}
symbol $\otimes$:
think of it as a ``wall'' that separates the elements
between the two vector spaces.
For example, the above example might be written as
\[ 4x^2 \otimes y + 5x \otimes y + 1 \otimes y + 3 \otimes 1. \]
But of course there should be no distinction between writing $4x^2 \otimes y$ and $x^2 \otimes 4y$ or even $2x^2 \otimes 2y$.
While we want to keep the $x$ and $y$ separate,
the scalars should be free to float around.

Of course, there's no need to do everything in terms of just the monomials.
We are free to write
\[ (x + 1) \otimes (y + 1). \]
If you like, you can expand this as
\[ x \otimes y + 1 \otimes y + x \otimes 1 + 1 \otimes 1. \]
Same thing.
The point is that we can take any two of our polynomials
and artificially ``tensor'' them together.


The definition of the tensor product does exactly this,
and nothing else.
I'll only define this for vector spaces for simplicity.
The definition for modules over a commutative ring $R$ is exactly the same.
\begin{definition}
	Let $V$ and $W$ be $k$-vector spaces.
	The \vocab{tensor product} $V \otimes_k W$ is the abelian group
	generated by elements of the form $v \otimes w$,
	subject to the following relations.
	\begin{align*}
		(v_1 + v_2) \otimes w &= v_1 \otimes w + v_2 \otimes w \\
		v \otimes (w_1 + w_2) &= v \otimes w_1 + v \otimes w_2 \\
		(c \cdot v) \otimes w &= v \otimes (c \cdot w).
	\end{align*}
	As a vector space,
	its action is given by
	$c \cdot (v \otimes w) = (c \cdot v) \otimes w = v \otimes (c \cdot w)$.
\end{definition}
Here's another way to phrase the same idea.
We define a \vocab{simple tensor} as an
element of the form $v \otimes w$ for $v \in V$ and $w \in W$.
But we let the $\otimes$ wall be ``permeable'' in the sense that
\[ (c \cdot v) \otimes w = v \otimes (c \cdot w) = c \cdot (v \otimes w) \]
and we let multiplication and addition distribute as we expect.
Finally, the elements of $V \otimes W$ are not the simple tensors,
but in fact \emph{sums} of these simple tensors!

As the example we gave suggested,
the basis of $V \otimes W$ is literally the ``product''
of the bases of $V$ and $W$.
In particular, this fulfills our desire that $\dim V \otimes W
= \dim V \cdot \dim W$.
\begin{proposition}
	Let $V$ and $W$ be finite-dimensional $k$-vector spaces.
	If $e_1, \dots, e_m$ is a basis of $V$ and $f_1, \dots, f_n$ is a basis of $W$,
	then the basis of $V \otimes_k W$
	is precisely $e_i \otimes f_j$, where $i=1,\dots,m$ and $j=1,\dots,n$.
\end{proposition}
\begin{proof}
	Boring and omitted.
\end{proof}

%\begin{example}[Explicit Computation]
%	Let $V$ have basis $e_1$, $e_2$ and $W$ have basis $f_1, f_2$.
%	Let $v = 3e_1 + 4e_2 \in V$ and $w = 5f_1 + 6f_2 \in W$.
%	Let's write $v \otimes w$ in this basis for $V \otimes_k W$.
%	\begin{align*}
%		v \otimes w &= (3e_1+4e_2) \otimes (5f_1+6f_2) \\
%		&= (3e_1) \otimes (5f_1) +  (4e_2) \otimes (5f_1)
%		+ (3e_1) \otimes (6f_2) + (4e_2) \otimes (6f_2) \\
%		&= 15 (e_1 \otimes f_1) + 20(e_2 \otimes f_1)
%		+ 18 (e_1 \otimes f_2) + 24(e_2 \otimes f_2).
%	\end{align*}
%\end{example}
%
%So in this way, you can see why tensor products are a nice ``product'' to consider if we're really interested in $V \times W$ in a way that's more intimate than just a direct sum.

\begin{abuse}
	We'll almost always abbreviate $\otimes_k$ to just $\otimes$.
\end{abuse}

\begin{remark}
	Observe that to define a linear map $V \otimes W \to X$,
	I only have to say what happens to each simple tensor $v \otimes w$,
	since the simple tensors \emph{generate} $V \otimes W$.
	But again, keep in mind that $V \otimes W$ consists of \emph{sums}
	of these simple tensors!
\end{remark}

\section{Dual Module}
\prototype{Rotate a column matrix by $90$ degrees.}

Consider the following vector space:
\begin{example}
	[Functions from $\RR^3 \to \RR$]
	The set of real functions $f(x,y,z)$ is an
	infinite-dimensional real vector space.
	Indeed, we can add two functions to get $f+g$,
	and we can think of functions like $2f$.
\end{example}
This is a terrifyingly large vector space,
but you can do some reasonable reductions.
For example, you can restrict your attention to just
the \emph{linear maps} from $\RR^3$ to $\RR$.

That's exactly what we're about to do.
The following definition might seem strange at first, but bear with me.

\begin{definition}
	Let $V$ be a $k$-vector space.
	Then $V^\vee$, the \vocab{dual space} of $V$, is defined
	as the vector space whose elements are \emph{linear maps from $V$ to $k$}.
\end{definition}
The addition and multiplication are pointwise:
it's the same notation we use when we write $cf+g$ to mean $c \cdot f(x) + g(x)$.
The dual space itself is less easy to think about.

Let's try to find a basis for $V^\vee$.
First, here is a very concrete interpretation of the vector space.
\begin{example}
	[Dual Spaces as Row Matrices]
	Suppose $V = \RR^3$.
	Then we can think of elements of $V$ as column matrices:
	\[ v = \left(
		\begin{array}{c}
			2 \\ 5 \\ 9
		\end{array}
		\right) \in V. \]
	Then a linear map $f : V \to k$ can be interpreted as a \emph{row matrix}:
	\[
		f = \left(
		\begin{array}{ccc}
			3 & 4 & 5
		\end{array}
		\right) \in V^\vee. \]
	Then
	\[
		f(v)
		= \left(
		\begin{array}{ccc}
			3 & 4 & 5
		\end{array}
		\right)
		\cdot
		\left(
		\begin{array}{c}
			2 \\ 5 \\ 9
		\end{array}
		\right)
		= 71. \]
\end{example}
More precisely: to specify a linear map $V \to k$,
I only have to tell you where each basis element of $V$ goes.
In the above example, $f$ sends $e_1$ to $3$, $e_2$ to $4$, and $e_3$ to $5$.
So $f$ sends \[ 2e_1 + 5e_2 + 9e_3 \mapsto 2 \cdot 3 + 5 \cdot 4 + 9 \cdot 5 = 71. \]

Let's make all this precise.
\begin{proposition}
	Let $V$ be a finite-dimensional vector space with basis $e_1, \dots, e_n$.
	For each $i$ consider the function $e_i^\vee : V \to k$ by
	\[
		e_i^\vee(e_j)
		= \begin{cases}
			1 & i=j \\
			0 & i \neq j.
		\end{cases}
	\]
	In more humane terms, $e_i^\vee(v)$ gives the coefficient of $e_i$ in $v$.

	Then $e_1^\vee$, $e_2^\vee$, \dots, $e_n^\vee$ is a basis of $V^\vee$.
\end{proposition}
I want to emphasize that this choice of basis for $V^\vee$ is absolutely not canonical, and depends very heavily on which basis of $V$ is originally selected.
It is not ``God-given'', in other words.

\begin{example}
	In this notation, $f = 3e_1^\vee + 4e_2^\vee + 5e_3^\vee$.
	Do you see why the ``sum'' notation works as expected here?
	Indeed
	\begin{align*}
		f(e_1) &= (3e_1^\vee + 4e_2^\vee + 5e_3^\vee)(e_1) \\
		&= 3e_1^\vee(e_1) + 4e_2^\vee(e_1) + 5e_3^\vee(e_1) \\
		&= 3 \cdot 1 + 4 \cdot 0 + 5 \cdot 0 = 3.
	\end{align*}
	That's exactly what we wanted.
\end{example}

Also, you might be inclined to point out that $V \cong V^\vee$ at this point,
since there's an obvious isomorphism $e_i \mapsto e_i^\vee$.
You might call it ``rotating the column matrix by $90\dg$''.
I want to point out again that
\begin{itemize}
	\ii This isomorphism is absolutely not canonical,
	and depends very much on which basis you choose.
	In other words, if I pick a different basis, then the isomorphism
	will be completely different.

	It \emph{is} true that $V$ and $V^\vee$ are isomorphic,
	but the fact is that any two vector spaces of the same dimension are isomorphic, so the fact that $V \simeq V^\vee$ is not especially impressive.
	
	\ii If $V$ is infinite-dimensional it is no longer necessarily true that $V \simeq V^\vee$.
\end{itemize}

\section{The Trace}
The point of introducing both the tensor product and the dual module is that together
we can use them to get an algebraic way of thinking about ``matrices''.
Here's the intuition.
If $V$ is three-dimensional and $W$ is five-dimensional, then we can think
of the maps $V \to W$ as a $5 \times 3$ array of numbers.
We want to think of these maps as a vector space:
(since one can add or scale matrices).
So it had better be a vector space with dimension $15$,
but just saying ``$k^{\oplus 15}$'' is not really that satisfying
(what is the basis?).
But we're about to show that we \emph{can} do this in a nice, general fashion:
the vector space \[ V^\vee \otimes W \]
which somehow is a product of our target space,
and the space of ``maps out of $V$''.

How does this work? For concreteness, suppose $V$ has a basis $e_1$, $e_2$, $e_3$,
and $W$ has a basis $f_1$, $f_2$, $f_3$, $f_4$, $f_5$..
Consider an element of $V^\vee \otimes W$, say
\[ e_1^\vee \otimes (f_2 + 2f_4) + 4e_2^\vee \otimes f_5. \]
We want to interpret this element as a function $V \to W$: given a $v \in V$,
we want to return an element of $W$.
How?
There's really only one way to proceed:
feed in $v \in V$ into the $V^\vee$ guys on the left.
That is, take the map
\[ v \mapsto e_1^\vee(v) \cdot (f_2 + 2f_4) + 4e_2^\vee(v) \cdot f_5 \in W. \]
So, there's a natural way to interpret any element of $V^\vee \otimes W$
as a linear map $V \to W$.
Our goal is to show that every linear map can be interpreted this way.

First, for notational convenience,
\begin{definition}
	Let $\Hom(V,W)$ be the set of linear maps from $V$ to $W$
	(which one can interpret as matrices which send $V$ to $W$),
	viewed as a vector space over $k$.
	Let $\End(V) = \Hom(V,V)$.
\end{definition}
In this notation, $V^\vee \defeq \Hom(V,k)$.

We can now write down something that's more true generally.
\begin{theorem}
	Let $V$ and $W$ be finite-dimensional vector spaces.
	We described a map
	\[ V^\vee \otimes W \to \Hom(V,W) \]
	by sending $\xi \otimes w$ to the function $v \mapsto \xi(v) \cdot w$.
	This map is an isomorphism.
\end{theorem}
This looks intimidating, but there's no content.
Think of it this way: suppose $V$ has basis $e_1$, $e_2$, $e_3$.
Suppose $T(e_1) = w_1$, $T(e_2) = w_2$ and $T(e_3) = w_3$.
In that case, we can think of $T$ as represented by
\[ e_1^\vee \otimes w_1 + e_2^\vee \otimes w_2 + e_3^\vee \otimes w_3. \]
This element of $V^\vee \otimes W$ is contrived to agree with $T$ on the basis elements $e_i$.
The beauty of this is that we don't actually have to refer to the basis at all to state the theorem.
\begin{proof}
	Let $e_1, \dots, e_m$ and $f_1, \dots, f_n$ be a basis of $V$ and $W$.
	Then let $e_1^\vee, \dots, e_m^\vee$ be the dual basis of $V$.
	Hence $e_i^\vee \otimes f_j$ is a basis of $V^\vee \otimes W$.

	Then $e_i^\vee \otimes f_j$ corresponds to the matrix
	which has $1$ in the $(i,j)$th component and zero everywhere else
	(can you see why?).
	With this the assertion is obvious,
	since the set of $m \times n$ matrices has this as an obvious basis.
\end{proof}

So there is a \textbf{natural isomorphism} from $V^\vee \otimes W \simeq \Hom(V,W)$.
While we did use a basis in the \emph{proof}, this doesn't change the 
fact that the isomorphism is ``God-given'', depending only on the spirit
of $V$ and $W$ itself and not which basis we choose to express the vector spaces in.

The above is perhaps a bit dense, so here is a concrete example.
\begin{example}[Explicit Example]
	Let $V = \RR^2$ and suppose we've picked a basis $e_1$, $e_2$ of $V$.
	Then let $T : V \to V$ by
	\[ T = \left(
		\begin{array}{cc}
			1 & 2 \\ 3 & 4
		\end{array}
		\right). \]
	Then the isomorphism sends $T$ to the following (pre)-image in
	$V^\vee \otimes V$:
	\[ T \mapsto e_1^\vee \otimes e_1 + 2e_1^\vee \otimes e_2
		+ 3e_2^\vee \otimes e_1 + 4e_2^\vee \otimes e_2. \]
	The beauty of the basis-free definition is that even if we change the basis,
	the above expression will look completely different, but
	the actual element in $V^\vee \otimes V$ doesn't change.
\end{example}


We are now ready to give the definition of a trace.
Recall that a square matrix $T$ can be thought of as a map $T : V \to V$.
According to the above theorem there is a map
\[ \End(V) \simeq V^\vee \otimes V \]
so every map $V \to V$ can be thought of as an element of $V^\vee \otimes V$.
But we can also define an
\emph{evaluation map} $\opname{ev} : V^\vee \otimes V \to k$
by ``collapsing'' each simple tensor: $f \otimes v \mapsto f(v)$.
So this gives us a composed map
\[ \End(V) \taking\simeq V^\vee \otimes V \taking{\opname{ev}} k. \]
This result is called the \vocab{trace} of a matrix $T$.

\begin{example}[Example of a Trace]
	Continuing the previous example,
	\[ \Tr T = e_1^\vee(e_1) + 2e_1^\vee(e_2) 
		+ 3e_2^\vee(e_1)  +4e_2^\vee(e_2)
		= 1 + 0 + 0 + 4 = 5. \]
	And that is why the trace is the sum of the diagonal entries.
\end{example}

\section{(Optional) Two more digressions on dual spaces}
\begin{itemize}
	\ii Suppose $T : V \to W$ is a linear map.
	Then we actually get a map $T^\vee : W^\vee \to V^\vee$,
	called the \vocab{dual map}, as follows:
	$T^\vee(f) = f \circ T$.
	In the concrete interpretation, if $T$ is a matrix, then
	$T$ multiplies column vectors in $V$ on the left to get column vectors in $W$.
	Then $T^\vee$ is the same matrix, but it multiplies
	row vectors in $W^\vee$ \emph{on the right} to get row vectors in $V$.
	Here is an explicit example: if $V = \RR^3$ and $W = \RR^2$ then
	\[
		\left(
		\begin{array}{ccc}
			1 & 2 & 3 \\
			4 & 5 & 6
		\end{array}
		\right)
		\left(
		\begin{array}{c}
			v_1 \\ v_2 \\ v_3 
		\end{array}
		\right)
		= \left(
		\begin{array}{c}
			w_1 \\ w_2
		\end{array}
		\right) 
		\text{ and }
		\left(
		\begin{array}{cc}
			w_1' & w_2'
		\end{array}
		\right)
		\left(
		\begin{array}{ccc}
			1 & 2 & 3 \\
			4 & 5 & 6
		\end{array}
		\right)
		=
		\left(
		\begin{array}{ccc}
			v_1' & v_2' & v_3'
		\end{array}
		\right).
	\]
	But once again, the definition $T^\vee(f) = f \circ T$
	has the nice property that it doesn't depend on a choice of a basis.
	That shows that $T^\vee$ really is well defined, depending only on $T : V \to W$.
	And in particular, this definition works even if $V$ or $W$
	is infinite-dimensional.

	\ii Why is the dual so named?
	We'll show that the dual of the dual (i.e.\ $(V^\vee)^\vee$)
	is somehow exactly the same as $V$.
	It's of course true that they are isomorphic, but somehow that's
	not very impressive (any vector spaces of the same dimension are isomorphic).
	But we will show that there is a very natural isomorphism between them;
	this will be more meaningful.

	Let $V$ be a finite-dimensional vector space.
	For every $v \in V$, we define the map $\opname{ev}_v : V^\vee \to k$
	called ``evaluation at $v$'': it takes a function $f \in V^\vee$,
	and since $f : V \to k$, we can return the value $f(v)$.
	Succinctly, $\opname{ev}_v(f) = f(v)$.

	Since $\opname{ev}_v : V^\vee \to k$, we see that
	$\opname{ev}_v \in (V^\vee)^\vee$.
	In fact, you can show that the map $v \mapsto \opname{ev}_v$
	is an isomorphism from $V$ to $(V^\vee)^\vee$ for dimension reasons.
	So unlike the situation $V \simeq V^\vee$, there
	is a \emph{canonical} isomorphism \[ V \simeq (V^\vee)^\vee \]
	that doesn't depend at all on what $V$ looks like,
	or on an arbitrary choice of basis.
	In other words, the structures of $V$ and $(V^\vee)^\vee$
	really are the same at heart.
\end{itemize}



\section{Wedge Product}
\prototype{$\Lambda^2(\RR^2)$.}
We're now going to define something called the wedge product.
It will look a ton like the tensor product, but we'll have one extra relation.

For simplicity, I'll first define the wedge product $\Lambda^2(V)$.
But we will later replace $2$ with any $n$.

\begin{definition}
	Let $V$ be a $k$-vector space.
	The $2$-wedge product $\Lambda^2(V)$ is the abelian group
	generated by elements of the form $v \wedge w$ (where $v,w \in V$),
	subject to the following relations:
	\begin{align*}
		(v_1 + v_2) \wedge w &= v_1 \wedge w + v_2 \wedge w \\
		v \wedge (w_1 + w_2) &= v \wedge w_1 + v \wedge w_2 \\
		(c \cdot v) \wedge w &= v \wedge (c \cdot w) \\
		v \wedge v &= 0.
	\end{align*}
	As a vector space, its action is given by
	$c \cdot (v \wedge w) = (c \cdot v) \wedge w = v \wedge (c \cdot w)$.
\end{definition}
\begin{exercise}
	Use these properties to show that $v \wedge w = - (w \wedge v)$
	for any $v,w \in V$.
\end{exercise}

This looks almost exactly the same as the definition for a tensor product,
with two subtle differences.
The first is that we only have $V$ now, rather than $V$ and $W$.
Secondly, there is a new \emph{mysterious} relation $v \wedge v = 0$,
which implies that
\[ v \wedge w = - (w \wedge v). \]
What's that doing there?
It seems kind of weird.

I'll give you a hint.
\begin{example}
	[Wedge Product Explicit Compuation]
	Let $V = \RR^2$, and let $v = ae_1 + be_2$, $w = ce_1 + de_2$.
	Now let's compute $v \wedge w$ in $\Lambda^2(V)$.
	\begin{align*}
		v \wedge w &= (ae_1 + be_2) \wedge (ce_1 + de_2) \\
		&= ac (e_1 \wedge e_1) + bd (e_2 \wedge e_2)
		+ ad (e_1 \wedge e_2) + bc (e_2 \wedge e_1) \\
		&= ad (e_1 \wedge e_2) + bc (e_2 \wedge e_1) \\
		&= (ad-bc) (e_1 \wedge e_2).
	\end{align*}
\end{example}

What is $ad-bc$? You might already recognize it:
\begin{itemize}
	\ii  You might know that the area of the parallelogram formed by $v$ and $w$ is $ad-bc$.
	\ii You might recognize it as the determinant of $\left(
		\begin{array}{cc}
			a & c \\
			b & d
		\end{array}
		\right)$.
	In fact, you might even know that the determinant is meant to interpret
	hypervolumes.
\end{itemize}
This is absolutely no coincidence.
The wedge product is designed to interpret signed areas.
That is, $v \wedge w$ is meant to interpret the area of the parallelogram
formed by $v$ and $w$.
You can see why the condition $(cv) \wedge w = v \wedge (cw)$ would make sense now.
And now of course you know why $v \wedge v$ ought to be zero:
it's an area zero parallelogram!

The \textbf{miracle of wedge products} is that the only additional condition
we need to add to the tensor product axioms is that $v \wedge w = -(w \wedge v)$,
Then suddenly, the wedge will do all our work of interpreting volumes for us.

\begin{ques}
	Let $V$ be a real finite-dimensional vector space.
	Convince yourself that if $e_1, \dots, e_m$ is a basis of $V$,
	then a basis of $\Lambda^2(V)$ is $e_i \wedge e_j$ where $i < j$.
	Hence $\Lambda^2(V)$ has dimension $\binom n2$.
\end{ques}

Now I have the courage to define a multi-dimensional wedge product.
It's just the same thing with more wedges.
\begin{definition}
	Let $V$ be a vector space and $m$ a positive integer.
	The space $\Lambda^m(V)$ is generated by wedges of the form
	\[ v_1 \wedge v_2 \wedge \dots \wedge v_m \]
	subject the following relations:
	\begin{align*}
		\dots \wedge (v_1+v_2) \wedge \dots
			&= (\dots \wedge v_1 \wedge \dots)
			 + (\dots \wedge v_2 \wedge \dots) \\
		\dots \wedge (cv_1) \wedge v_2 \wedge \dots
			&= \dots \wedge v_1 \wedge (cv_2) \wedge \dots  \\
		\dots \wedge v \wedge v \wedge \dots
			&= 0.
	\end{align*}
	As a vector space
	\[ c \cdot (v_1 \wedge v_2 \wedge \dots \wedge v_m)
	 = (cv_1) \wedge v_2 \wedge \dots \wedge v_m
	 = v_1 \wedge (cv_2) \wedge \dots \wedge v_m
	 = \dots .
	\]
\end{definition}
This definition is pretty wordy, but in English the three conditions say
\begin{itemize}
	\ii We should be able to add products like before,
	\ii You can put constants onto any of the $m$ components
	(as is directly pointed out in the ``vector space'' action), and
	\ii Switching any two adjacent wedges negates the whole wedge.
\end{itemize}
It's a pretty standard generalization of $\Lambda^2(V)$.
You can convince yourself that any element of the form
\[ \dots \wedge v \wedge \dots \wedge v \wedge \dots \]
should still be zero.

Just like $e_1 \wedge e_2$ was a basis earlier, we can find the basis
for general $m$ and $n$.
\begin{proposition}
	Let $V$ be a vector space with basis $e_1, \dots, e_n$.
	A basis for $\Lambda^m(V)$ consists of the elements
	\[ e_{i_1} \wedge e_{i_2} \wedge \dots \wedge e_{i_m} \]
	where
	\[ 1 \le i_1 < i_2 < \dots < i_m \le n. \]
	Hence $\Lambda^m(V)$ has dimension $\binom nm$.
\end{proposition}
\begin{proof}
	We knew earlier that $e_{i_1} \otimes \dots \otimes e_{i_m}$ was a basis for the tensor product.
	Here we have the additional property that (a) if two basis elements re-appear then the whole thing vanishes, and
	(b) we can shuffle around elements, and so we arbitrarily decide to put the basis elements
	in increasing order.
\end{proof}


\section{The Determinant}
\prototype{$(ae_1+be_2)\wedge(ce_1+de_2) = (ad-bc)e_1\wedge e_2$.}
Now we're ready to define the determinant.
Suppose $T : V \to V$ is a square matrix.
We claim that the map $\Lambda^m(V) \to \Lambda^m(V)$ given by
\[ v_1 \wedge v_2 \wedge \dots \wedge v_m
	\mapsto T(v_1) \wedge T(v_2) \wedge \dots \wedge T(v_m) \]
is also a linear map.
You can check this yourself if you like.
We call that map $\Lambda^m(T)$.

Now here's something interesting.
Suppose $V$ has dimension $n$, and let $m=n$.
Then $\Lambda^n(V)$ has dimension $\binom nn = 1$ -- it's a one dimensional space!
Hence $\Lambda^n(V) \simeq k$.

So $\Lambda^n(T)$ can be thought of as a linear map from $k$ to $k$.
But we know that \emph{a linear map from $k$ to $k$ is just multiplication by a constant}.
Hence $\Lambda^n(T)$ is multiplication by some constant.
\begin{definition}
	Let $T : V \to V$, where $V$ is an $n$-dimensional vector space.
	Then $\Lambda^n(T)$ is multiplication by a constant $c$;
	we define the \vocab{determinant} of $T$ as $c = \det T$.
\end{definition}

\begin{example}[The Determinant of a $2 \times 2$ Matrix]
	Let $V = \RR^2$ again with basis $e_1$ and $e_2$.
	Let
	\[ T = \left(
		\begin{array}{cc}
			a & c \\ b & d
		\end{array}
		\right).
	\]
	In other words, $T(e_1) = ae_1 + be_2$ and $T(e_2) = ce_1 + de_2$.

	Now let's consider $\Lambda^2(V)$.
	It has a basis $e_1 \wedge e_2$.
	Now $\Lambda^2(T)$ sends it to
	\[ e_1 \wedge e_2 \overset{\Lambda^2{T}}{\longmapsto} T(e_1) \wedge T(e_2) =
		(ae_1 + be_2) \wedge (ce_1 + de_2) 
		= (ad-bc)(e_1 \wedge e_2). 
	\]
	So $\Lambda^2(T) : \Lambda^2(V) \to \Lambda^2(V)$
	is multiplication by $\det T = ad-bc$,
	because it sent $e_1 \wedge e_2$ to $(ad-bc)(e_1 \wedge e_2)$.
\end{example}
And that is the definition of a determinant.
Once again, since we defined it in terms of $\Lambda^n(T)$,
this definition is totally independent of the choice of basis.
In other words, the determinant can be defined based on $T : V \to V$ alone
without any reference to matrices.

\begin{ques}
	Why does $\Lambda^n(S \circ T) = \Lambda^n(S) \circ \Lambda^n(T)$?
	(Remember that these are one-dimensional maps.)
\end{ques}
In this way, we also get \[ \det(S \circ T) = \det(S) \det(T) \] for free.
The general nasty formula for a determinant in terms of the matrix also follows from our work,
and is just a generalization of the work we did for $n=2$.
Simply write out
\[ \left( a_{11}e_1 + a_{21}e_2 + \dots \right) \wedge \dots \wedge
	\left( a_{1n}e_1 + a_{2n}e_2 + \dots + a_{nn} e_n \right)
\]
and do a full expansion.
\begin{exercise}
	Convince yourself this gives the right answer.
	(For example, expand this for $n=3$.)
\end{exercise}

\section\problemhead
\begin{problem}
	Show that for any real numbers $x_{ij}$ (here $1 \le i,j \le n$) we have
	\[
		\det \left(
		\begin{array}{cccc}
			x_{11} & x_{12} & \dots & x_{1n} \\
			x_{21} & x_{22} & \dots & x_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			x_{n1} & x_{n2} & \dots & x_{nn} \\
		\end{array}
		\right)
		=
		\det \left(
		\begin{array}{cccc}
			x_{11} + x_{12} & x_{12} & \dots & x_{1n} \\
			x_{21} + x_{22} & x_{22} & \dots & x_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			x_{n1} + x_{n2} & x_{n2} & \dots & x_{nn} \\
		\end{array}
		\right).
	\]
	\begin{hint}
		The point is that 
		\[ 
			(v_1+v_2) \wedge v_2 \dots \wedge v_n
			= v_1 \wedge v_2 \dots \wedge v_n
			+ v_2 \wedge v_2 \dots \wedge v_n
		\]
		and the latter term is zero.
	\end{hint}
\end{problem}


%\begin{problem}
%	[Sweden 2010, edited] A herd of $1000$ cows of nonzero weight is given.
%	Prove that we can remove one cow such that the remaining $999$ cows cannot be split
%	into two halves of equal weights.
%\end{problem}

\todo{maybe this isn't the best problem}
\begin{problem}[USAMO 2008, edited]
	At a certain mathematical conference,
	every two mathematicians are either friends or strangers.
	At mealtime, every participant eats in one of two large dining rooms.
	Each mathematician insists upon eating in a room which contains an
	even number of his or her friends.
	Assuming that such a split exists, prove that the number of ways
	that the mathematicians may be split between the two rooms is a power of two.
	% http://www.artofproblemsolving.com/Forum/viewtopic.php?p=3338962#p3338962
\end{problem}

\begin{problem}
	[Farey Sequences]
	\yod
	Let $a,b,c,d,p,q$ be positive integers such that
	\[ \frac ab < \frac pq < \frac cd \text{ and } \left\lvert ad-bc \right\rvert=1. \]
	Prove that $q \ge b+d$.
	(This equality is ``sharp'' in the sense that $p=a+c$, $q=b+d$ always works.)
	\begin{hint}
		Pick's Theorem.
	\end{hint}
	\begin{sol}
		Apply Pick's Theorem on the parallelogram with vertices $(0,0)$, $(a,b)$, $(c,d)$, and $(a+c,b+d)$
		to show that there are no lattice points in its interior.
	\end{sol}
\end{problem}
