\chapter{Spectral theory}
This chapter will develop the theory of eigenvalues and eigenvectors, the so-called
``Jordan canonical form'', and from it the characteristic polynomial.
In what follows, $V$ is a finite-dimensional vector space over a ground field $k$.

\section{Why you should care}
We know that a square matrix $T$ is really just
a linear map from $V$ to $V$.
What's the simplest type of linear map?
It would just be multiplication by some scalar $\lambda$,
which would have associated matrix
\[
	\left(
	\begin{array}{cccc}
		\lambda & 0 & \dots & 0 \\
		0 & \lambda & \dots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \dots & \lambda
	\end{array}
	\right).
\]
That's perhaps \emph{too} simple, though.
If we had a basis $e_1, \dots, e_n$ then another very ``simple'' operation
would just be scaling each basis element by $\lambda_i$,
i.e.\ a \vocab{diagonal} matrix of the form
\[
	T = 
	\left(
	\begin{array}{cccc}
		\lambda_1 & 0 & \dots & 0 \\
		0 & \lambda_2 & \dots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \dots & \lambda_n
	\end{array}
	\right).
\]
These maps are more general.
Indeed, you can, for example, compute $T^{100}$ in a heartbeat:
the map sends $e_1 \to \lambda_1^{100} e_1$.
Try doing that with an arbitrary $n \times n$ matrix.

Of course, most linear maps are probably not that nice.
Or are they?
Let's consider a map $T : \RR^2 \to \RR^2$
by $e_1 \mapsto 2e_1$ and $e_2 \mapsto e_1+3e_2$.
This doesn't look anywhere as nice until we realize we can rewrite it as
\begin{align*}
	e_1 &\mapsto 2e_1 \\ 
	e_1+e_2 &\mapsto 3(e_1+e_2).
\end{align*}
So our completely random-looking map, under a suitable change of basis,
looks like the very nice maps we described before!
That's the motivation for this chapter.

\section{Eigenvectors and eigenvalues}
Let $k$ be a field and $V$ a vector space over it.
In the above example, we saw that there were two very nice
vectors, $e_1$ and $e_1+e_2$, for which $V$ did something very simple.
Naturally, these vectors have a name.
\begin{definition}
	Let $T : V \to V$ and $v \in V$, $v \neq 0$.
	We say that $v$ is an \vocab{eigenvector} if $T(v) = \lambda v$
	for some $\lambda \in k$ (possibly zero, but remember $v \neq 0$).
	The value $\lambda$ is called an \vocab{eigenvalue} of $T$.
\end{definition}

Unfortunately, it's not exactly true that eigenvalues always exist.
\begin{example}[Eigenvalues need not exist]
	Let $V = \RR^2$ and let $T$ be the map which rotates a vector by $90\dg$
	around the origin.
	Then $T(v)$ is not a multiple of $v$ for any $v \in V$, other than the trivial $v=0$.
\end{example}

However, it is true if we replace $k$ with an algebraically closed field.
\begin{theorem}[Eigenvalues always exist over algebraically closed fields]
	Let $k$ be an algebraically closed field and $T : V \to V$ a linear map.
	Then there exists an eigenvalue $\lambda \in k$.
\end{theorem}
\begin{proof}
	I'm going to cheat a little here:
	\todo{Cite Axler, put in good proof here.}
	% http://www.maa.org/sites/default/files/images/upload_library/22/Ford/Axler139-154.pdf
	It's a theorem that if $\det S = 0$ for some linear map $S$,
	then $\ker S$ has a nonzero element (i.e.\ has dimension at least one).%
	\footnote{
		If you read the chapter on determinants:
		This follows from the fact that a wedge product
		$v_1 \wedge v_2 \wedge \dots \wedge v_m$ is zero
		if and only if the $v_i$ are linearly dependent.
		One direction of this is obvious; the converse is intuitive
		but somewhat annoying to prove.
	}

	Basically, we want to show that for some $\lambda$, we have
	$\lambda v = T(v)$.
	Letting $\id$ be the identity matrix, it's equivalent to showing that
	$(\lambda \cdot \id - T)(v) = 0$, i.e.\
	\[ \det \left( \lambda \id - T \right) = 0. \]
	The left-hand side is a polynomial in $\lambda$ when you expand it fully,
	so it has a root, as needed.
\end{proof}
So in general we like to consider algebraically closed fields.
This is not a big loss: any real matrix can be interpreted as a complex matrix
whose entries just happen to be real, for example.

\section{The Jordan form}
So that you know exactly where I'm going, I'm now going to state the main theorem.
\begin{theorem}
	[Jordan canonical form]
	Let $T : V \to V$ be a linear map over an algebraically closed field $k$.
	Then we can choose a basis of $V$ such that the matrix $T$ has the following form:
	\begin{center}
		\includegraphics[scale=0.25]{media/jordan.png}
	\end{center}
	Each of the blocks is called a \vocab{Jordan block}.
\end{theorem}
\todo{get better image, change $n$ to $m$.}
What does this mean?
Basically, it means our dream is almost true.
What happens is that $V$ can get broken down as a direct sum
\[ V = J_1 \oplus J_2 \oplus \dots \oplus J_m \]
and $T$ acts on each of these subspaces independently.
These subspaces correspond to the blocks in the matrix above.
In the simplest case, $\dim J_i = 1$, so $J_i$ has a basis element $e$ for which $T(e) = \lambda_i e$;
in other words, we just have a simple eigenvalue.
But on occasion, the situation is not quite so simple, and we have a block of size greater than $1$;
this leads to $1$'s just above the diagonals.
I'll explain later how to interpret the $1$'s.
For now, you should note that even if $\dim J_i \ge 2$, we still have a basis element
which is an eigenvector with element $\lambda_i$.

\begin{example}
	Let $T : k^6 \to k^6$ and suppose $T$ is given by the matrix
	\[ 
		T = 
		\left(
		\begin{array}{cccccc}
			3 & 0 & 0 & 0 & 0 & 0 \\
			0 & 7 & 0 & 0 & 0 & 0 \\
			0 & 0 & 2 & 1 & 0 & 0 \\
			0 & 0 & 0 & 2 & 0 & 0 \\
			0 & 0 & 0 & 0 & 5 & 0 \\
			0 & 0 & 0 & 0 & 0 & 3 \\
		\end{array}
		\right).
	\]
	Reading the matrix, we can compute all the eigenvectors and eigenvalues:
	they are
	\begin{align*}
		T(e_1) &= 3e_1 \\
		T(e_2) &= 7e_2 \\
		T(e_3) &= 2e_3 \\
		T(e_5) &= 5e_5 \\
		T(e_6) &= 3e_6.
	\end{align*}
\end{example}

Here's the idea behind the proof.
We would like to be able to break down $V$ directly into components as above,
but some of the $\lambda_i$'s from the different Jordan blocks could coincide,
and this turns out to prove a technical difficulty for detecting them.
So instead, we're going to break down $V$ first into subspaces based on the values of $\lambda$'s;
these are called the \vocab{generalized eigenspaces}.
In other words, the generalized eigenspace of a given $\lambda$
is just all the Jordan blocks which have eigenvalue $\lambda$.
Only after that will we break the generalized eigenspace into the individual Jordan blocks.

The sections below record this proof in the other order:
the next part deals with breaking generalized eigenspaces into Jordan blocks,
and the part after that is the one where we break down $V$ into
generalized eigenspaces.

\section{Nilpotent maps}
First, let's consider the following --- bear with me for a moment.
\begin{definition}
	A map $T: V \to V$ is \vocab{nilpotent} if $T^m$ is the zero map for some integer $m$.
	(Here $T^m$ means ``$T$ applied $m$ times''.)
\end{definition}
What's an example of a nilpotent map?
\begin{example}
	[The ``Descending staircase'']
	Let $V = k^3$ have basis $e_1$, $e_2$, $e_3$.
	Then the map $T$ which sends
	\[ e_3 \mapsto e_2 \mapsto e_1 \mapsto 0 \]
	is nilpotent, since $T(e_1) = T^2(e_2) = T^3(e_3) = 0$,
	and hence $T^3(v) = 0$ for all $v \in V$.
\end{example}
That's a pretty nice example. As another example, we can have multiple such staircases.
\begin{example}
	[Double staircase]
	Let $V = k^{\oplus 5}$ have basis $e_1$, $e_2$, $e_3$, $e_4$, $e_5$.
	Then the map 
	\[ e_3 \mapsto e_2 \mapsto e_1 \mapsto 0 \text{ and }
		e_5 \mapsto e_4 \mapsto 0 \]
	is nilpotent.
\end{example}
However this isn't really that different from the previous example;
it's just the same idea repeated multiple times.
And in fact we now claim that \emph{all} nilpotent maps have essentially that form.

\begin{theorem}
	[Nilpotent Jordan]
	Let $T : V \to V$ be a nilpotent map.
	Then we can write $V = \bigoplus_{i=1}^m V_i$
	where each $V_i$ has a basis of the form
	$v_i$, $T(v_i)$, \dots, $T^{\dim V_i - 1}(v_i)$
	for some $v_i \in V_i$.
	Hence every nilpotent map can be viewed as independent staircases.
\end{theorem}
Each chain $v_i$, $T(v_i)$, $T(T(v_i))$, \dots is just one staircase.
The proof is given later, but first let me point out where this is going.

Here's the punch line.
Let's take the double staircase again.
Expressing it as a matrix gives, say
\[
	S = \left(
	\begin{array}{ccccc}
		0 & 1 & 0 & 0 & 0 \\
		0 & 0 & 1 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0 & 0
	\end{array}
	\right).
\]
Then we can compute
\[
	S + \lambda \id = \left(
	\begin{array}{ccccc}
		\lambda & 1 & 0 & 0 & 0 \\
		0 & \lambda & 1 & 0 & 0 \\
		0 & 0 & \lambda & 0 & 0 \\
		0 & 0 & 0 & \lambda & 1 \\
		0 & 0 & 0 & 0 & \lambda
	\end{array}
	\right).
\]
It's a bunch of Jordan blocks!
This gives us a plan to proceed: we need to break $V$ into
a bunch of subspaces such that $T - \lambda \id$ is nilpotent over each subspace.
Then Nilpotent Jordan will finish the job.

\section{Reducing to the nilpotent case}
\begin{definition}
	Let $T : V \to V$. A subspace of $W$ is \vocab{$T$-invariant}
	if $T(w) \in W$ for any $w \in W$.
	In this way, $T$ can be thought of as a map $W \to W$.
\end{definition}
In this way, the Jordan form is a decomposition of $V$ into invariant subspaces.

Now I'm going to be cheap, and do the following.
\begin{definition}
	A map $T : V \to V$ is called \vocab{irreducible} if it's impossible to write $V = W_1 \oplus W_2$
	where both $W_1$ and $W_2$ are $T$-invariant.
\end{definition}
Picture of a \emph{reducible} map:
\[ 
	\left(
	\begin{array}{ccccc}
		\multicolumn{2}{c|}{\multirow{2}{*}{$W_1$}} & 0 & 0 & 0  \\ 
		\multicolumn{2}{c|}{} & 0 & 0 & 0 \\ \hline
		0 & 0 & \multicolumn{3}{|c}{\multirow{3}{*}{$W_2$}} \\
		0 & 0 & \multicolumn{3}{|c}{} \\
		0 & 0 & \multicolumn{3}{|c}{} 
	\end{array}
	\right)
\]
As you might expect, we can break a space apart into ``irreducible'' parts.
\begin{proposition}
	Given any map $T: V \to V$, we can write
	\[ V = V_1 \oplus V_2 \oplus \dots \oplus V_m \]
	where each $V_i$ is $T$-invariant, and for any $i$ the map $T : V_i \to V_i$ is irreducible.
\end{proposition}
\begin{proof}
	Same as the proof that every integer is the product of primes.
\end{proof}

Incredibly, with just that we're almost done!
Consider a decomposition as above, so that $T : V_1 \to V_1$ is an irreducible map.
Then $T$ has an eigenvalue $\lambda_1$, so let $S = T - \lambda_1 \id$; hence $\ker S \neq \{0\}$.
\begin{ques}
	Show that $V_1$ is also $S$-invariant, so we can consider $S : V_1 \to V_1$.
\end{ques}
By \Cref{prob:endomorphism_eventual_lemma}, we have
\[ V_1 = \ker S^N \oplus \img S^N \]
for some $N$.
But we assumed $V_1$ was irreducible, so this can only happen if $\img S^N = \{0\}$ and $\ker S^N = V$
(since $\ker S^N$ contains our eigenvector).
Hence $S$ is nilpotent, so it's a collection of staircases.
In fact, since $V_1$ is irreducible, there is only one staircase.
Hence $V_1$ is a Jordan block, as desired.

\section{(Optional) Proof of nilpotent Jordan}
The proof is just induction on $\dim V$.
Assume $\dim V \ge 1$, and let $W = T``(V)$ be the image of $V$.
Since $T$ is nilpotent, we must have $W \subsetneq V$.
Moreover, if $W = \{0\}$ (i.e.\ $T$ is the zero map) then we're already done.
So assume $\{0\} \subsetneq W \subsetneq V$.

By the inductive hypothesis, we can select a good basis of $W$:
\begin{align*}
	\mathcal B' =
	\Big\{ & T(v_1), T(T(v_1)), T(T(T(v_1))), \dots \\
	& T(v_2), T(T(v_2)), T(T(T(v_2))), \dots \\
	& \dots, \\
	& T(v_\ell), T(T(v_\ell)), T(T(T(v_\ell))), \dots \Big\}
\end{align*}
for some $T(v_i) \in W$ (here we have taken advantage of the fact that each element of $W$ is itself of the form $T(v)$ for some $v$).

Also, note that there are exactly $\ell$ elements of $\mathcal B'$ which are in $\ker T$
(namely the last element of each of the $\ell$ staircases).
We can thus complete it to a basis $v_{\ell+1}, \dots, v_m$ (where $m = \dim \ker T$).
(In other words, the last element of each staircase plus the $m-\ell$ new ones are a basis for $\ker T$.)

Now consider
\begin{align*}
	\mathcal B =
	\Big\{ & v_1, T(v_1), T(T(v_1)), T(T(T(v_1))), \dots \\
	& v_2, T(v_2), T(T(v_2)), T(T(T(v_2))), \dots \\
	& \dots, \\
	& v_\ell, T(v_\ell), T(T(v_\ell)), T(T(T(v_\ell))), \dots \\
	& v_{\ell+1}, v_{\ell+2}, \dots, v_m \Big\}.
\end{align*}
\begin{ques}
Check that there are exactly $\ell + \dim W + (\dim \ker T - \ell) = \dim V$ elements.
\end{ques}
\begin{exercise}
	Show that all the elements are linearly independent.
	(Assume for contradiction there is some linear independence,
	then take $T$ of both sides.)
\end{exercise}
Hence $\mathcal B$ is a basis of the desired form.


\section{Characteristic polynomials, and Cayley-Hamilton}
Take a map $T : V \to V$, where $V$ is $n$-dimensional, and suppose its eigenvalues
are $a_1$, $a_2$, \dots, $a_n$ (with repetition).
Then the \vocab{characteristic polynomial} is given by
\[
	p_T(X) = (X-a_1)(X-a_2) \dots (X-a_n).
\]
Note that if we've written $T$ in Jordan form, i.e.\ 
\[
	T
	=
	\left(
	\begin{array}{ccccc}
		a_1 & \ast & 0 & \dots & 0 \\
		0 & a_2 & \ast & \dots & 0 \\
		0 & 0 & a_3 & \dots & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0 & 0 & 0 & \dots & a_n
	\end{array}
	\right)
\]
(here each $\ast$ is either $0$ or $1$),
then we can ``hack'' together the following ``immoral'' definition:
\[
	p_T(X)
	\defeq
	\det
	\left(
	\begin{array}{ccccc}
		X - a_1 & \ast & 0 & \dots & 0 \\
		0 & X - a_2 & \ast & \dots & 0 \\
		0 & 0 & X - a_3 & \dots & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0 & 0 & 0 & \dots & X - a_n
	\end{array}
	\right)
	= \det \left( X \cdot \id_n - T \right).
\]
The latter definition is what you'll see in most linear algebra books because
it lets you define the characteristic polynomial without having to say anything,
and more importantly it lets you construct the characteristic polynomial
even if you use the evil ``array of numbers'' way of teaching linear algebra.
I'll admit it does have the merit that it means that given any matrix,
it's easy to compute the characteristic polynomial and hence
compute the eigenvalues.
But what a \emph{terrible} way to define the characteristic polynomial.

In particular, it obscures the following theorem, which is actually a completely triviality.

\begin{theorem}[Cayley-Hamilton theorem]
	For any $T : V \to V$,
	the map $p_T(T)$ is the zero map.
\end{theorem}
Here, by $p_T(T)$ we mean that if \[ p_T(X) = X^n + c_{n-1} X^{n-1} + \dots + c_0 \]
then \[ p_T(T) = T^n + c_{n-1} T^{n-1} + \dots + c_1 T +  c_0 I \] is the zero map,
where $T^k$ denotes $T$ applied $k$ times.

\begin{example}[Example of Cayley-Hamilton, immorally]
	Suppose $T = \left(
	\begin{array}{cc}
		1 & 2 \\ 3 & 4
	\end{array}
	\right)$.
	Using the immoral definition of characteristic polynomial,
	we find that $p_T(X) = (X-1)(X-4)-(-2)(-3) = X^2 - 5X - 2$.
	Indeed, you can verify that
	\[ T^2 - 5T - 2
		=
		\left(
		\begin{array}{cc}
			7 & 10 \\
			15 & 22
		\end{array}
		\right)
		- 5 \cdot \left(
		\begin{array}{cc}
			1 & 2 \\
			3 & 4
		\end{array}
		\right)
		- 2 \cdot \left(
		\begin{array}{cc}
			1 & 0 \\
			0 & 1
		\end{array}
		\right)
		=
		\left(
		\begin{array}{cc}
			0 & 0 \\
			0 & 0
		\end{array}
		\right).
	\]
\end{example}
If you are only told the immoral definition of $p_T$ and the evil view that matrices are
arrays of numbers, then this looks like a complete miracle.
Indeed, just look at the terrible proofs on Wikipedia.

But if you use the abstract viewpoint of $T$ as a map, then it is a total tautology:
\begin{proof}[Proof of Cayley-Hamilton]
	Suppose we write $V$ in Jordan normal form as
	\[ V = J_1 \oplus \dots \oplus J_m \]
	where $J_i$ has eigenvalue $\lambda_i$ and dimension $d_i$.
	By definition,
	\[ p_T(X) = (X - \lambda_1)^{d_1} (X - \lambda_2)^{d_2} \dots (X - \lambda_m)^{d_m}. \]
	By definition, $(X - \lambda_1)^{d_1}$ is the zero map on $J_1$.
	So $T$ is zero on $J_1$.
	Similarly it's zero on each of the other $J_i$'s --- end of story.
\end{proof}

Also, a quick fun fact: all of this actually gives us another way to show the trace
is independent from the actual matrix, as follows.
\begin{exercise}
	Let $T : V \to V$, and suppose $p_T(X) = X^n + c_{n-1}X^{n-1} + \dots + c_0$.
	Show that in any basis, the sum of the diagonal entries of $T$ is $-c_{n-1}$.
	Moreover, $\det T = (-1)^n c_0$ is the product of the eigenvalues.
\end{exercise}
The fact that $\det T$ is the \emph{product} of the eigenvalues should
actually not be surprising in retrospect.
Suppose $T : \RR^2 \to \RR^2$ has a basis of eigenvectors $e_1$ and $e_2$
with eigenvalues $4$ and $5$; hence $T(e_1) = 4e_1$ and $T(e_2) = 5e_2$,
and so of course we would say that $T$ stretches areas by a factor of $20$!

\section{(Digression) Tensoring up}
Earlier I said that we'll generally just work over algebraically closed fields,
because, for example, a real matrix can be interpreted as a complex matrix whose
entries just happen to be real numbers.
I just want to briefly tell you, in terms of tensor products, exactly what we have done.

Let's take the space $V = \RR^3$, with basis $e_1$, $e_2$, $e_3$.
Thus objects in $V$ are of the form
\[ r_1 e_1 + r_2 e_2 + r_3 e_3 \]
where $r_1$, $r_2$, $r_3$ are real numbers.
We want to consider essentially the same vector space,
but with complex coefficients $z_i$ rather than real coefficients $e_i$.

So here's what we do: view $\CC$ as a $\RR$-vector space (with basis $\{1,i\}$, say)
and consider the \vocab{complexification}
\[ V_\CC \defeq \CC \largeotimes{\RR} V. \]
Then you can check that our elements are actually of the form
\[ z_1 \otimes e_1 + z_2 \otimes e_2 + z_3 \otimes e_3. \]
(Here, the tensor product is over $\RR$, so we have $z \otimes re_i = (zr) \otimes e_i$ for $r \in \RR$.)
Then $V_{\CC}$ can be thought as a vector space over $\CC$, since you can multiply on the left
by complex numbers.
In this way, the tensor product lets us ``fuse on'' complex coefficients even though technically
we shouldn't be able to do so.

If $T : V \to W$, is a map, then $T_\CC : V_\CC \to W_\CC$
is just the map $z \otimes v \mapsto z \otimes T(v)$.
You'll see this written sometimes as $T_\CC = \id \otimes T$.


\section\problemhead

\begin{problem}
	For a matrix $X$, we define the exponential map by
	\[ \exp(X) = 1 + X + \frac{X^2}{2!} + \frac{X^3}{3!} + \dots \]
	(take it for granted that this converges to a matrix).
	Prove that
	\[ \det(\exp(X)) = e^{\Tr X}. \]
	\begin{hint}
		This is actually immediate by taking any basis
		in which $X$ is upper-triangular.
	\end{hint}
\end{problem}

\begin{problem}
	[Putnam 1988]
	\gim
	Let $V$ be an $n$-dimensional vector space.
	Let $T : V \to V$ be a linear map and suppose there exists $n+1$ eigenvectors,
	any $n$ of which are linearly independent.
	Does it follow that $T$ is a scalar multiple of the identity?
	\begin{hint}
		Look at the trace of $T$.
	\end{hint}
\end{problem}

\begin{problem}
	[Putnam 2015]
	\yod
	Define $S$ to be the set of real matrices 
	$\left(\begin{smallmatrix} a & b \\ c & d \end{smallmatrix}\right)$
	such that $a$, $b$, $c$, $d$ form
	an arithmetic progression in that order.
	Find all $M \in S$ such that for some integer $k > 1$, $M^k \in S$.
	\begin{hint}
		Show the matrix is invertible (unless $a=b=c=d$)
		and then diagonalize the matrix as
		$
		M =
		\begin{pmatrix} s & -q \\ -r & p \end{pmatrix}
		\begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix}
		\begin{pmatrix} p & q \\ r & s \end{pmatrix}
		=
		\begin{pmatrix} 
			ps\lambda_1 - qr\lambda_2 & qs(\lambda_1-\lambda_2) \\
			pr(\lambda_2-\lambda_1) & ps\lambda_2 - qr\lambda_1
		\end{pmatrix} 
		$.
		There is a family of solutions other than just $a=b=c=d$.
	\end{hint}
	\begin{sol}
		The answer is 
		\[ \begin{pmatrix} t&t\\t&t \end{pmatrix}
			\quad\text{and}\quad 
			\begin{pmatrix} -3t&-t\\t&3t \end{pmatrix} \]
		for $t \in \RR$.
		These work by taking $k=3$.
		Now to see these are the only ones, consider an arithmetic matrix
		\[ M = \begin{pmatrix} a & a+e \\ a+2e & a+3e \end{pmatrix}. \]
		which is not zero everywhere.
		Its characteristic polynomial is $t^2 - (2a+3e)t - 2e^2$,
		with discriminant $(2a+3e)^2 + 8e^2$,
		so it has two distinct real roots; moreover, since $-2e^2 \le 0$
		either one of the roots is zero or they are of opposite signs.
		Now we can diagonalize $M$ by writing
		\[
			M =
			\begin{pmatrix} s & -q \\ -r & p \end{pmatrix}
			\begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix}
			\begin{pmatrix} p & q \\ r & s \end{pmatrix}
			=
			\begin{pmatrix} 
				ps\lambda_1 - qr\lambda_2 & qs(\lambda_1-\lambda_2) \\
				pr(\lambda_2-\lambda_1) & ps\lambda_2 - qr\lambda_1
			\end{pmatrix} 
		\]
		where $ps-qr=1$. By using the fact the diagonal entries have sum equalling
		the off-diagonal entries, we obtain that
		\[ (ps-qr)(\lambda_1+\lambda_2) = (qs-pr)(\lambda_1-\lambda_2) 
			\implies qs-pr = \frac{\lambda_1+\lambda_2}{\lambda_1-\lambda_2}. \]
		Now if $M^k \in S$ too then the same calculation gives
		\[ qs-pr = \frac{\lambda_1^k+\lambda_2^k}{\lambda_1^k-\lambda_2^k}. \]
		Let $x = \lambda_1/\lambda_2 < 0$ (since $-2e^2 < 0$). We appropriately get
		\[ \frac{x+1}{x-1} = \frac{x^k+1}{x^k-1}
			\implies \frac{2}{x-1} = \frac{2}{x^k-1}
			\implies x = x^k \implies x = -1 \text{ or } x = 0 \]
		and $k$ odd. If $x=0$ we get $e=0$ and if $x=-1$ we get $2a+3e=0$,
		which gives the curve of solutions that we claimed.
		(Unfortunately, during the test I neglected to address $x=-1$,
		which probably means I will get null marks.)
	\end{sol}
\end{problem}

\endinput
\section{Reducing to the nilpotent case}
\begin{definition}
	Let $T : V \to V$. A subspace of $W$ is \vocab{$T$-invariant}
	if $T(w) \in W$ for any $w \in W$.
	In this way, $T$ can be thought of as a map $W \to W$.
\end{definition}
Visually as a matrix, if $V = W_1 \oplus W_2$ and $W_1$ is $T$-invariant, that means we have zero's in the other rows for the columns associated to $W_1$.
\[ 
	\left(
	\begin{array}{ccccc}
		\multicolumn{2}{c|}{\multirow{2}{*}{$W_1$}} &  &  &  \\ 
		\multicolumn{2}{c|}{} &  &  &  \\ \cline{1-2}
		0 & 0 & \multicolumn{3}{c}{\multirow{3}{*}{$W_2$}} \\
		0 & 0 & \multicolumn{3}{c}{} \\
		0 & 0 & \multicolumn{3}{c}{} 
	\end{array}
	\right)
\]

\begin{ques}
	Let $\lambda \in k$.
	Show that a subspace $W$ is $T$-invariant if and only if it is $(T - \lambda I)$-invariant.
\end{ques}

Our goal is to prove that we can decompose $V$ as
\[ V = \bigoplus_\lambda V^{\lambda} \]
for some choices of $\lambda$, such that each $V^\lambda$ is $T$-invariant,
and moreover $T-\lambda \id$ is nilpotent on it.
(As I said earlier, each $V^\lambda$ is called a \emph{generalized eigenspace}.)
If we succeed in doing so, then we can use Nilpotent Jordan to finish.

For this, I'll proceed in the spirit of Terrence Tao.\todo{link?}
The idea behind this proof is to consider ``polynomials'' in $T$.
For example, $2T^2-4T+5$ would be shorthand for $T(T(v)) - 4T(v) + 5$.
In this way we can consider ``polynomials'' $P(T)$;
this lets us tie in the ``algebraically closed'' condition.
These polynomial behave nicely: we have $P(T)+Q(T) = (P+Q)T$ and $P(T) \circ Q(T) = (P \circ Q)(T)$.
In fact, we get the very nice bonus property
\[ P(T) \circ Q(T) = Q(T) \circ P(T) \]
which is otherwise completely non-obvious!

Specifically, consider the infinite sequence
\[ 1, T, T^2, T^3, \dots. \]
Since the space of maps $V \to V$ is itself finite dimensional (at most $n^2$; why?) then there is some linear dependency.
So there exists a polynomial $P$ with coefficients in $k$ such that $P(T)$ is the zero map:
algebraically, this says $V = \ker P(T)$.

Since $k$ is algebraically closed, we now know that $P(x)$ can be factored completely in $k$:
\[ P(x) = \left( x - \lambda_1 \right)^{t_1} \left( x - \lambda_2 \right)^{t_2} \dots (x - \lambda_s)^{t_s}. \]
But let's take this a bit more slowly.
Suppose that $s \ge 2$, meaning $P(x) = A_1(x) A_2(x)$ for relatively prime $A_1$ and $A_2$.
Then by Bezout's lemma, there are ``dummy'' polynomials $D_1(x)$ and $D_2(x)$ such that
\[ 1 = D_1(x) A_1(x) + D_2(x) A_2(x). \]
Plugging in $x=T$, we get
\[ \id_V = D_1(T) \circ A_1(T) + D_2(T) \circ A_2(T). \]
Now let $V_1 = \ker A_1(T)$ and $V_2 = \ker A_2(T)$.

We claim that $V = V_1 \oplus V_2$.
\begin{ques}
	Show that $V_1 \cap V_2 = \{0\}$; that is,
	if $A_1(T)(v) = A_2(T)(v) = 0$, then $v=0$.
\end{ques}
Moreover, for any $v \in V$, $v = (A_1 \cdot D_1)(T)(v) + (A_2 \cdot D_2)(T)(V)$.
\begin{ques}
	Show that $(A_1 \cdot D_1)(T)(v) \in V_2$
	and similarly $(A_2 \cdot D_2)(T)(v) \in V_1$.
\end{ques}
Finally, $V_1$ and $V_2$ are $T$-invariant, as $(A_1(T) \cdot T)(v) = (T \cdot A_1(T))(v)$.

Thus, we deduce
\[ V = V_1 \oplus V_2 \]
is an irreducible representation.
We can repeat this procedure on $\ker A_1$
