\chapter{Spectral theory}
This chapter will develop the theory of eigenvalues and eigenvectors, the so-called
``Jordan canonical form'', and from it the characteristic polynomial.
In what follows, $V$ is a finite-dimensional vector space over a ground field $k$.

\section{Why you should care}
We know that a square matrix $T$ is really just
a linear map from $V$ to $V$.
What's the simplest type of linear map?
It would just be multiplication by some scalar $\lambda$,
which would have associated matrix
\[
	\begin{bmatrix}
		\lambda & 0 & \dots & 0 \\
		0 & \lambda & \dots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \dots & \lambda
	\end{bmatrix}.
\]
That's perhaps \emph{too} simple, though.
If we had a basis $e_1, \dots, e_n$ then another very ``simple'' operation
would just be scaling each basis element by $\lambda_i$,
i.e.\ a \vocab{diagonal} matrix of the form
\[
	T = \begin{bmatrix}
		\lambda_1 & 0 & \dots & 0 \\
		0 & \lambda_2 & \dots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \dots & \lambda_n
	\end{bmatrix}.
\]
These maps are more general.
Indeed, you can, for example, compute $T^{100}$ in a heartbeat:
the map sends $e_1 \to \lambda_1^{100} e_1$.
Try doing that with an arbitrary $n \times n$ matrix.

Of course, most linear maps are probably not that nice.
Or are they?
Let's consider a map $T : \RR^2 \to \RR^2$
by $e_1 \mapsto 2e_1$ and $e_2 \mapsto e_1+3e_2$.
This doesn't look anywhere as nice until we realize we can rewrite it as
\begin{align*}
	e_1 &\mapsto 2e_1 \\ 
	e_1+e_2 &\mapsto 3(e_1+e_2).
\end{align*}
So our completely random-looking map, under a suitable change of basis,
looks like the very nice maps we described before!
That's the motivation for this chapter.

\section{Eigenvectors and eigenvalues}
Let $k$ be a field and $V$ a vector space over it.
In the above example, we saw that there were two very nice
vectors, $e_1$ and $e_1+e_2$, for which $V$ did something very simple.
Naturally, these vectors have a name.
\begin{definition}
	Let $T : V \to V$ and $v \in V$, $v \neq 0$.
	We say that $v$ is an \vocab{eigenvector} if $T(v) = \lambda v$
	for some $\lambda \in k$ (possibly zero, but remember $v \neq 0$).
	The value $\lambda$ is called an \vocab{eigenvalue} of $T$.
	(Of course, no mention to a basis anywhere.)
\end{definition}

Unfortunately, it's not exactly true that eigenvalues always exist.
\begin{example}[Eigenvalues need not exist]
	Let $V = \RR^2$ and let $T$ be the map which rotates a vector by $90\dg$
	around the origin.
	Then $T(v)$ is not a multiple of $v$ for any $v \in V$, other than the trivial $v=0$.
\end{example}

However, it is true if we replace $k$ with an algebraically closed field
\footnote{A field is \vocab{algebraically closed} if all its
polynomials have roots, the archetypal example being $\CC$.}
\begin{theorem}[Eigenvalues always exist over algebraically closed fields]
	Suppose $k$ is also algebraically closed field.
	Then if $T : V \to V$ is a linear map,
	there exists an eigenvalue $\lambda \in k$.
\end{theorem}
\begin{proof}
	(From \cite{ref:axler})
	The idea behind this proof is to consider ``polynomials'' in $T$.
	For example, $2T^2-4T+5$ would be shorthand for $2T(T(v)) - 4T(v) + 5v$.
	In this way we can consider ``polynomials'' $P(T)$;
	this lets us tie in the ``algebraically closed'' condition.
	These polynomials behave nicely:
	\begin{ques}
		Show that $P(T)+Q(T) = (P+Q)(T)$ and $P(T) \circ Q(T) = (P \cdot Q)(T)$.
	\end{ques}

	Let $n = \dim V < \infty$ and fix any nonzero vector $v \in V$,
	and consider vectors $v$, $T(v)$, \dots, $T^n (v)$.
	They must be linearly dependent for dimension reasons;
	thus there is a nonzero polynomial $P$ such that $P(T)$
	is zero when applied to $v$.
	WLOG suppose $P$ is a monic polynomial,
	and thus $P(z) = (z-r_1)\dots(z-r_m)$ say.
	Then we get
	\[ 0 = (T - r_1 \id) \circ (T - r_2 \id) \circ \dots
		\circ (T - r_m \id)(v) \]
	which means at least one of $T - r_i \id$ is not injective, done.
\end{proof}
So in general we like to consider algebraically closed fields.
This is not a big loss: any real matrix can be interpreted as a complex matrix
whose entries just happen to be real, for example.

\section{The Jordan form}
So that you know exactly where I'm going, here's the main theorem.
\begin{theorem}
	[Jordan canonical form]
	Let $T : V \to V$ be a linear map over an algebraically closed field $k$.
	Then we can choose a basis of $V$ such that the matrix $T$ has the form
	\[
	\begin{bmatrix}
		\lambda_1 & 1 \\
		0 & \lambda_1 \\
		&& \lambda_2 \\
		&&& \lambda_3 & 1 & 0 \\
		&&& 0 & \lambda_3 & 1 \\
		&&& 0 & 0 & \lambda_3 \\
		&&&&&& \ddots \\
		&&&&&&& \lambda_m & 1 \\
		&&&&&&& 0 & \lambda_m
	\end{bmatrix}
	\]
	Each of the blocks is called a \vocab{Jordan block}.
	(The blank entries are all zero.)
\end{theorem}
What does this mean?
Basically, it means our dream is almost true.
What happens is that $V$ can get broken down as a direct sum
\[ V = J_1 \oplus J_2 \oplus \dots \oplus J_m \]
and $T$ acts on each of these subspaces independently.
These subspaces correspond to the blocks in the matrix above.
In the simplest case, $\dim J_i = 1$, so $J_i$ has a basis element $e$ for which $T(e) = \lambda_i e$;
in other words, we just have a simple eigenvalue.
But on occasion, the situation is not quite so simple, and we have a block of size greater than $1$;
this leads to $1$'s just above the diagonals.
I'll explain later how to interpret the $1$'s.
For now, you should note that even if $\dim J_i \ge 2$, we still have a basis element
which is an eigenvector with element $\lambda_i$.

\begin{example}
	Let $T : k^6 \to k^6$ and suppose $T$ is given by the matrix
	\[ 
		T = \begin{bmatrix}
			3 & 0 & 0 & 0 & 0 & 0 \\
			0 & 7 & 0 & 0 & 0 & 0 \\
			0 & 0 & 2 & 1 & 0 & 0 \\
			0 & 0 & 0 & 2 & 0 & 0 \\
			0 & 0 & 0 & 0 & 5 & 0 \\
			0 & 0 & 0 & 0 & 0 & 3 \\
		\end{bmatrix}.
	\]
	Reading the matrix, we can compute all the eigenvectors and eigenvalues:
	they are
	\begin{align*}
		T(e_1) &= 3e_1 \\
		T(e_2) &= 7e_2 \\
		T(e_3) &= 2e_3 \\
		T(e_5) &= 5e_5 \\
		T(e_6) &= 3e_6.
	\end{align*}
\end{example}

Here's the idea behind the proof.
We would like to be able to break down $V$ directly into components as above,
but some of the $\lambda_i$'s from the different Jordan blocks could coincide,
and this turns out to prove a technical difficulty for detecting them.
So instead, we're going to break down $V$ first into subspaces based on the values of $\lambda$'s;
these are called the \vocab{generalized eigenspaces}.
In other words, the generalized eigenspace of a given $\lambda$
is just all the Jordan blocks which have eigenvalue $\lambda$.
Only after that will we break the generalized eigenspace into the individual Jordan blocks.

The sections below record this proof in the other order:
the next part deals with breaking generalized eigenspaces into Jordan blocks,
and the part after that is the one where we break down $V$ into
generalized eigenspaces.

\section{Nilpotent maps}
Bear with me for a moment.  First, define:
\begin{definition}
	A map $T: V \to V$ is \vocab{nilpotent} if $T^m$ is the zero map for some integer $m$.
	(Here $T^m$ means ``$T$ applied $m$ times''.)
\end{definition}
What's an example of a nilpotent map?
\begin{example}
	[The ``Descending staircase'']
	Let $V = k^3$ have basis $e_1$, $e_2$, $e_3$.
	Then the map $T$ which sends
	\[ e_3 \mapsto e_2 \mapsto e_1 \mapsto 0 \]
	is nilpotent, since $T(e_1) = T^2(e_2) = T^3(e_3) = 0$,
	and hence $T^3(v) = 0$ for all $v \in V$.
\end{example}
That's a pretty nice example. As another example, we can have multiple such staircases.
\begin{example}
	[Double staircase]
	Let $V = k^{\oplus 5}$ have basis $e_1$, $e_2$, $e_3$, $e_4$, $e_5$.
	Then the map 
	\[ e_3 \mapsto e_2 \mapsto e_1 \mapsto 0 \text{ and }
		e_5 \mapsto e_4 \mapsto 0 \]
	is nilpotent.
\end{example}
However this isn't really that different from the previous example;
it's just the same idea repeated multiple times.
And in fact we now claim that \emph{all} nilpotent maps have essentially that form.

\begin{theorem}
	[Nilpotent Jordan]
	Let $T : V \to V$ be a nilpotent map.
	Then we can write $V = \bigoplus_{i=1}^m V_i$
	where each $V_i$ has a basis of the form
	$v_i$, $T(v_i)$, \dots, $T^{\dim V_i - 1}(v_i)$
	for some $v_i \in V_i$.
	Hence every nilpotent map can be viewed as independent staircases.
\end{theorem}
Each chain $v_i$, $T(v_i)$, $T(T(v_i))$, \dots is just one staircase.
The proof is given later, but first let me point out where this is going.

Here's the punch line.
Let's take the double staircase again.
Expressing it as a matrix gives, say
\[
	S = \begin{bmatrix}
		0 & 1 & 0 & 0 & 0 \\
		0 & 0 & 1 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0 & 0
	\end{bmatrix}.
\]
Then we can compute
\[
	S + \lambda \id = \begin{bmatrix}
		\lambda & 1 & 0 & 0 & 0 \\
		0 & \lambda & 1 & 0 & 0 \\
		0 & 0 & \lambda & 0 & 0 \\
		0 & 0 & 0 & \lambda & 1 \\
		0 & 0 & 0 & 0 & \lambda
	\end{bmatrix}.
\]
It's a bunch of Jordan blocks!
This gives us a plan to proceed: we need to break $V$ into
a bunch of subspaces such that $T - \lambda \id$ is nilpotent over each subspace.
Then Nilpotent Jordan will finish the job.

\section{Reducing to the nilpotent case}
\begin{definition}
	Let $T : V \to V$. A subspace $W \subseteq V$
	is called \vocab{$T$-invariant}
	if $T(w) \in W$ for any $w \in W$.
	In this way, $T$ can be thought of as a map $W \to W$.
\end{definition}
In this way, the Jordan form is a decomposition of $V$ into invariant subspaces.

Now I'm going to be cheap, and define:
\begin{definition}
	A map $T : V \to V$ is called \vocab{indecomposable}
	if it's impossible to write $V = W_1 \oplus W_2$
	where both $W_1$ and $W_2$ are $T$-invariant.
\end{definition}
Picture of a \emph{decomposable} map:
\[ 
	\begin{bmatrix}
		\multicolumn{2}{c|}{\multirow{2}{*}{$W_1$}} & 0 & 0 & 0  \\ 
		\multicolumn{2}{c|}{} & 0 & 0 & 0 \\ \hline
		0 & 0 & \multicolumn{3}{|c}{\multirow{3}{*}{$W_2$}} \\
		0 & 0 & \multicolumn{3}{|c}{} \\
		0 & 0 & \multicolumn{3}{|c}{} 
	\end{bmatrix}
\]
As you might expect, we can break a space apart into ``indecomposable'' parts.
\begin{proposition}
	Given any map $T: V \to V$, we can write
	\[ V = V_1 \oplus V_2 \oplus \dots \oplus V_m \]
	where each $V_i$ is $T$-invariant,
	and for any $i$ the map $T : V_i \to V_i$ is indecomposable.
\end{proposition}
\begin{proof}
	Same as the proof that every integer is the product of primes.
\end{proof}

Incredibly, with just that we're almost done!
Consider a decomposition as above,
so that $T : V_1 \to V_1$ is an indecomposable map.
Then $T$ has an eigenvalue $\lambda_1$, so let $S = T - \lambda_1 \id$; hence $\ker S \neq \{0\}$.
\begin{ques}
	Show that $V_1$ is also $S$-invariant, so we can consider $S : V_1 \to V_1$.
\end{ques}
By \Cref{prob:endomorphism_eventual_lemma}, we have
\[ V_1 = \ker S^N \oplus \img S^N \]
for some $N$.
But we assumed $T$ was indecomposable,
so this can only happen if $\img S^N = \{0\}$ and $\ker S^N = V$
(since $\ker S^N$ contains our eigenvector).
Hence $S$ is nilpotent, so it's a collection of staircases.
In fact, since $T$ is indecomposable, there is only one staircase.
Hence $V_1$ is a Jordan block, as desired.

\section{(Optional) Proof of nilpotent Jordan}
The proof is just induction on $\dim V$.
Assume $\dim V \ge 1$, and let $W = T``(V)$ be the image of $V$.
Since $T$ is nilpotent, we must have $W \subsetneq V$.
Moreover, if $W = \{0\}$ (i.e.\ $T$ is the zero map) then we're already done.
So assume $\{0\} \subsetneq W \subsetneq V$.

By the inductive hypothesis, we can select a good basis of $W$:
\begin{align*}
	\mathcal B' =
	\Big\{ & T(v_1), T(T(v_1)), T(T(T(v_1))), \dots \\
	& T(v_2), T(T(v_2)), T(T(T(v_2))), \dots \\
	& \dots, \\
	& T(v_\ell), T(T(v_\ell)), T(T(T(v_\ell))), \dots \Big\}
\end{align*}
for some $T(v_i) \in W$ (here we have taken advantage of the fact that each element of $W$ is itself of the form $T(v)$ for some $v$).

Also, note that there are exactly $\ell$ elements of $\mathcal B'$ which are in $\ker T$
(namely the last element of each of the $\ell$ staircases).
We can thus complete it to a basis $v_{\ell+1}, \dots, v_m$ (where $m = \dim \ker T$).
(In other words, the last element of each staircase plus the $m-\ell$ new ones are a basis for $\ker T$.)

Now consider
\begin{align*}
	\mathcal B =
	\Big\{ & v_1, T(v_1), T(T(v_1)), T(T(T(v_1))), \dots \\
	& v_2, T(v_2), T(T(v_2)), T(T(T(v_2))), \dots \\
	& \dots, \\
	& v_\ell, T(v_\ell), T(T(v_\ell)), T(T(T(v_\ell))), \dots \\
	& v_{\ell+1}, v_{\ell+2}, \dots, v_m \Big\}.
\end{align*}
\begin{ques}
Check that there are exactly $\ell + \dim W + (\dim \ker T - \ell) = \dim V$ elements.
\end{ques}
\begin{exercise}
	Show that all the elements are linearly independent.
	(Assume for contradiction there is some linear dependence,
	then take $T$ of both sides.)
\end{exercise}
Hence $\mathcal B$ is a basis of the desired form.


\section{Characteristic polynomials, and Cayley-Hamilton}
Take a map $T : V \to V$, where $V$ is $n$-dimensional, and suppose its eigenvalues
are $a_1$, $a_2$, \dots, $a_n$ (with repetition).
Then the \vocab{characteristic polynomial} is given by
\[
	p_T(X) = (X-a_1)(X-a_2) \dots (X-a_n).
\]
Note that if we've written $T$ in Jordan form, i.e.\ 
\[
	T = \begin{bmatrix}
		a_1 & \ast & 0 & \dots & 0 \\
		0 & a_2 & \ast & \dots & 0 \\
		0 & 0 & a_3 & \dots & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0 & 0 & 0 & \dots & a_n
	\end{bmatrix}
\]
(here each $\ast$ is either $0$ or $1$),
then we can hack together the definition
\[
	p_T(X) \defeq
	\det \left( X \cdot \id_n - T \right).
	=
	\det \begin{bmatrix}
		X - a_1 & \ast & 0 & \dots & 0 \\
		0 & X - a_2 & \ast & \dots & 0 \\
		0 & 0 & X - a_3 & \dots & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0 & 0 & 0 & \dots & X - a_n
	\end{bmatrix}.
\]
The latter definition is what you'll see in most
linear algebra books because it lets you define the characteristic polynomial
without mentioning the word ``eigenvalue''
(i.e.\ entirely in terms of arrays of numbers).
I'll admit it does have the merit that it means that given any matrix,
it's easy to compute the characteristic polynomial and hence
compute the eigenvalues;
but I still think the definition should be done in terms of
eigenvalues to begin with.
For instance the determinant definition obscures the following theorem,
which is actually a completely triviality.
\begin{theorem}[Cayley-Hamilton theorem]
	For any $T : V \to V$,
	the map $p_T(T)$ is the zero map.
\end{theorem}
Here, by $p_T(T)$ we mean that if \[ p_T(X) = X^n + c_{n-1} X^{n-1} + \dots + c_0 \]
then \[ p_T(T) = T^n + c_{n-1} T^{n-1} + \dots + c_1 T +  c_0 I \]
is the zero map,
where $T^k$ denotes $T$ applied $k$ times.

\begin{example}[Example of Cayley-Hamilton using determinant definition]
	Suppose $T = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$.
	Using the determinant definition of characteristic polynomial,
	we find that $p_T(X) = (X-1)(X-4)-(-2)(-3) = X^2 - 5X - 2$.
	Indeed, you can verify that
	\[ T^2 - 5T - 2
		= \begin{bmatrix}
			7 & 10 \\
			15 & 22
		\end{bmatrix}
		- 5 \cdot \begin{bmatrix}
			1 & 2 \\
			3 & 4
		\end{bmatrix}
		- 2 \cdot \begin{bmatrix}
			1 & 0 \\
			0 & 1
		\end{bmatrix}
		= \begin{bmatrix}
			0 & 0 \\
			0 & 0
		\end{bmatrix}.
	\]
\end{example}
If you define $p_T$ without the word eigenvalue,
and adopt the evil view that matrices are arrays of numbers,
then this looks like a complete miracle.
(Indeed, just look at the terrible proofs on Wikipedia.)

But if you use the abstract viewpoint of $T$ as a linear map,
then the theorem is a total tautology:
\begin{proof}[Proof of Cayley-Hamilton]
	Suppose we write $V$ in Jordan normal form as
	\[ V = J_1 \oplus \dots \oplus J_m \]
	where $J_i$ has eigenvalue $\lambda_i$ and dimension $d_i$.
	By definition,
	\[ p_T(X) = (X - \lambda_1)^{d_1} (X - \lambda_2)^{d_2} \dots (X - \lambda_m)^{d_m}. \]
	By definition, $(X - \lambda_1)^{d_1}$ is the zero map on $J_1$.
	So $T$ is zero on $J_1$.
	Similarly it's zero on each of the other $J_i$'s --- end of story.
\end{proof}

Also, a quick fun fact: all of this actually gives us another way to show the trace
is independent from the actual matrix, as follows.
\begin{exercise}
	Let $T : V \to V$, and suppose $p_T(X) = X^n + c_{n-1}X^{n-1} + \dots + c_0$.
	Show that in any basis, the sum of the diagonal entries of $T$ is $-c_{n-1}$.
	Moreover, $\det T = (-1)^n c_0$ is the product of the eigenvalues.
\end{exercise}
The fact that $\det T$ is the \emph{product} of the eigenvalues should
actually not be surprising in retrospect.
Suppose $T : \RR^2 \to \RR^2$ has a basis of eigenvectors $e_1$ and $e_2$
with eigenvalues $4$ and $5$; hence $T(e_1) = 4e_1$ and $T(e_2) = 5e_2$,
and so of course we would say that $T$ stretches areas by a factor of $20$!

\section{(Digression) Tensoring up}
Earlier I said that we'll generally just work over algebraically closed fields,
because, for example, a real matrix can be interpreted as a complex matrix whose
entries just happen to be real numbers.
I just want to briefly tell you, in terms of tensor products, exactly what we have done.

Let's take the space $V = \RR^3$, with basis $e_1$, $e_2$, $e_3$.
Thus objects in $V$ are of the form
\[ r_1 e_1 + r_2 e_2 + r_3 e_3 \]
where $r_1$, $r_2$, $r_3$ are real numbers.
We want to consider essentially the same vector space,
but with complex coefficients $z_i$ rather than real coefficients $r_i$.

So here's what we do: view $\CC$ as a $\RR$-vector space (with basis $\{1,i\}$, say)
and consider the \vocab{complexification}
\[ V_\CC \defeq \CC \largeotimes{\RR} V. \]
Then you can check that our elements are actually of the form
\[ z_1 \otimes e_1 + z_2 \otimes e_2 + z_3 \otimes e_3. \]
(Here, the tensor product is over $\RR$, so we have $z \otimes re_i = (zr) \otimes e_i$ for $r \in \RR$.)
Then $V_{\CC}$ can be thought as a vector space over $\CC$, since you can multiply on the left
by complex numbers.
In this way, the tensor product lets us ``fuse on'' complex coefficients even though technically
we shouldn't be able to do so.

If $T : V \to W$, is a map, then $T_\CC : V_\CC \to W_\CC$
is just the map $z \otimes v \mapsto z \otimes T(v)$.
You'll see this written sometimes as $T_\CC = \id \otimes T$.

\section\problemhead
\begin{problem}
	For a matrix $X$, we define the exponential map by
	\[ \exp(X) = 1 + X + \frac{X^2}{2!} + \frac{X^3}{3!} + \dots \]
	(take it for granted that this converges to a matrix).
	Prove that
	\[ \det(\exp(X)) = e^{\Tr X}. \]
	\begin{hint}
		This is actually immediate by taking any basis
		in which $X$ is upper-triangular.
	\end{hint}
\end{problem}

\begin{problem}
	[Putnam 1988]
	\gim
	Let $V$ be an $n$-dimensional vector space.
	Let $T : V \to V$ be a linear map and suppose there exists $n+1$ eigenvectors,
	any $n$ of which are linearly independent.
	Does it follow that $T$ is a scalar multiple of the identity?
	\begin{hint}
		Look at the trace of $T$.
	\end{hint}
\end{problem}

\begin{problem}
	[Putnam 2015]
	\yod
	Define $S$ to be the set of real matrices 
	$\left(\begin{smallmatrix} a & b \\ c & d \end{smallmatrix}\right)$
	such that $a$, $b$, $c$, $d$ form
	an arithmetic progression in that order.
	Find all $M \in S$ such that for some integer $k > 1$, $M^k \in S$.
	\begin{hint}
		There is a family of solutions other than just $a=b=c=d$.

		One can solve the problem using Cayley-Hamilton.
		A more ``bare-hands'' approach is to
		show the matrix is invertible (unless $a=b=c=d$)
		and then diagonalize the matrix as
		$
		M =
		\begin{bmatrix} s & -q \\ -r & p \end{bmatrix}
		\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}
		\begin{bmatrix} p & q \\ r & s \end{bmatrix}
		=
		\begin{bmatrix} 
			ps\lambda_1 - qr\lambda_2 & qs(\lambda_1-\lambda_2) \\
			pr(\lambda_2-\lambda_1) & ps\lambda_2 - qr\lambda_1
		\end{bmatrix} 
		$.
	\end{hint}
	\begin{sol}
		The answer is 
		\[ \begin{bmatrix} t&t\\t&t \end{bmatrix}
			\quad\text{and}\quad 
			\begin{bmatrix} -3t&-t\\t&3t \end{bmatrix} \]
		for $t \in \RR$.
		These work by taking $k=3$.

		Now to see these are the only ones, consider an arithmetic matrix
		\[ M = \begin{bmatrix} a & a+e \\ a+2e & a+3e \end{bmatrix}. \]
		with $e \neq 0$.
		Its characteristic polynomial is $t^2 - (2a+3e)t - 2e^2$,
		with discriminant $(2a+3e)^2 + 8e^2$,
		so it has two distinct real roots; moreover, since $-2e^2 \le 0$
		either one of the roots is zero or they are of opposite signs.
		Now we can diagonalize $M$ by writing
		\[
			M =
			\begin{bmatrix} s & -q \\ -r & p \end{bmatrix}
			\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}
			\begin{bmatrix} p & q \\ r & s \end{bmatrix}
			=
			\begin{bmatrix} 
				ps\lambda_1 - qr\lambda_2 & qs(\lambda_1-\lambda_2) \\
				pr(\lambda_2-\lambda_1) & ps\lambda_2 - qr\lambda_1
			\end{bmatrix} 
		\]
		where $ps-qr=1$. By using the fact the diagonal entries have sum equalling
		the off-diagonal entries, we obtain that
		\[ (ps-qr)(\lambda_1+\lambda_2) = (qs-pr)(\lambda_1-\lambda_2) 
			\implies qs-pr = \frac{\lambda_1+\lambda_2}{\lambda_1-\lambda_2}. \]
		Now if $M^k \in S$ too then the same calculation gives
		\[ qs-pr = \frac{\lambda_1^k+\lambda_2^k}{\lambda_1^k-\lambda_2^k}. \]
		Let $x = \lambda_1/\lambda_2 < 0$ (since $-2e^2 < 0$). We appropriately get
		\[ \frac{x+1}{x-1} = \frac{x^k+1}{x^k-1}
			\implies \frac{2}{x-1} = \frac{2}{x^k-1}
			\implies x = x^k \implies x = -1 \text{ or } x = 0 \]
		and $k$ odd. If $x=0$ we get $e=0$ and if $x=-1$ we get $2a+3e=0$,
		which gives the curve of solutions that we claimed.

		A slicker approach is to note that by Cayley-Hamilton.
		Assume that $e \neq 0$, so $M$ has two distinct real eigenvalues as above.
		We have $M^k = cM + d\id$ for some constants $c$ and $d$
		(since $M$ satisfies some quadratic polynomial).
		Since $M \in S$, $M^k \in S$ we obtain $d=0$.
		Thus $M^k = cM$, so it follows the eigenvalues of $M$ are negatives of each other.
		That means $\Tr M = 0$, and the rest is clear.
	\end{sol}
\end{problem}

\endinput
\section{Reducing to the nilpotent case}
\begin{definition}
	Let $T : V \to V$. A subspace of $W$ is \vocab{$T$-invariant}
	if $T(w) \in W$ for any $w \in W$.
	In this way, $T$ can be thought of as a map $W \to W$.
\end{definition}
Visually as a matrix, if $V = W_1 \oplus W_2$ and $W_1$ is $T$-invariant, that means we have zero's in the other rows for the columns associated to $W_1$.
\[ 
	\begin{bmatrix}
		\multicolumn{2}{c|}{\multirow{2}{*}{$W_1$}} &  &  &  \\ 
		\multicolumn{2}{c|}{} &  &  &  \\ \cline{1-2}
		0 & 0 & \multicolumn{3}{c}{\multirow{3}{*}{$W_2$}} \\
		0 & 0 & \multicolumn{3}{c}{} \\
		0 & 0 & \multicolumn{3}{c}{} 
	\end{bmatrix}
\]

\begin{ques}
	Let $\lambda \in k$.
	Show that a subspace $W$ is $T$-invariant if and only if it is $(T - \lambda I)$-invariant.
\end{ques}

Our goal is to prove that we can decompose $V$ as
\[ V = \bigoplus_\lambda V^{\lambda} \]
for some choices of $\lambda$, such that each $V^\lambda$ is $T$-invariant,
and moreover $T-\lambda \id$ is nilpotent on it.
(As I said earlier, each $V^\lambda$ is called a \emph{generalized eigenspace}.)
If we succeed in doing so, then we can use Nilpotent Jordan to finish.

The idea behind this proof is to consider ``polynomials'' in $T$.
For example, $2T^2-4T+5$ would be shorthand for $T(T(v)) - 4T(v) + 5$.
In this way we can consider ``polynomials'' $P(T)$;
this lets us tie in the ``algebraically closed'' condition.
These polynomial behave nicely: we have $P(T)+Q(T) = (P+Q)T$ and $P(T) \circ Q(T) = (P \circ Q)(T)$.
In fact, we get the very nice bonus property
\[ P(T) \circ Q(T) = Q(T) \circ P(T) \]
which is otherwise completely non-obvious!

Specifically, consider the infinite sequence
\[ 1, T, T^2, T^3, \dots. \]
Since the space of maps $V \to V$ is itself finite dimensional (at most $n^2$; why?) then there is some linear dependency.
So there exists a polynomial $P$ with coefficients in $k$ such that $P(T)$ is the zero map:
algebraically, this says $V = \ker P(T)$.

Since $k$ is algebraically closed, we now know that $P(x)$ can be factored completely in $k$:
\[ P(x) = \left( x - \lambda_1 \right)^{t_1} \left( x - \lambda_2 \right)^{t_2} \dots (x - \lambda_s)^{t_s}. \]
But let's take this a bit more slowly.
Suppose that $s \ge 2$, meaning $P(x) = A_1(x) A_2(x)$ for relatively prime $A_1$ and $A_2$.
Then by Bezout's lemma, there are ``dummy'' polynomials $D_1(x)$ and $D_2(x)$ such that
\[ 1 = D_1(x) A_1(x) + D_2(x) A_2(x). \]
Plugging in $x=T$, we get
\[ \id_V = D_1(T) \circ A_1(T) + D_2(T) \circ A_2(T). \]
Now let $V_1 = \ker A_1(T)$ and $V_2 = \ker A_2(T)$.

We claim that $V = V_1 \oplus V_2$.
\begin{ques}
	Show that $V_1 \cap V_2 = \{0\}$; that is,
	if $A_1(T)(v) = A_2(T)(v) = 0$, then $v=0$.
\end{ques}
Moreover, for any $v \in V$, $v = (A_1 \cdot D_1)(T)(v) + (A_2 \cdot D_2)(T)(V)$.
\begin{ques}
	Show that $(A_1 \cdot D_1)(T)(v) \in V_2$
	and similarly $(A_2 \cdot D_2)(T)(v) \in V_1$.
\end{ques}
Finally, $V_1$ and $V_2$ are $T$-invariant, as $(A_1(T) \cdot T)(v) = (T \cdot A_1(T))(v)$.

Thus, we deduce
\[ V = V_1 \oplus V_2 \]
is an indecomposable representation.
We can repeat this procedure on $\ker A_1$
