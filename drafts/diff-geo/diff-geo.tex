\documentclass[11pt]{scrreprt}
\input{../../tex/preamble}
\addbibresource{../../references.bib}
\def\asydir{}

\begin{document}
\title{Differential Geometry}
\maketitle



\chapter{Multivariable Calculus Done Correctly}
As I have ranted about before, linear algebra is done wrong
by the extensive use of matrices to obscure the structure of a linear map.
Similar problems occur with multivariable calculus, so here I would like to set 
the record straight.

Once that's done, I will tell you what a differential form is,
and you'll finally know all those stupid $dx$'s and $dy$'s really mean.
(They weren't just there for decoration!)

Since we are doing this chapter using morally correct linear algebra,
it's imperative you're comfortable with linear maps,
and in particular the dual space $V^\vee$ which we will repeatedly use.

\section{Preliminaries}
\prototype{$V = \RR^n$, and $\norm{(x_1, \dots, x_n)} = \sqrt{x_1^2 + \dots + x_n^2}$.}
In this chapter, all vector spaces are \vocab{normed} and finite-dimensional over $\RR$.
By ``normed'' I mean there is an ``absolute value'' function $\norm{-}_V : V \to \RR_{\ge 0}$
satisfying the triangle inequality and scaling by constants: 
\[
	\norm{v_1+v_2}_V \le \norm{v_1}_V+\norm{v_2}_V
	\qquad\text{and}\qquad
	\norm{cv}_V = c\norm{v}_V \quad\text{(for $c \ge 0$)}.
\]
In particular, $\norm{0_V}_V = 0$.

The norm can be used as a metric on $V$ (by taking $d(x,y) = \norm{x-y}$),
thus we can talk about continuous maps,  open sets, etc.
The typical example, of course, is $V = \RR^n$ as above.


\section{The Total Derivative}
\prototype{If $f(x,y) = x^2+y^2$, then $(Df)_{(x,y)} = 2x\ee_1^\vee + 2y\ee_2^\vee$.}
First, let $f : (a,b) \to \RR$.
You might recall from high school calculus that for every point $p \in \RR$,
we defined $f'(p)$ as the derivative at the point $p$ (if it existed), which we interpreted as the \emph{slope} of
the ``tangent line''.

\begin{center}
	\begin{asy}
		import graph;
		size(150,0);

		real f(real x) {return 3-2/(x+2.5);}
		graph.xaxis("$x$");
		graph.yaxis();
		draw(graph(f,-2,2,operator ..), heavygray, Arrows);

		real p = -1;
		real h = 1000 * (f(p+0.001)-f(p));
		real r = 0.9;
		draw( (p+r,f(p)+r*h)--(p-r,f(p)-r*h), red);
		dot( (p, f(p)) );
		draw( (p, f(p))--(p,0), dashed);
		dot("$p$", (p, 0), dir(-90));
		label("$f'(p)$", (p+r/2, f(p) + h*r/2), dir(115));
	\end{asy}
\end{center}

That's fine, but I claim that the ``better'' way to interpret
the derivative at that point is as a \emph{linear map},
that is, as a \emph{function}.
If $f'(p) = 1.5$,
then the derivative tells me that if I move $\eps$ away from $p$
then I should expect $f$ to change by about $1.5\eps$.
In other words,
\begin{moral}
The derivative of $f$ at $p$ approximates $f$ near $p$ by a \emph{linear function}.
\end{moral}

What about more generally?
Suppose I have a function like $f : \RR^2 \to \RR$, say 
\[ f(x,y) = x^2+y^2 \]
for concreteness or something.
For a point $p \in \RR^2$, the ``derivative'' of $f$ at $p$ ought to represent a linear map
that approximates $f$ at that point $p$.
That means I want a linear map $T : \RR^2 \to \RR$ such that
\[ f(p + v) \approx f(p) + T(v) \]
for small displacements $v \in \RR^2$.

Even more generally, if $f : U \to W$ with $U \subseteq V$ open,
then the derivative at $p \in U$ ought to be so that
\[ f(p + v) \approx f(p) + T(v) \in W. \]
(We need $U$ open so that for small enough $v$, $p+v \in U$ as well.)
In fact this is exactly what we're doing earlier with $f'(p)$ in high school.

\missingfigure{2D image}

The only difference is that, by an unfortunate coincidence,
a linear map $\RR \to \RR$ can be represented by just its slope.
And in the unending quest to make everything a number so that it can be AP tested,
we immediately forgot all about what we were trying to do in the first place
and just defined the derivative of $f$ to be a \emph{number} instead of a \emph{function}.

\begin{moral}
	The fundamental idea of Calculus is the local approximation of functions by linear functions.
	The derivative does exactly this.
\end{moral}
Jean Dieudonn\'e as quoted in \cite{ref:pugh} continues:
\begin{quote}
	In the classical teaching of Calculus, this idea is immediately obscured
	by the accidental fact that, on a one-dimensional vector space,
	there is a one-to-one correspondence between linear forms and numbers,
	and therefore the derivative at a point is defined as a number instead of a linear form.
	This \textbf{slavish subservience to the shibboleth of numerical interpretation at any cost}
	becomes much worse . . .
\end{quote}

So let's do this right.
The only thing that we have to do is say what ``$\approx$'' means, and for
this we use the norm of the vector space.
\begin{definition}
	Let $U \subseteq V$ be open.
	Let $f : U \to W$ be a continuous function, and $p \in U$.
	Suppose there exists a linear map $T : V \to W$ such that
	\[
		\lim_{\norm{v} \to 0}
		\frac{\norm{f(p + v) - f(p) - T(v)}_W}{\norm{v}_V} = 0.
	\]
	Then $T$ is the \vocab{total derivative} of $f$ at $p$.
	We denote this by $(Df)_p$, and say $f$ is \vocab{differentiable at $p$}.

	If $(Df)_p$ exists at every point, we say $f$ is \vocab{differentiable}.
\end{definition}

\begin{ques}
	Check that $V = W = \RR$, this is equivalent to the single-variable definition.
	(What are the linear maps from $V$ to $W$?)
\end{ques}
\begin{example}[Total Derivative of $f(x,y) = x^2+y^2$]
	Let $V = \RR^2$ with standard basis $\ee_1$, $\ee_2$ and let $W = \RR$,
	and let $f\left( x \ee_1 + y \ee_2 \right) = x^2+y^2$.  Let $p = a\ee_1 + b\ee_2$.
	Then, we claim that \[ (Df)_p : \RR^2 \to \RR \quad\text{by}\quad
	v \mapsto 2a \cdot \ee_1^\vee(v) + 2b \cdot \ee_2^\vee(v). \]
\end{example}
Here, the notation $\ee_1^\vee$ and $\ee_2^\vee$ makes sense,
because by definition $(Df)_p \in V^\vee$: these are functions from $V$ to $\RR$!

Let's check this manually with the limit definition.
Set $v = xe_1 + ye_2$, and note that the norm on $V$ is $\norm{(x,y)}_V = \sqrt{x^2+y^2}$
while the norm on $W$ is just the absolute value $\norm{c}_W = \left\lvert c \right\rvert$.
Then we compute
\begin{align*}
	\frac{\norm{f(p + v) - f(p) - T(v)}_W}{\norm{v}_V} 
	&= \frac{\left\lvert (a+x)^2 + (b+y)^2 - (a^2+b^2) - (2ax+2by) \right\rvert}{\sqrt{x^2+y^2}} \\
	&= \frac{x^2+y^2}{\sqrt{x^2+y^2}} = \sqrt{x^2+y^2} \\
	&\to 0
\end{align*}
as $\norm{v} \to 0$.
Thus, for $p = ae_1 + be_2$ we indeed have $(Df)_p = 2a \cdot e_1^\vee + 2b \cdot e_2^\vee$.

\begin{remark}
	As usual, differentiability implies continuity.
\end{remark}
\begin{remark}
	Although $U \subseteq V$, it might be helpful to think of vectors from $U$ and $V$
	as different types of objects (in particular, note that it's possible for $0_V \notin U$).
	The vectors in $U$ are ``inputs'' on our space
	while the vectors coming from $V$ are ``small displacements''.
	For this reason, I deliberately try to use $p \in U$ and $v \in V$ when possible.
\end{remark}

\section{The Projection Principle}
Before proceeding I need to say something really important.
\begin{theorem}[Projection Principle]
	\label{thm:project_principle}
	Let $W$ be an $n$-dimensional real vector space with basis $w_1, \dots, w_n$.
	Then there is a bijection between continuous functions $f : U \to W$ and
	$n$-tuples of continuous $f_1, f_2, \dots, f_n : U \to \RR$
	by projection onto the $i$th basis element, i.e.\ 
	\[ f(v) = f_1(v)w_1 + \dots + f_n(v)w_n. \]
\end{theorem}
\begin{proof}
	Obvious.
\end{proof}
The theorem remains true if one replaces ``continuous'' by ``differentiable'', ``smooth'', ``arbitrary'',
or most other reasonable words. Translation:
\begin{moral}
To think about a function $f : U \to \RR^{n}$,
it suffices to think about each coordinate separately.
\end{moral}
For this reason, we'll most often be interested in functions $f : U \to \RR$.
That's why the dual space $V^\vee$ is so important.

\section{Total and Partial Derivatives}
\prototype{If $f(x,y) = x^2+y^2$, then $(Df) : (x,y) \mapsto 2x\ee_1^\vee + 2y\ee_2^\vee$, and
$\fpartial fx = 2x$, $\fpartial fy = 2y$.}
Let $U \subseteq V$ be open and let $V$ have a basis $e_1$, \dots, $e_n$.
Suppose $f : U \to \RR$ is a function which is differentiable everywhere,
meaning $(Df)_p \in V^\vee$ exists for every $p$.
In that case, one can consider $Df$ as \emph{itself} a function:
\begin{align*}
	Df : U &\to V^\vee \\
	p &\mapsto (Df)_p.
\end{align*}
This is a little crazy: to every \emph{point} in $U$ we associate a \emph{function} in $V^\vee$.
We say $Df$ is the \vocab{total derivative} of $f$ at $p$,
to reflect how much information we're dealing with.

Let's apply the projection principle now to $Df$.
Since we picked a basis $e_1$, \dots, $e_n$ of $V$,
there is a corresponding dual basis
$e_1^\vee$, $e_2^\vee$, \dots, $e_n^\vee$.
The Projection Principle tells us that $Df$ can thus be thought of as just $n$ functions, so we can write
\[ Df = \psi_1 e_1^\vee + \dots + \psi_n e_n^\vee.  \]
In fact, we can even describe what the $\psi_i$ are.
\begin{definition}
	The \vocab{$i^{\text{th}}$ partial derivative} of $f : U \to \RR$, denoted 
	\[ \fpartial{f}{e_i}: U \to \RR \]
	is defined by
	\[
		\fpartial{f}{e_i} (p)
		\defeq \lim_{t \to 0} \frac{f(p + te_i) - f(v)}{t}.
	\]
\end{definition}
You can think of it as ``$f'$ along $e_i$''.
\begin{ques}
	Check that if $Df$ exists, then \[ (Df)_p(e_i) = \fpartial{f}{e_i}(p). \]
\end{ques}
\begin{remark}
	Of course you can write down a definition of $\fpartial{f}{v}$
	for any $v$ (rather than just the $e_i$).
\end{remark}

From the above remarks, we can derive that
\[
	\boxed{
	Df =
	\frac{\partial f}{\partial e_1} \cdot e_1^\vee
	+ \dots + 
	\frac{\partial f}{\partial e_n} \cdot e_n^\vee .
	}
\]
and so given a basis of $V$, we can think of $Df$ as just
the $n$ partials.
\begin{remark}
Keep in mind that each $\frac{\partial f}{\partial e_i}$ is a function from $U$ to the \emph{reals}.
That is to say,
\[
	(Df)_p =
	\underbrace{\frac{\partial f}{\partial e_1}(p)}_{\in \RR} \cdot e_1^\vee
	+ \dots + 
	\underbrace{\frac{\partial f}{\partial e_n}(p)}_{\in \RR} \cdot e_n^\vee
	\in V^\vee.
\]
\end{remark}


\begin{example}[Partial Derivatives of $f(x,y) = x^2+y^2$]
	Let $f : \RR^2 \to \RR$ by $(x,y) \mapsto x^2+y^2$.
	Then in our new language, 
	\[ Df : (x,y) \mapsto 2x \cdot \ee_1^\vee + 2y \cdot \ee_2^\vee. \]
	Thus the partials are
	\[
		\frac{\partial f}{\partial x} : (x,y) \mapsto 2x \in \RR
		\quad\text{and}\quad
		\frac{\partial f}{\partial y} : (x,y) \mapsto 2y \in \RR
	\]
\end{example}

With all that said, I haven't really said much about how to
find the total derivative itself.
For example, if I told you
\[ f(x,y) = x \sin y + x^2y^4 \]
you might want to be able to compute $Df$ without going through
that horrible limit definition I told you about earlier.

Fortunately, it turns out you already know how to compute partial derivatives,
because you had to take AP Calculus at some point in your life.
It turns out for most reasonable functions, this is all you'll ever need.
\begin{theorem}[Continuous Partials Implies Differentiable]
	Let $U \subset V$ be open and pick any basis $e_1, \dots, e_n$.
	Let $f : U \to W$ and suppose that $\fpartial{f}{e_i}$ is defined
	for each $i$ and moreover is \emph{continuous}.
	Then $f$ is differentiable and given by
	\[ Df = \sum \fpartial{f}{e_i} \cdot e_i^\vee. \]
\end{theorem}
\begin{proof}
	Not going to write out the details, but\dots
	given $v = t_1e_1 + \dots + t_ne_n$,
	the idea is to just walk from $p$ to $p+t_1e_1$, $p+t_1e_1+t_2e_2$, \dots,
	up to $p+t_1e_1+t_2e_2+\dots+t_ne_n = p+v$,
	picking up the partial derivatives on the way.
	Do some calculation.
\end{proof}

\begin{remark}
	The continuous condition cannot be dropped. The function
	\[
			f(x,y)
		=
		\begin{cases}
			\frac{xy}{x^2+y^2} & (x,y) \neq (0,0) \\
			0 & (x,y) = (0,0).
		\end{cases}
	\]
	is the classical counterexample -- the total derivative $Df$ does not exist at zero,
	even though both partials do.
\end{remark}

\begin{example}
	[Actually Computing a Total Derivative]
	Let $f(x,y) = x \sin y + x^2y^4$. Then
	\begin{align*}
		\fpartial fx (x,y) &= \sin y + y^4 \cdot 2x \\
		\fpartial fy (x,y) &= x \cos y + x^2 \cdot 4y^3.
	\end{align*}
	Then $Df = \fpartial fx \ee_1^\vee + \fpartial fy \ee_2^\vee$,
	which I won't bother to write out.
\end{example}

The example $f(x,y) = x^2+y^2$ is the same thing.
That being said, who cares about $x \sin y + x^2y^4$ anyways?

\section{A Word on Higher Derivatives}
Let $U \subseteq V$ be open, and take $f : U \to W$, so that $Df : U \to \Hom(V,W)$.

Well, $\Hom(V,W)$ can also be thought of as a normed vector space in its own right:
it turns out that one can define an operator norm on it by setting
\[ \norm{T} \defeq \sup \left\{ \frac{\norm{T(v)f}_W}{\Norm{v}_W} \mid v \neq 0_V \right\}. \] 
So $\Hom(V,W)$ can be thought of as a normed vector space as well.
Thus it makes sense to write
\[ D(Df) : U \to \Hom(V,\Hom(V,W)) \]
which we abbreviate as $D^2 f$. Dropping all doubt and plunging on,
\[ D^3f : U \to \Hom(V, \Hom(V,\Hom(V,W))). \]
I'm sorry.
As consolation, we at least know that $\Hom(V,W) \simeq V^\vee \otimes W$ in a natural way,
so we can at least condense this as
\[ D^kf : V \to (V^\vee)^{\otimes k} \otimes W \]
rather than writing a bunch of $\Hom$'s.
\begin{remark}
	If $k=2$, $W = \RR$, then $D^2f(v) \in (V^\vee)^{\otimes 2}$,
	so it can be represented as an $n \times n$ matrix, which for some reason is called Hessian.
\end{remark}
The most important property of the second derivative is that
\begin{theorem}
	[Symmetry of $Df^2$]
	Let $f : U \to W$ with $U \subseteq V$ open.
	If $(D^2f)_p$ exists at some $p \in U$, then it is symmetric, meaning
	\[ (D^2f)_p(v_1, v_2) = (D^2f)_p(v_2, v_1). \]
\end{theorem}
I'll just quote this without proof, because double derivatives make my head spin.
An important corollary of this theorem:
\begin{corollary}
	[Clairaut's Theorem: Mixed Partials are Symmetric]
	Let $f : U \to \RR$ with $U \subseteq V$ open be twice differentiable.
	Then for any point $p$ such that the quantities are defined,
	\[
		\frac{\partial}{\partial e_i}
		\frac{\partial}{\partial e_j}
		f(p)
		=
		\frac{\partial}{\partial e_j}
		\frac{\partial}{\partial e_i}
		f(p).
	\]
\end{corollary}

\section{Differential Forms}
\prototype{Algebraically, something that looks like $f \ee_1^\vee \wedge \ee_2^\vee + \dots$,
and geometrically, see the picture.}

Let's now get a handle on what $dx$ means. This turns out to be a special case of a so-called \emph{differential form}.
Fix a real vector space $V$ of dimension $n$, and let $\ee_1$, \dots, $\ee_n$ be a standard basis.

\begin{definition}
	We define a \vocab{differential $k$-form} $\alpha$ to be a smooth (infinitely differentiable)
	map $\alpha : V \to \Lambda^k(V^\vee)$.
	(Here $\Lambda^k(V^\vee)$ is the wedge product.)
\end{definition}

Like with $Df$, we'll use $\alpha_p$ instead of $\alpha(p)$.

\begin{example}
	[$k$-forms for $k=0,1$]
	\listhack
	\begin{enumerate}[(a)]
		\item A $0$-form is just a function $V \to \RR$.
		\item A $1$-form is a function $V \to V^\vee$.
		For example, the total derivative $df$ of a function $V \to \RR$ is a $1$-form.
		\item Let $V = \RR^3$ with standard basis $\ee_1$, $\ee_2$, $\ee_3$.
		Then a typical $2$-form is given by
		\[
			\alpha_p
			=
			f(p) \ee_1^\vee \wedge \ee_2^\vee
			+ g(p) \ee_1^\vee \wedge \ee_3^\vee
			+ h(p) \ee_2^\vee \wedge \ee_3^\vee
			\in \Lambda^2(V)
		\]
		where $f,g,h : V \to \RR$ are smooth functions.
	\end{enumerate}
\end{example}

Now, by the projection principle (\Cref{thm:project_principle}) we only have to specify
a function on each of $\binom nk$ basis elements of $\Lambda^k(V^\vee)$.
So, take any basis $\{e_i\}$ of $V$, and 
take the usual basis for $\Lambda^k(V^\vee)$ of elements
\[ e_{i_1}^\vee \wedge e_{i_2}^\vee \wedge \dots \wedge e_{i_k}^\vee. \]
Thus, a general $k$-form takes the shape
\[ \alpha_p = \sum_{1 \le i_1 < \dots < i_k \le n} 
	f_{i_1, \dots, i_k}(p)
	e_{i_1}^\vee \wedge e_{i_2}^\vee \wedge \dots \wedge e_{i_k}^\vee. \]
Since this is a huge nuisance to write, we will abbreviate this to just
\[ \alpha = \sum_I f_I de_I \]
where we understand the sum runs over $I = (i_1, \dots, i_k)$,
and $de_I$ represents $e_{i_1}^\vee \wedge \dots \wedge e_{i_k}^\vee$.

Now that we have an element $\Lambda^k(V^\vee)$, what can it do?
Well, first let me get the definition on the table, then tell you what it's doing.
\begin{definition}
	For linear functions $\xi_1, \dots, \xi_k \in \Lambda^k(V)$
	and vectors $v_1, \dots, v_k \in V$, we define
	\[
		(\xi_1 \wedge \dots \wedge \xi_k)(v_1, \dots, v_k)
		\defeq
		\det
		\left(
		\begin{array}{ccc}
			\xi_1(v_1) & \dots & \xi_1(v_k) \\
			\vdots & \ddots & \vdots \\
			\xi_k(v_k) & \dots & \xi_k(v_k)
		\end{array}
		\right).
	\]
	You can check that this is well-defined
	under e.g. $v \wedge w = -w \wedge v$ and so on.
\end{definition}

\begin{example}
	[Evaluation of a Differential Form]
	Set $V = \RR^3$
	Suppose that at some point $p$, the $2$-form $\alpha$ returns
	\[ \alpha_p = 2 \ee_1^\vee \wedge \ee_2^\vee + \ee_1^\vee \wedge \ee_3^\vee. \]
	Let $v_1 = 3\ee_1 + \ee_2 + 4\ee_3$ and $v_2 = 8\ee_1 + 9\ee_2 + 5\ee_3$.
	Then
	\[
		\alpha_p(v_1, v_2)
		=
		2\det \left( \begin{array}{cc}
			3 & 1 \\ 8 & 9 \end{array} \right)
		+
		\det \left( \begin{array}{cc}
			3 & 4 \\ 8 & 5 \end{array} \right)
		= 21.
	\]
\end{example}

What does this definition mean?
One way to say it is that
\begin{moral}
	If I walk to a point $p \in U$,
	a $k$-form $\alpha$ will take in $k$ vectors $v_1, \dots, v_k$
	and spit out a number, which is to be interpreted as a (signed) volume.
\end{moral}

Picture:
\begin{center}
	\begin{asy}
		bigblob("$U$");
		pair p = (-2,-2);
		dot("$p$", p, dir(225), red);
		pair p1 = p + 1.4*dir(120);
		pair p2 = p + 1.7*dir(10);
		draw(p--p1, red, EndArrow);
		draw(p--p2, red, EndArrow);
		label("$v_1$", p1, dir(p1-p), red);
		label("$v_2$", p2, dir(p2-p), red);
		label("$\alpha_p(v_1, v_2) \in \mathbb R$", p+dir(45)*3);
	\end{asy}
\end{center}

In other words, at every point $p$, we get a function $\alpha_p$.
Then I can feed in $k$ tangent vectors to $\alpha_p$ and get a number,
which I interpret as a signed volume of the parallelpiped spanned the $\{v_i\}$'s
in some way (e.g.\ the flux of a force field).
That's why $\alpha_p$ as a ``function'' is contrived to lie in the wedge product:
this ensures that the notion of ``volume'' makes sense, so that for example,
the equality $\alpha_p(v_1, v_2) = -\alpha_p(v_2, v_1)$ holds.

This is what makes differential forms good for integrating,
which we'll do in the next chapter.
Indeed,
\begin{quote}
	\noindent ``The definition of a differential form is:
	something you can integrate.'' \\ --- Joe Harris
\end{quote}
But for now, let's differentiate them.

\section{Exterior Derivatives}
\prototype{Possibly $dx_1 = \ee_1^\vee$.}
First, given a function $f : V \to \RR$,
we define 
\[ df \defeq Df = \sum_i f_i e_i^\vee \]
In particular, suppose $V = \RR^n$ and $f(x_1, \dots, x_n) = x_1$ (i.e.\ $f = \ee_1^\vee$). Then:
\begin{ques}
	Show that for any $p \in U$, \[ \left( d(\ee_1^\vee) \right)_p = \ee_1^\vee. \]
\end{ques}
\begin{abuse}
	Unfortunately, someone somewhere decided it would be a good idea to use ``$x_1$'' to denote $\ee_1^\vee$
	(because \emph{obviously}\footnote{Sarcasm.} $x_1$ means
	``the function that takes $(x_1, \dots, x_n) \in \RR^n$ to $x_1$'')
	and then decided that \[ dx_1 \defeq \ee_1^\vee. \]
	This notation is so entrenched that I have no choice but to grudgingly accept it.
	Note that it's not even right, since technically it's $(dx_1)_p = \ee_1^\vee$; $dx_1$ is a $1$-form.
	\label{abuse:dx}
\end{abuse}
\begin{remark}
	This is the reason why we use the notation $\frac{df}{dx}$ in calculus now:
	given, say $f : \RR \to \RR$ by $f(x) = x^2$, it is indeed true that
	\[ df = 2x \cdot \ee_1^\vee = 2x \cdot dx \]
	and so by abuse of notation we write $df/dx = 2x$.
\end{remark}

More generally, we can define the \vocab{exterior derivative} in terms of our basis $e_1$, \dots, $e_n$ as follows:
if $\alpha = \sum_I f_I de_I$ then we set
\[ d\alpha \defeq \sum_I df_I \wedge de_I = \sum_I \sum_j \fpartial{f_I}{e_j} de_j \wedge de_I. \]
We'll show this doesn't depend on the choice of basis in a moment.

\begin{example}[Computing some exterior derivatives]
	Let $V = \RR^3$ with standard basis $\ee_1$, $\ee_2$, $\ee_3$.
	Let $f(x,y,z) = x^4 + y^3 + 2xz$.
	Then we compute
	\[ df = Df = (4x^3+2z) \; dx + 3y^2 \; dy + 2x \; dz. \]
	Next, we can evaluate $d(df)$ as prescribed: it is
	\begin{align*}
		d^2f &= (12x^2 \; dx + dz) \wedge dx + (6y \; dy) \wedge dy + (dx \wedge dz) \\
		&= 12x^2 (dx \wedge dx) + 2(dz \wedge dx) + 6y (dy \wedge dy) + 2(dx \wedge dz) \\
		&= 2(dz \wedge dx) + 2(dx \wedge dz) \\
		&= 0.
	\end{align*}
	So surprisingly, $d^2f$ is the zero map.
	Here, we have exploited \Cref{abuse:dx} for the first time,
	in writing $dx$, $dy$, $dz$.
\end{example}
And in fact, this is always true in general:
\begin{theorem}[Exterior Derivative Vanishes]
	\label{thm:dd_zero}
	Let $\alpha$ be any $k$-form.
	Then $d^2(\alpha) = 0$.
	Even more succinctly, \[ d^2 = 0. \]
\end{theorem}
The proof is left as \Cref{prob:dd_zero}.

Here are some other properties of $d$:
\begin{itemize}
	\ii As we just saw, $d^2 = 0$.
	\ii It is bilinear; $d(c\alpha+\beta) = c \cdot d\alpha + d\beta$.
	\ii For a $k$-form $\alpha$ and $\ell$-form $\beta$, one can show that
	\[ d(\alpha \wedge \beta) = d\alpha \wedge \beta + (-1)^k (\alpha \wedge d\beta). \]
\end{itemize}
In fact, one can show that the $df$ as defined in terms of bases above is
the \emph{unique} map sending $k$-forms to $(k+1)$-forms.
Actually, one way to define $df$ is to take as axioms the bulleted properties above
and then declare $d$ to be the unique solution to this functional equation.

In any case, this tells us that our definition of $d$ does not depend on the basis chosen.

According to Wikipedia, ``if a $k$-form $\alpha$ is thought of as measuring the flux through a $k$-parallelpiped,
then $d\alpha$ measures the flux through the boundary of a $(k+1)$-parallelpiped''.
In that sense, $d^2 = 0$ is saying something like ``the boundary of the boundary is empty''.
We'll make this precise when we see Stoke's Theorem in the next chapter.

\section{Problems}
\begin{sproblem}[Chain Rule]
	Let $U_1 \taking f U_2 \taking g U_3$ be differentiable maps
	between open sets of normed vector spaces $V_i$, and let $h = g \circ f$.
	Prove the Chain Rule: for any point $p \in U_1$, we have
	\[ (Dh)_p = (Dg)_{f(p)} \circ (Df)_p. \]
\end{sproblem}
\begin{problem}
	\label{prob:dd_zero}
	Establish \Cref{thm:dd_zero}, which states that $d^2 = 0$.
	\begin{hint}
		This is just a summation.
		You will need the fact that mixed partials are symmetric.
	\end{hint}
\end{problem}
\begin{problem}
	Let $U \subseteq V$ be open, and $f : U \to \RR$ be differentiable $k$ times.
	Show that $(D^kf)_p$ is symmetric in its $k$ arguments, meaning for any $v_1, \dots, v_k \in V$
	and any permutation $\sigma$ on $\left\{ 1, \dots, k \right\}$ we have
	\[ (D^kf)_p(v_1, \dots, v_k) = (D^kf)_p(v_{\sigma(1)}, \dots, v_{\sigma(k)}). \]
	\begin{hint}
		Simply induct, with the work having been done on the $k=2$ case.
	\end{hint}
\end{problem}
\chapter{Integrating Differential Forms}
In this chapter, we'll show how to integrate differential forms over cells,
and state Stoke's Theorem in this context.

\section{Motivation: Line Integrals}
Given a function $g : [a,b] \to \RR$ we know by the Fundamental Theorem of Calculus
that it can be integrated as
\[
	\int_{[a,b]} g(t) \; dt = f(b) - f(a)
\]
where $f$ is a function such that $\frac{df}{dt} = g$.
(You might recognize this more readily as the more customary $\int_a^b g(t) \; dt$.)
Put another way, given a function $f : (a,b) \to \RR$,
we can write
\[ \int_{[a,b]} g \; dt = \int_{[a,b]} df = f(b) - f(a) \]
where $df$ is the exterior derivative we defined earlier.

Cool, so we can integrate over $[a,b]$.
Now, suppose more generally, we have $U$ an open subset of our real vector space $V$
and a $1$-form $\alpha : U \to V^\vee$.
We consider a \vocab{parametrized curve}, which is a smooth function $c : [a,b] \to U$.
Picture:
\begin{center}
	\begin{asy}
		size(9cm);
		pair A = (-13,0);
		pair B = (-9,0);
		draw(A--B, grey);
		dot(A, grey); dot(B, grey);
		label("$[a,b]$", A--B, dir(90), grey);
		dot("$t$", 0.3*A+0.7*B, dir(-90), grey);

		draw( (-8,0) -- (-6,0) , EndArrow);
		label("$c$", (-7,0), dir(90));

		bigblob("$U$");
		pair a = (-2,-2);
		pair b = (3,0);
		pair p = (0,1);
		pair q = (2,0);
		label("$c$", q, dir(45), red);
		draw(a..p..q..b, red);
		dot("$p = c(t)$", p, dir(90), blue);
		draw(p--(p+1.5*dir(-10)), blue, EndArrow);

		draw( (0,-4)--(0,-8), EndArrow );
		label("$\alpha$", (0,-6), dir(0));
		label("$\alpha_p(v) \in \mathbb R$", (0,-9), heavygreen);

		draw( (-10,-1)--(-1,-8), EndArrow);
		label("$c^\ast \alpha$", (-5.5,-4.5), dir(225));
	\end{asy}
\end{center}

We want to define an $\int_c \alpha$ satisfying the following geometric interpretation:
\begin{moral}
	The integral $\int_c \alpha$ should add up all the $\alpha$ along the curve $c$.
\end{moral}
Our differential form $\alpha$ first takes in a point $p$ to get $\alpha_p \in V^\vee$.
Then, it eats a tangent vector $v \in V$ to the curve $c$ to finally give a real number $\alpha_p(v) \in \RR$.
We would like to ``add all these numbers up'', using only the notion of an integral over $[a,b]$.

\begin{exercise}
	Try to guess what the definition of the integral should be.
	(By type-checking, there's only one reasonable answer.)
\end{exercise}

So, the definition we give is
\[
	\int_c \alpha
	\defeq
	\int_{[a,b]}
	\alpha_{c(t)} \left( c'(t) \right) \; dt.
\]
Here, $c'(t)$ is shorthand for $(Dc)_{c(t)}(1)$.
It represents the \emph{tangent vector} to the curve $c$ at the point $p=c(t)$, at time $t$.
(Here we are taking advantage of the fact that $[a,b]$ is one-dimensional).

Now that definition was a pain to write, so we will define a differential
$1$-form $c^\ast \alpha$ on $[a,b]$ to swallow that entire thing:
specifically, in this case we define $c^\ast\alpha$ to be
\[ \left( c^\ast \alpha \right)_t (\eps) = (Dc)_{c(t)} (\eps) \]
(here $\eps$ is some displacement in time).
Thus, we can more succinctly write
\[
	\int_c \alpha
	\defeq
	\int_{[a,b]}
	\alpha_{c(t)} \left( c'(t) \right) \; dt.
\]
This is a special case of a \emph{pullback}:
roughly, if $\phi : U \to U'$ (where $U \subseteq V$, $U' \subseteq V'$),
we can change any differential $k$-form $\alpha$ on $U'$
to a $k$-form on $U$.
In particular, if $U = [a,b]$,\footnote{
OK, so $[a,b]$ isn't actually open, sorry. I ought to write $(a-\eps, b+\eps)$, or something.}
we can resort to our old definition of an integral.
Let's now do this in full generality.

\section{Pullbacks}
Let $V$ and $V'$ be finite dimensional real vector spaces (possibly different dimensions)
and suppose $U$ and $U'$ are open subsets of each;
next, consider a $k$-form $\alpha$ on $U'$.

Given a map $\phi : U \to U'$ we now want to define a pullback in much the same way as before.
Picture:
\begin{center}
	\begin{asy}
		size(13cm);
		bigblob("$U$");
		pair p = (-1,0);
		dot("$p$", p, dir(90), red);
		pair p1 = p + 1.4*dir(150);
		pair p2 = p + 1.7*dir(-50);
		draw(p--p1, red, EndArrow);
		draw(p--p2, red, EndArrow);

		add(scale(0.8)*shift(14*dir(180))*CC());
		bigblob("$U'$");
		pair q = (-0.5,0.5);
		dot("$q = \phi(p)$", q, dir(90), blue);
		pair q1 = q + 1.8*dir(-100);
		pair q2 = q + 2.3*dir(-10);
		draw(q--q1, blue, EndArrow);
		draw(q--q2, blue, EndArrow);

		draw((-9,0)--(-3,0), EndArrow);
		label("$\phi$", (-6,0), dir(90));
	
		draw( (0,-4)--(0,-8), EndArrow );
		label("$\alpha$", (0,-6), dir(0));
		label("$\alpha_q(\dots) \in \mathbb R$", (0,-9), heavygreen);

		draw( (-11,-3)--(-1,-8), EndArrow);
		label("$\phi^\ast \alpha$", (-6,-6), dir(225));
	\end{asy}
\end{center}

Well, there's a total of about one thing we can do.
Specifically: $\alpha$ accepts a point in $U'$ and $k$ tangent vectors in $V'$,
and returns a real number.
We want $\phi^\ast \alpha$ to accept a point in $p \in U$
and $k$ tangent vectors $v_1, \dots, v_k$ in $V$,
and feed the corresponding information to $\alpha$

Clearly we give the point $q = \phi(p)$.
As for the tangent vectors, since we are interested in volume, we take the
derivative of $\phi$ at $p$, $(D\phi)_p$, which will scale each of our vectors $v_i$
into some vector in the target $V'$.
To cut a long story short:
\begin{definition}
	Given $\phi : U \to U'$ and $\alpha$ a $k$-form, we define the \vocab{pullback}
	\[
		(\phi^\ast \alpha)_p(v_1, \dots, v_k)
		\defeq \alpha_{\phi(p)}
		\left( (D\phi)_p(v_1), \dots, (D\phi)_p(v_k) \right).
	\]
\end{definition}

There is a more concrete way to define the pullback using bases.
Suppose $w_1, \dots, w_n$ is a basis of $V'$
and $e_1, \dots, e_m$ is a basis of $V$.
Thus, by the projection principle (\Cref{thm:project_principle}) 
the map $\phi : V \to V'$ can be thought of as
\[ \phi(v) = \phi_1(v) w_1 +  \dots \phi_n(v) w_n \]
where each $\phi_i$ takes in a $v \in V$ and returns a real number.
We know also that $\alpha$ can be written concretely as
\[ \alpha = \sum_{J \subseteq \{1, \dots, n\}} f_J w_J. \]
Then, we define
\[
	\phi^\ast\alpha
	= \sum_{I \subseteq \{1, \dots, m\}}
	(f_I \circ \phi) (D\phi_{i_1} \wedge \dots \wedge D\phi_{i_k}).
\]
A diligent reader can check these definitions are equivalent.
\begin{example}
	[Computation of a Pullback]
	Let $V = \RR^2$ with basis $\ee_1$ and $\ee_2$,
	and suppose $\phi : V \to V'$ is given by sending
	\[ \phi(a\ee_1 + b\ee_2) = (a^2+b^2)w_1 + \log(a^2+1) w_2 + b^3 w_3 \]
	where $w_1$, $w_2$, $w_3$ is a basis for $V'$.
	Consider the form $\alpha_q = f(q) w_1 \wedge w_3$, where $f : V' \to \RR$.
	Then
	\[ (\phi^\ast\alpha)_p = f(\phi(p)) \cdot (2a \ee_1^\vee + 2b\ee_2^\vee) \wedge (3b^2 \ee_2^\vee)
		= f(\phi(p)) \cdot 6ab^2 \cdot \ee_1^\vee \wedge \ee_2^\vee. \]
\end{example}

It turns out that the pullback basically behaves nicely as possible, e.g.\
\begin{itemize}
	\ii $\phi^\ast(c\alpha + \beta) = c\phi^\ast \alpha + \phi^\ast\beta$ (bilinearity)
	\ii $\phi^\ast(\alpha\wedge\beta)
	= (\phi^\ast \alpha)\wedge(\phi^\ast \beta)$
	\ii $\phi_1^\ast(\phi_2^\ast(\alpha)) 
	= (\phi_1 \circ \phi_2)^\ast(\alpha)$ (naturality)
\end{itemize}
but I won't take the time to check these here
(one can verify them all by expanding with a basis).
	
\section{Cells}
\prototype{The circle in $\RR^2$ can be thought of as the cell $[0,R]\times[0,2\pi] \to \RR^2$ by
$(r,\theta) \mapsto (r\cos\theta)\ee_1 + (r\sin\theta)\ee_2$.}
Now that we have the notion of a pullback,
we can define the notion of an integral for more general spaces.
Specifically, to generalize the notion of integrals we had before:
\begin{definition}
	A \vocab{$k$-cell} is a smooth function $c : [a_1, b_1] \times [a_2,b_2] \times \dots [a_k, b_k] \to V$.
\end{definition}
\begin{example}
	[Examples of Cells]
	Let $V = \RR^2$ for convenience.
	\begin{enumerate}[(a)]
		\ii A $0$-cell consists of a single point.
		\ii As we saw, a $1$-cell is an arbitrary curve.
		\ii A $2$-cell corresponds to a $2$-dimensional surface.
		For example, the map $c : [0,R] \times [0,2\pi]$ by
		\[ c : (r,\theta) \mapsto (r\cos\theta, r\sin\theta) \]
		can be thought of as a disk of radius $R$.
	\end{enumerate}
\end{example}
Then, to define an integral
\[ \int_c \alpha \]
for a differential $k$-form $\alpha$ and a $k$-cell $c : [0,1]^k$, we simply take the pullback
\[ \int_{[0,1]^k} c^\ast \alpha \]
Since $c^\ast \alpha$ is a $k$-form on the $k$-dimensional unit box,
it can be written as $f(x_1, \dots, x_n) \; dx_1 \wedge \dots \wedge dx_n$,
so the above integral can be written as
\[ \int_0^1 \dots \int_0^1 f(x_1, \dots, x_n) \; dx_1 \wedge \dots \wedge dx_n \]

\begin{example}[Area of a Circle]
	Consider $V = \RR^2$ and let $c : (r,\theta) \mapsto (r\cos\theta)\ee_1 + (r\sin\theta)\ee_2$
	on $[0,R] \times [0,2\pi]$ as before.
	Take the $2$-form $\alpha$ which gives $\alpha_p = \ee_1^\vee \wedge \ee_2^\vee$ at every point $p$.
	Then
	\begin{align*}
		c^\ast\alpha &= 
		\left( \cos\theta dr - r\sin\theta d\theta \right)
		\wedge
		\left( \sin\theta dr + r\cos\theta d\theta \right) \\
		&= r(\cos^2\theta+\sin^2\theta) (dr \wedge d\theta) \\
		&= r \; dr \wedge d\theta
	\end{align*}
	Thus,
	\[ \int_c \alpha
		= \int_0^R \int_0^{2\pi} r \; dr \wedge d\theta
		= \pi r^2 \]
	which is the area of a circle.
\end{example}

Here's some geometric intuition for what's happening.
Given a $k$-cell in $V$, a differential $k$-form $\alpha$ accepts a point $p$ and some tangent vectors $v_1$, \dots, $v_k$
and spits out a number $\alpha_p(v_1, \dots, v_k)$,
which as before we view as a signed hypervolume.
Then the integral \emph{adds up all these infinitesimals across the entire cell}.
In particular, if $V = \RR^k$ and we take the form $\alpha : p \mapsto \ee_1^\vee \wedge \dots \wedge \ee_k^\vee$,
then what these $\alpha$'s give is the $k$th hypervolume of the cell.
For this reason, this $\alpha$ is called the \vocab{volume form} on $\RR^k$.

You'll notice I'm starting to play loose with the term ``cell'':
while the cell $c : [0,R] \times [0,2\pi] \to \RR^2$ is supposed to be a function
I have been telling you to think of it as a unit disk (i.e.\ in terms of its image).
In the same vein, a curve $[0,1] \to V$ should be thought of as a curve in space,
rather than a function on time.

This error turns out to be benign.
Let $\alpha$ be a $k$-form on $U$ and $c : [a_1, b_1] \times \dots \times [a_k, b_k] \to U$ a $k$-cell.
Suppose $\phi : [a_1', b_1'] \times \dots [a_k', b_k'] \to [a_1, b_1] \times \dots \times [a_k, b_k]$;
it is a \vocab{reparametrization} if $\phi$ is bijective and $(D\phi)_p$ is always invertible
(think ``change of variables'');
thus
\[ c \circ \phi : [a_1', b_1'] \times \dots \times [a_k',b_k'] \to U \]
is a $k$-cell as well.
Then it is said to be \vocab{preserve orientation} if $\det(D\phi)_p > 0$ for all $p$
and \vocab{reverse orientation} if $\det(D\phi)_p < 0$ for all $p$.
\begin{exercise}
	Why is it that exactly one of these cases must occur?
\end{exercise}

\begin{theorem}
	[Changing Variables Doesn't Affect Integrals]
	Let $c$ be a $k$-cell, $\alpha$ a $k$-form, and $\phi$ a reparametrization.
	Then
	\[ \int_{c \circ \phi} \alpha
		=
		\begin{cases}
			\int_c \alpha & \phi \text{ preserves orientation} \\
			- \int_c \alpha & \phi \text{ reverses orientation}.
		\end{cases}
	\]
\end{theorem}
\begin{proof}
	Use naturality of the pullback to reduce it to the corresponding
	theorem in normal calculus.
\end{proof}

So for example, if we had parametrized the unit circle as $[0,1] \times [0,1] \to \RR^2$
by $(r,t) \mapsto R\cos(2\pi t) \ee_1 + R\sin(2\pi t) \ee_2$, we would have arrived at the same result.
So we really can think of a $k$-cell just in terms of the points it specifies.

\section{Boundaries}
\prototype{The boundary of $[a,b]$ is $\{b\}-\{a\}$. The boundary of a square goes around its edge counterclockwise.}
First, I introduce a technical term that lets us consider multiple cells at once.
\begin{definition}
	A \vocab{$k$-chain} $U$ is a formal linear combination of $k$-cells over $U$,
	i.e. a sum of the form
	\[ c = a_1 c_1 + \dots + a_m c_m \]
	where each $a_i \in \RR$ and $c_i$ is a $k$-cell.
	We define $\int_c \alpha = \sum_i a_i \int c_i$.
\end{definition}
In particular, a $0$-cell consists of several points, each with a given weight.

Now, how do we define the boundary?
For a $1$-cell $[a,b] \to U$, as I hinted earlier we want the answer to be the $0$-chain $\{c(b)\}-\{c(a)\}$.
Here's how we do it in general.
\begin{definition}
	Suppose $c : [0,1]^k \to U$ is a $k$-cell.
	Then the \vocab{boundary} of $c$, denoted $\partial c : [0,1]^{k-1} \to U$,
	is the $(k-1)$-chain defined as follows.
	For each $i = 1,\dots,k$ define
	\begin{align*}
		c_i^{\text{start}}(t_1, \dots, t_{k-1}) &
		= (t_1, \dots, t_{i-1}, 0, t_i, \dots, t_k) \\
		c_i^{\text{stop}}(t_1, \dots, t_{k-1}) &
		= (t_1, \dots, t_{i-1}, 1, t_i, \dots, t_k).
	\end{align*}
	Then
	\[ \partial c \defeq
	\sum_{i=1}^k (-1)^{i+1} \left( c_i^{\text{stop}} - c_i^{\text{start}}  \right). \]
	Finally, the boundary of a chain is the sum of the boundaries of each cell (with the appropriate weights).
	That is, $\partial(\sum a_ic_i) = \sum a_i \partial c_i$.
\end{definition}
\begin{ques}
	Satisfy yourself that one can extend this definition to
	a $k$-cell $c$ defined on $c : [a_1, b_1] \times \dots \times [a_k, b_k] \to V$
	(rather than from $[0,1]^k \to V$).
\end{ques}

\begin{example}
	[Examples of Boundaries]
	Consider the $2$-cell $c : [0,1]^2 \to \RR^2$ shown below.
	\begin{center}
		\begin{asy}
			size(7cm);
			pen e1 = heavyred;
			pen e2 = orange;
			pen e3 = olive;
			pen e4 = heavymagenta;
			draw((0,0)--(2,0), e1, EndArrow);
			draw((2,0)--(2,2), e2, EndArrow);
			draw((2,2)--(0,2), e3, EndArrow);
			draw((0,2)--(0,0), e4, EndArrow);
			label(scale(0.8)*"$[0,1]^2$", (1,1));
			draw( (3,1)--(6,1), EndArrow);
			label("$c$", (4.5,1), dir(90));
			pair p1 = (7,-1);
			pair p2 = (12,-2);
			pair p3 = (11,3);
			pair p4 = (8,2);
			fill(p1--p2--p3--p4--cycle, palecyan);
			draw(p1--p2, e1, EndArrow, Margins);
			draw(p2--p3, e2, EndArrow, Margins);
			draw(p3--p4, e3, EndArrow, Margins);
			draw(p4--p1, e4, EndArrow, Margins);
			dot("$p_1$", p1, dir(225), blue+4);
			dot("$p_2$", p2, dir(315), blue+4);
			dot("$p_3$", p3, dir( 45), blue+4);
			dot("$p_4$", p4, dir(135), blue+4);
			label("$c$", (p1+p2+p3+p4)/4);
		\end{asy}
	\end{center}
	Here $p_1$, $p_2$, $p_3$, $p_4$ are the images of $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$, respectively.
	Then we can think of $\partial c$ as
	\[ \partial c = [p_1,p_2] + [p_2,p_3] + [p_3,p_4] + [p_4,p_1] \]
where each ``interval'' represents the $1$-cell shown by the reddish arrows on the right.
	We can take the boundary of this as well, and obtain an empty chain as
	\[ \partial(\partial c) = \sum_{i=1}^4 \partial([p_i, p_{i+1}]) = \sum_{i=1}^4 \{p_{i+1}\}-\{p_i\} = 0. \]
\end{example}

\begin{example}
	[Boundary of a Unit Disk]
	Consider the unit disk given by
	\[ c : [0,1] \times [0,2\pi] \to \RR^2 \quad\text{by}\quad
	(r,\theta) \mapsto s\cos(2\pi t)\ee_1 + s\sin(2\pi t)\ee_2. \]
	The four parts of the boundary are shown in the picture below:
	\begin{center}
		\begin{asy}
			size(7cm);
			pen e1 = heavyred;
			pen e2 = orange;
			pen e3 = olive;
			pen e4 = heavymagenta;
			draw((0,0)--(2,0), e1, EndArrow);
			draw((2,0)--(2,2), e2, EndArrow);
			draw((2,2)--(0,2), e3, EndArrow);
			draw((0,2)--(0,0), e4, EndArrow);
			label("$r$", (2,0), dir(-45));
			label("$\theta$", (0,2), dir(135));

			label(scale(0.8)*"$[0,1]^2$", (1,1));
			draw( (3,1)--(6,1), EndArrow);
			label("$c$", (4.5,1), dir(90));

			pair O = (9,1);
			pair P = O + 2*dir(0);
			fill(CP(O,P), palecyan);
			real eps = 0.3;
			draw(shift(0,eps) * (O--P), e1, EndArrow, Margins);
			draw(shift(0,-eps) * (P--O), e3, EndArrow, Margins);
			draw(CP(O,P), e2, EndArrow, Margins);
			dot(O, e4+4);
		\end{asy}
	\end{center}
	Note that two of the arrows more or less cancel each other out when they are integrated.
	Moreover, we interestingly have a \emph{degenerate} $1$-cell at the center of the circle;
	it is a constant function $[0,1] \to \RR^2$ which always gives the origin.
\end{example}

Obligatory theorem, analogous to $d^2=0$ and left as a problem.
\begin{theorem}[The Boundary of the Boundary is Empty]
	$\partial^2 = 0$, in the sense that for any $k$-chain $c$ we have $\partial^2(c) = 0$.
\end{theorem}

\section{Stoke's Theorem}
\prototype{$\int_{[a,b]} dg = g(b) - g(a)$.}

We now have all the ingredients to state Stoke's Theorem for cells.
\begin{theorem}
	[Stoke's Theorem for Cells]
	Take $U \subseteq V$ as usual, let $c : [0,1]^k \to U$ be a $k$-cell
	and let $\alpha : U \to \Lambda^k(V)$ be a $k$-form.
	Then
	\[ \int_c d\alpha = \int_{\partial c} \alpha. \]
	In particular, if $d\alpha = 0$ then the left-hand side vanishes.
\end{theorem}
For example, if $c$ is the interval $[a,b]$ then $\partial c = \{b\} - \{a\}$,
and thus we obtain the Fundamental Theorem of Calculus.

\section{Problems}
\begin{dproblem}[Green's Theorem]
	Let $f,g : \RR^2 \to \RR$ be smooth functions.
	Prove that
	\[ \int_c \left( \fpartial gx - \fpartial fy \right) \; dx \wedge dy
	= \int_{\partial c} (f \; dx + g \; dy). \]
	\begin{hint}
		Direct application of Stoke's Theorem to $\alpha = f \; dx + g \; dy$.
	\end{hint}
\end{dproblem}

\begin{problem}
	Show that $\partial^2 = 0$.
	\begin{hint}
		This is just an exercises in sigma notation.
	\end{hint}
\end{problem}

\begin{problem}[Pullback and $d$ Commute]
	Let $U$ and $U'$ be open sets of vector spaces $V$ and $V'$
	and let $\phi : U \to U'$ be a smooth map between them.
	Prove that for any differential form $\alpha$ on $U'$ we have
	\[ \phi^\ast(d\alpha) = d(\phi^\ast\alpha). \]
	\begin{hint}
		This is a straightforward (but annoying) computation.
	\end{hint}
\end{problem}

\begin{problem}[Arc Length Isn't a Form]
	Show that there does \emph{not} exist a $1$-form $\alpha$ on $\RR^2$ such that
	for a curve $c : [0,1] \to \RR^2$,
	the integral $\int_c \alpha$ gives the arc length of $c$.
	\begin{hint}
		We would want $\alpha_p(v) = \norm{v}$.
	\end{hint}
\end{problem}

\begin{problem}
	\gim
	An \vocab{exact} $k$-form $\alpha$ is one satisfying $\alpha = d\beta$ for some $\beta$.
	Prove that
	\[ \int_{C_1} \alpha = \int_{C_2} \alpha \]
	where $C_1$ and $C_2$ are any concentric circles in the plane
	and $\alpha$ is some exact $1$-form.
	\begin{hint}
		Show that $d^2=0$ implies $\int_{\partial c} \alpha = 0$ for exact $\alpha$.
		Draw an annulus.
	\end{hint}
\end{problem}
\end{document}
