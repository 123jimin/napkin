\documentclass[11pt]{scrreprt}
\input{../../tex/preamble}
\def\asydir{}
\addbibresource{../../references.bib}
\renewcommand{\gim}{($\ast$)}


\begin{document}
\title{Representation Theory}
\maketitle

\chapter{Representations of Algebras}
In the 19th century, a group was literally defined
as a subset of $\GL(n)$ or of $S_n$: the word ``group'' hadn't been invented yet.
Only much later did the abstract definition of a group was given,
an abstract set $G$ which was an object in its own right.

While this abstraction is good for some reasons,
it is often also useful to work with concrete representations.
This is the subject of representation theory.
Linear algebra is easier than abstract algebra,
so if we can take a group $G$ and represent it concretely
as a set of matrices in $\opname{GL}(n)$,
this makes them easier to study.
This is the \emph{representation theory of groups}:
how can we take a group and represent its elements as matrices?

\section{Algebras}
\prototype{$k[x_1, \dots, x_n]$ and $k[G]$.}
Rather than working directly with groups from the beginning,
it will be more convenient to deal with so-called $k$-algebras.
This setting is more natural and general than that of groups,
so once we develop the theory of algebras well enough,
it will be fairly painless to specialize to the case of groups.

Colloquially,
\begin{moral}
	An associative $k$-algebra is
	a (possibly noncommutative) ring with a copy of $k$ inside it.
	It is thus a $k$-vector space.
\end{moral}
In particular this makes such an algebra it into a $k$-vector space.
I'll present examples before the definition:
\begin{example}
	[Examples of $k$-Algebras]
	Let $k$ be any field.
	\begin{enumerate}[(a)]
		\ii The field $k$ itself is a $k$-algebra.
		\ii The polynomial ring $k[x_1, \dots, x_n]$.
		\ii The set of $n \times n$ matrices with entries in $k$,
		which we denote by $\Mat_n(k)$.
		Note the multiplication here is not commutative.
		\ii The set $\Mat(V)$ of linear operators $T : V \to V$,
		with multiplication given by the composition of operators.
		(Here $V$ is some vector space over $k$.)
		This is really the same as the previous example.
	\end{enumerate}
\end{example}
\begin{definition}
	Let $k$ be a field.
	A \vocab{$k$-algebra} $A$ is a ring, \emph{possibly noncommutative},
	equipped with an ring homomorphism $k \to A$
	(whose image is the ``copy of $k$'').
	Thus we can consider $k$ as a subset of $A$, and
	we then additionally require $\lambda \cdot a = a \cdot \lambda$
	for each $\lambda0 \in k$ and $a \in A$.

	If the multiplication operation is also commutative,
	then we say $A$ is a \vocab{commutative} algebra.
\end{definition}
\begin{definition}
	Equivalently, a \vocab{$k$-algebra} $A$ is a
	$k$-\emph{vector space} which also has a associative,
	bilinear multiplication operation (with an identity $1_A$).
	The ``copy of $k$'' is obtained by considering elements
	$\lambda 1_A$ for each $\lambda \in k$
	(i.e.\ scaling the identity by the elements of $k$,
	taking advantage of the vector space structure).
\end{definition}

\begin{abuse}
	Some authors don't require $A$ to be associative or have unit,
	so to them what we have just defined is an
	``associative algebra with $1$''.
	However, this is needlessly wordy for our purposes.
\end{abuse}

\begin{example}
	[Group Algebra]
	The \vocab{group algebra} $k[G]$ is the $k$-vector space
	whose \emph{basis elements} are the elements of a group $G$,
	and where the product of two basis elements is the group multiplication.
	For example, suppose $G = \Zc 2 = \{1_G, x\}$.
	Then
	\[ k[G] = \left\{ a1_G + bx \mid a,b \in k \right\} \]
	with multiplication given by
	\[ (a1_G + bx)(c1_G+dx) = (ac+bd)1_G + (bc+ad)x. \]
\end{example}
\begin{ques}
	When is $k[G]$ commutative?
\end{ques}
The example $k[G]$ is very important,
because (as we will soon see) a representation of the algebra $k[G]$
amounts to a representation of the group $G$ itself.

It is worth mentioning at this point that:
\begin{definition}
	A \vocab{homomorphism} of $k$-algebras $A$, $B$ is a
	linear map $T : A \to B$ which respects multiplication
	(i.e.\ $T(xy) = T(x)T(y)$) and which sends $1_A$ to $1_B$.
	In other words, $T$ is both a homomorphism as a ring and as a vector space.
\end{definition}
\begin{definition}
	Given $k$-algebras $A$ and $B$, the \vocab{direct sum} $A \oplus B$
	is defined as pairs $a + b$, where addition is done in the obvious way,
	but we declare $ab = 0$ for any $a \in A$ and $b \in B$.
\end{definition}
\begin{ques}
	Show that $1_A + 1_B$ is the multiplicative identity of $A \oplus B$.
\end{ques}

\section{Representations}
\prototype{$k[S_3]$ acting on $k^{\oplus 3}$ is my favorite.}

\begin{definition}
	A \vocab{representation} of a $k$-algebra $A$ consists of
	\begin{enumerate}[(i)]
		\ii A $k$-vector space $V$, and
		\ii An \emph{action} $\cdot$ of $A$ on $V$: thus, for every $a \in A$
		we can take $v \in V$ and act on it to get $a \cdot v$.
		This satisfies the usual axioms:
		\begin{itemize}
			\ii $(a+b) \cdot v = a \cdot v + b \cdot v$,
			and $(ab) \cdot v = a(b \cdot v)$.
			\ii $\lambda \cdot v = \lambda v$ for $\lambda \in k$.
			In particular, $1_A \cdot v = v$.
		\end{itemize}
	\end{enumerate}
	In other words, a representation is a \textbf{left $A$-module} $V$.
\end{definition}

\begin{definition}
	The action of $A$ can be more succinctly described as saying
	that there is a \emph{$k$-algebra homomorphism} $\rho : A \to \Mat(V)$.
	(So $a \cdot v = \rho(a)(v)$.)
	Thus we can also define a \vocab{representation} of $A$ as a pair
	\[ \left( V, \rho : A \to \Mat(V) \right). \]
\end{definition}
\begin{remark}
	This is completely analogous to how a group action $G$ on a set $X$
	with $n$ elements just amounts to a group homomorphism $G \to S_n$.
\end{remark}
From this perspective, what we are really trying to do is:
\begin{moral}
	If $A$ is an algebra,
	we are trying to \emph{represent}
	the elements of $A$ as matrices.
\end{moral}

\begin{abuse}
	While a representation is a pair $(V, \rho)$
	of both the vector space $V$ and the action $\rho$,
	we frequently will just abbreviate it to ``$V$''.
	This is probably one of the worst abuses I will commit.
\end{abuse}

\begin{example}
	[Representations of $\Mat(V)$]
	\listhack
	\begin{enumerate}[(a)]
		\ii Let $A = \Mat_2(\RR)$.
		Then there is a representation $(\RR^{\oplus 2}, \rho)$
		where a matrix $a \in A$ just acts by $a \cdot v = \rho(a)(v) = a(v)$.

		\ii More generally, given a vector space $V$ over any field $k$,
		there is an obvious representation of $A = \Mat(V)$
		by $a \cdot v = \rho(a)(v) = a(v)$ (since $a \in \Mat(V)$).

		From the matrix perspective: if $A = \Mat(V)$,
		then we can just represent $A$ as matrices over $V$.

		\ii There are other representations of $A = \Mat_2(\RR)$.
		A silly example is the representation $(\RR^{\oplus 4}, \rho)$ given by
		\[
			\rho : 
			\begin{pmatrix} a & b \\ c & d \end{pmatrix} 
			\mapsto
			\begin{pmatrix} a & b & 0 & 0 \\ c & d & 0 & 0 \\
				0 & 0 & a & b \\ 0 & 0 & c & d \end{pmatrix} .
		\]
		More abstractly, viewing $\RR^{\oplus 4}$ as
		$(\RR^{\oplus 2}) \oplus (\RR^{\oplus 2})$,
		this is $a \cdot (v_1,v_2) = (a \cdot v_1, a \cdot v_2)$.
	\end{enumerate}
\end{example}

\begin{example}
	[Representations of Polynomials Algebras]
	\listhack
	\begin{enumerate}[(a)]
		\ii Let $A = k$.
		Then a representation of $k$ is just any $k$-vector space $V$.

		\ii If $A = k[x]$,
		then a representation $(V, \rho)$ of $A$
		amounts to a vector space $V$ plus the choice of
		a linear operator $T \in \Mat(V)$ (the image $\rho(x)$).

		\ii If $A = k[x] / (x^2)$
		then a representation $(V, \rho)$ of $A$
		amounts to a vector space $V$ plus the choice of
		a linear operator $T \in \Mat(V)$ satisfying $T^2 = 0$.

		\ii We can create arbitrary ``functional equations'' with this pattern.
		For example, if $A = k[x,y] / (x^2 - x+y, y^4)$
		then representing $A$ by $V$ amounts to finding operators
		$S, T \in \Mat(V)$ satisfying $S^2 = S-T$ and $T^4 = 0$.
	\end{enumerate}
\end{example}


\begin{example}
	[Representations of Groups]
	\listhack
	\begin{enumerate}[(a)]
		\ii Let $A = \RR[S_3]$.
		Then let 
		\[ V = \RR^{\oplus 3} = \{ (x,y,z) \mid x,y,z \in \RR \}. \]
		We can let $A$ act on $V$ as follows:
		given a permutation $\pi \in S_3$, we permute the corresponding
		coordinates in $V$.
		So for example, if 
		\[ \text{If } \pi = (1 \; 2)
		\text{ then } \pi \cdot (x,y,z) = (y,x,z). \]
		This extends linearly to let $A$ act on $V$,
		by permuting the coordinates.

		From the matrix perspective, what we are doing
		is representing the permutations in $S_3$
		as permutation matrices on $k^{\oplus 3}$, like
		\[ (1 \; 2)
		\mapsto \begin{pmatrix} 0&1&0 \\ 1&0&0 \\ 0&0&1 \end{pmatrix}. \]
		
		\ii More generally, let $A = k[G]$.
		Then a representation $(V, \rho)$ of $A$ amounts to 
		a group homomorphism $\psi : G \to \GL(V)$.
	\end{enumerate}
\end{example}
\begin{abuse}
	From now on, we will often shorten ``representation of $k[G]$''
	to ``representation of $G$''.
\end{abuse}
\begin{example}[Regular Representation]
	Any $k$-algebra $A$ is a representation $(A, \rho)$ over itself,
	with $a \cdot b = \rho(a)(b) = ab$ (i.e.\ multiplication given by $A$).
	This is called the \vocab{regular representation}, denoted $\Reg(A)$.
\end{example}

\section{Direct Sums}
\prototype{The example with $\RR[S_3]$ seems best.}
\begin{definition}
	Let $A$ be $k$-algebra and let $V = (V, \rho_V)$ and $W = (W, \rho_W)$
	be two representations of $A$.
	Then $V \oplus W$ is a representation, with action $\rho$ by
	\[ a \cdot (v,w) = (a \cdot v, a \cdot w). \]
	This representation is called the \vocab{direct sum} of $V$ and $W$.
\end{definition}
\begin{example}
	Earlier we let $\Mat_2(\RR)$ act on $\RR^{\oplus 4}$ by
	\[
		\rho : 
		\begin{pmatrix} a & b \\ c & d \end{pmatrix} 
		\mapsto
		\begin{pmatrix} a & b & 0 & 0 \\ c & d & 0 & 0 \\
			0 & 0 & a & b \\ 0 & 0 & c & d \end{pmatrix} .
	\]
	So this is just a direct sum of two two-dimensional representations.
\end{example}
More generally, given representations $(V, \rho_V)$ and $(W, \rho_W)$
the representation $\rho$ of $V \oplus W$ looks like
\[ \rho(a) =
	\begin{pmatrix}
		\rho_V(a) & 0 \\ 0 & \rho_W(a)
	\end{pmatrix}.
\]
\begin{example}[Representation of $S_n$ decomposes]
	Let $A = \RR[S_3]$ again,
	acting via permutation of coordinates on
	\[ V = \RR^{\oplus 3} = \{ (x,y,z) \mid x,y,z \in \RR \}. \]
	Consider the two subspaces
	\begin{align*}
		W_1 &= \left\{ (t,t,t) \mid t \in \RR \right\} \\
		W_2 &= \left\{ (x,y,z) \mid x+y+z = 0 \right\}.
	\end{align*}
	Note $V = W_1 \oplus W_2$ as vector spaces.
	But each of $W_1$ and $W_2$ is a subrepresentation
	(since the action of $A$ keeps each $W_i$ in place),
	so $V = W_1 \oplus W_2$ as representations too.
\end{example}

Direct sums also come up when we play with algebras.
\begin{proposition}[Representations of $A \oplus B$ are $V_A \oplus V_B$]
	\label{prop:rep_direct_sum}
	Let $A$ and $B$ be $k$-algebras.
	Then every representation of $A \oplus B$ is of the form 
	\[ V_A \oplus V_B \]
	where $V_A$ and $V_B$ are representations of $A$ and $B$, respectively.
\end{proposition}
\begin{proof}[Sketch of Proof]
	Let $(V, \rho)$ be a representation of $A \oplus B$.
	For any $v \in V$, $\rho(1_A+1_B)v = \rho(1_A)v + \rho(1_B)v$.
	One can then set $V_A = \{ \rho(1_A)v \mid v \in V \}$
	and $V_B = \{ \rho(1_B)v \mid v \in V \}$.
	These are disjoint, since if $\rho(1_A) v = \rho(1_B) v'$,
	we have $\rho(1_A)v = \rho(1_A1_A)v = \rho(1_A1_B) v' = 0_V$,
	and similarly for the other side.
\end{proof}

\section{Irreducible and Indecomposable Representations}
\prototype{$k[S_3]$ decomposes as the sum of two spaces.}

One of the goals of representation theory will be to classify
all possible representations of an algebra $A$.
If we want to have a hope of doing this,
then we want to discard ``silly'' representations such as
\[
	\rho : 
	\begin{pmatrix} a & b \\ c & d \end{pmatrix} 
	\mapsto
	\begin{pmatrix} a & b & 0 & 0 \\ c & d & 0 & 0 \\
		0 & 0 & a & b \\ 0 & 0 & c & d \end{pmatrix} .
\]
and focus our attention instead on ``irreducible'' representations.
This motivates the following definition.

\begin{definition}
	Let $V$ be a representation of $A$.
	A \vocab{subrepresentation} $W \subset V$ is a subspace $W$
	with the property that for any $a \in A$ and $w \in W$,
	$a \cdot w = w$.
	In other words, this subspace is invariant under actions by $A$.
\end{definition}
Thus if $V \neq W_1 \oplus W_2$ for representations $W_1$, $W_2$
then $W_1$ and $W_2$ are subrepresentations.

\begin{definition}
	If $V$ has no proper subrepresentations then it is \vocab{irreducible}.
	If $V \neq W_1 \oplus W_2$ for proper subrepresentations $W_1$, $W_2$,
	then we say it is \vocab{indecomposable}.
\end{definition}
\begin{definition}
	For brevity, an \vocab{irrep} is a
	finite-dimensional irreducible representation.
\end{definition}

\begin{example}[Representation of $S_n$ decomposes]
	Let $A = \RR[S_3]$ again, acting via permutation of coordinates on
	\[ V = \RR^{\oplus 3} = \{ (x,y,z) \mid x,y,z \in \RR \}. \]
	Consider again the two subspaces
	\begin{align*}
		W_1 &= \left\{ (t,t,t) \mid t \in \RR \right\} \\
		W_2 &= \left\{ (x,y,z) \mid x+y+z = 0 \right\}.
	\end{align*}
	As we've seen, $V = W_1 \oplus W_2$, and thus $V$ is not irreducible.
	But one can show that $W_1$ and $W_2$ are irreducible 
	(and hence indecomposable) as follows.
	\begin{itemize}
		\ii For $W_1$ it's obvious, since $W_1$ is one-dimensional.
		\ii For $W_2$, consider any vector $w = (a,b,c)$
		with $a+b+c=0$ and not all zero.  Then WLOG we can assume $a \neq b$ 
		(since not all three coordinates are equal).
		In that case, $(1 \; 2)$ sends $w$ to $w' = (b,a,c)$.
		Then $w$ and $w'$ span $W_2$.
	\end{itemize}
	Thus $V$ breaks down completely into irreps.
\end{example}

Unfortunately, if $W$ is a subrepresentation of $V$,
then it is not necessarily the case that we can find a
supplementary vector space $W'$ such that $V = W \oplus W'$.
Put another way, if $V$ is reducible, we know that it has a subrepresentation,
but a decomposition requires \emph{two} subrepresentations.
Here is a standard counterexample:
\begin{exercise}
	Let $A = \RR[x]$, and $V = \RR^{\oplus 2}$ the representation with action
	\[ \rho(x) = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}. \]
	Show that the only subrepresentation is $W = \{ (t,0) \mid t \in \RR \}$.
	So $V$ is not irreducible, but it is indecomposable.
\end{exercise}

Here is a slightly more optimistic example,
and the ``prototypical example'' that you should keep in mind.

\begin{exercise}
	Let $A = \Mat_d(k)$ and consider the obvious representation $k^{\oplus d}$
	of $A$ that we described earlier. Show that it is irreducible.
	(This is obvious if you understand the definitions well enough.)
\end{exercise}

\section{Morphisms of Representations}
We now proceed to define the morphisms between representations.

\begin{definition}
	Let $(V, \rho_1)$ and $(W, \rho_2)$ be representations of $A$.
	An \vocab{intertwining operator}, or \vocab{morphism}, is a
	linear map $T : V \to W$ such that
	\[ T(a \cdot v) = a \cdot T(v) \]
	for any $a \in A$, $v \in V$.
	(Note the first $\cdot$ is the action $\rho_1$
	and the second $\cdot$ is the action of $\rho_2$.)
	This is exactly what you expect if you think that $V$ and $W$
	are ``left $A$-modules''.
	If $T$ is invertible, then it is an \vocab{isomorphism} of representations
	and we say $V \cong W$.
\end{definition}
\begin{remark}
	[For Commutative Diagram Lovers]
	The condition $T(a \cdot v) = a \cdot T(v)$ can be read as saying that
	\begin{diagram}
		V & \rTo^{\rho_1(a)} & V \\
		\dTo^T & & \dTo_T \\
		W & \rTo_{\rho_2(a)} & W
	\end{diagram}
	commutes for any $a \in A$.
\end{remark}

\begin{remark}
	[For Category Lovers]
	A representation is just a ``bilinear'' functor from an
	abelian one-object category $\{\ast\}$ (so $\Hom(\ast, \ast) \cong A$)
	to the abelian category $\catname{Vect}_k$.
	Then an intertwining operator is just a \emph{natural transformation}.
\end{remark}

Here are some examples of intertwining operators.
\begin{example}[Intertwining Operators]
	\listhack
	\begin{enumerate}[(a)]
		\ii For any $\lambda \in k$, the scalar map $T(v) = \lambda v$
		is intertwining.
		\ii If $W \subseteq V$ is a subrepresentation,
		then the inclusion $W \injto V$ is a subrepresentation.
		\ii The projection map $V_1 \oplus V_2 \surjto V_1$
		is an intertwining operator.
		\ii Let $V  = \RR^{\oplus 2}$ 
		and represent $A = k[x]$ by $(V, \rho)$ where
		\[ \rho(x) = \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}. \]
		Thus $\rho(x)$ is rotation by $90\dg$ around the origin.
		Let $T$ be rotation by $30\dg$.
		Then $T : V \to V$ is intertwining (the rotations commute).
	\end{enumerate}
\end{example}

\begin{exercise}[Kernel and Image Are Subrepresentations]
	Let $T : V \to W$ be an intertwining operator.
	\begin{enumerate}[(a)]
		\ii Show that $\ker T \subseteq V$ is a subrepresentation of $V$.
		\ii Show that $\img T \subseteq W$ is a subrepresentation of $W$.
	\end{enumerate}
\end{exercise}

This gives us the famous Schur's Lemma.
\begin{theorem}
	[Schur's Lemma]
	Let $V$ and $W$ be representations of a $k$-algebra $A$.
	Let $T : V \to W$ be a \emph{nonzero} intertwining operator.
	Then
	\begin{enumerate}[(a)]
		\ii If $V$ is irreducible, then $T$ is injective.
		\ii If $W$ is irreducible, then $T$ is surjective.
	\end{enumerate}
	In particular if both $V$ and $W$ are irreducible then $T$
	is either zero or an isomorphism.
\end{theorem}
An important special case is if $k$ is algebraically closed:
then the only intertwining operators $V \to V$
are the trivial constant ones.
\begin{theorem}
	[Schur's Lemma for Algebraically Closed Fields]
	Let $k$ be an algebraically closed field.
	Let $V$ be an irrep of a $k$-algebra $A$.
	Then any intertwining operator $T : V \to V$ is multiplication by a scalar.
\end{theorem}
\begin{exercise}
	Use the fact that $T$ has an eigenvalue $\lambda$ to
	deduce this from Schur's Lemma.
	(Consider $T - \lambda \cdot \id_V$, and use Schur to deduce it's zero.)
\end{exercise}
We have already seen the counterexample of rotation by $90\dg$ for $k = \RR$;
this was the same counterexample we gave to the assertion that all linear maps
have eigenvalues.

\section{The Representations of $\Mat_d(k)$}
To give an example of the kind of progress already possible,
we prove the following theorem.
\begin{theorem}
	[Representations of $\Mat_d(k)$]
	\label{thm:rep_1mat}
	Let $k$ be any field, $d$ be a positive integer and
	let $X = k^{\oplus d}$ be the obvious representation of $A = \Mat_d(k)$.
	Then the only finite-dimensional representations
	of $\Mat_d(k)$ are $W^{\oplus n}$
	for some positive integer $n$ (up to isomorphism).
	In particular, it is irreducible if and only if $n=1$.
\end{theorem}
For concreteness, I'll just sketch the case $d=2$,
since the same proof applies verbatim to other situations.
This shows that the examples of representations of $\Mat_2(\RR)$
we gave earlier are the only ones.

As we've said this is essentially a functional equation.
The algebra $A = \Mat_2(k)$ has basis given by four matrices
\[
	E_1 = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix},
	\qquad
	E_2 = \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix},
	\qquad
	E_3 = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix},
	\qquad
	E_4 = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}
\]
satisfying relations like $E_1 + E_2 = \id$, $E_i^2 = E_i$, $E_1E_2 = 0$, etc.
So let $V$ be a representation of $A$, and let $M_i = \rho(E_i)$ for each $i$;
we want to classify the possible matrices $M_i$ on $V$
satisfying the same functional equations.
This is because, for example,
\[ \id_V = \rho(\id_A) = \rho(E_1+E_2) = M_1 + M_2. \]
By the same token $M_1M_3 = M_3$.
Proceeding in a similar way, we can obtain the following multiplication table.
\[
	\begin{array}{r|llll}
		\times & M_1 & M_2 & M_3 & M_4 \\ \hline
		M_1 & M_1 & 0 & M_3 & 0 \\
		M_2 & 0 & M_2 & 0 & M_4 \\
		M_3 & 0 & M_3 & 0 & M_1 \\
		M_4 & M_4 & 0 & M_2 & 0
	\end{array}
	\qquad \text{and} \qquad
	M_1 + M_2 = \id_V
\]
Note that each $M_i$ is a linear operator $V \to V$;
for all we know, it could have hundreds of entries.
Nonetheless, given the multiplication table of the basis $E_i$
we get the corresponding table for the $M_i$.

So, in short, the problem is as follows:
\begin{moral}
	Find all vector spaces $V$ and quadruples of matrices $M_i$
	satisfying the multiplication table above.
\end{moral}

Let $W_1$ be the image of $M_1 : V \to V$ and $W_2$ the image of $M_2 : V \to V$.
\begin{claim}
	$V = W_1 \oplus W_2$.
\end{claim}
\begin{proof}
	First, note that for any $v \in V$ we have
	\[ v = \rho(\id)(v) = (M_1+M_2)v = M_1v + M_2v. \]
	Moreover, we have that $W_1 \cap W_2 = \{0\}$, because if
	$M_1v_1 = M_2v_2$ then $M_1v_1 = M_1(M_1v_1) = M_1(M_2v_2) = 0$.
\end{proof}
\begin{claim}
	$W_1 \cong W_2$.
\end{claim}
\begin{proof}
	Check that the maps $W_1 \taking{\times M_4} W_2$
	and $W_2 \taking{\times M_3} W_1$ are
	well-defined and mutually inverse.
\end{proof}
Now, let $e_1, \dots, e_n$ be basis elements of $W_1$;
thus $M_4e_1$, \dots, $M_4e_n$ are basis elements of $W_2$.
However, each $\{e_j, M_4e_j\}$ forms a basis of a subrepresentation
isomorphic to $X = k^{\oplus 2}$ (what's the isomorphism?).

This finally implies that all representations of $A$
are of the form $W^{\oplus n}$.
In particular, $W$ is irreducible because there are no representations
of smaller dimension at all!

\section\problemhead
\begin{dproblem}
	\label{prob:one_dim}
	Suppose we have one-dimensional representations
	$V_1 = (V_1, \rho_1)$ and $V_2 = (V_2, \rho_2)$ of $A$.
	Show that $V_1 \cong V_2$ if and only if
	$\det\rho_1(a) = \det\rho_2(a)$ for every $a \in A$
	(intuitively, $\rho_1$ and $\rho_2$ are exactly the same).
\end{dproblem}

\begin{dproblem}
	[Schur's Lemma for Commutative Algebras]
	Let $A$ be a \emph{commutative} algebra
	over an algebraically closed field $k$.
	Prove that any irrep of $A$ is one-dimensional.
	\begin{hint}
		For any $a \in A$, the map $v \mapsto a \cdot v$ is intertwining.
	\end{hint}
\end{dproblem}

\begin{sproblem}
	\label{prob:reg_mat}
	Let $(V, \rho)$ be a representation of $A$.
	Then $\Mat(V)$ is a representation of $A$
	with action given by $a \cdot T = \rho(a) \circ T$, for $T \in \Mat(V)$.
	\begin{enumerate}[(a)]
		\ii Show that $\rho : \Reg(A) \to \Mat(V)$ is an intertwining operator.
		\ii If $V$ is $d$-dimensional, show that $\Mat(V) \cong V^{\oplus d}$
		as representations of $A$.
	\end{enumerate}
	\begin{hint}
		For part (b), pick a basis and do $T \mapsto (T(e_1), \dots, T(e_n))$.
	\end{hint}
\end{sproblem}

\begin{sproblem}
	\label{prob:regA_intertwine}
	Find all intertwining operators $T : \Reg(A) \to \Reg(A)$,
	where $A$ is a fixed algebra.
\end{sproblem}

\chapter{Semisimple Algebras}
In what follows, assume the field $k$ is algebraically closed.

A \vocab{completely reducible} representation is one which
can be written as a direct sum of irreducible representations.
We like these types of representations, because they don't have the
undesired behavior of being indecomposable while being reducible.

Now, we name the good algebras:
a finite-dimensional algebra $A$ is \vocab{semisimple}
if all its finite-dimensional representations are completely reducible.
The culminating point of the chapter is when we
prove that $A$ is semisimple if and only if
$A \cong \bigoplus_i \Mat(V_i)$,
where the $V_i$ are the irreps of $A$ (yes, there are only finitely many!).

The goal for the chapter is to examine these in more detail:
we'll find conditions on algebras $A$ which cause all its representations
to be completely reducible.

\section{Schur's Lemma Continued}
\prototype{For $V$ irreducible,
	$\Homrep(V^{\oplus 2}, V^{\oplus 2}) \simeq k^{\oplus 4}$.}
\begin{definition}
	For an algebra $A$ and representations $V$ and $W$,
	we let $\Homrep(V,W)$ be the set of intertwining operators between them.
	(It is also a $k$-algebra.)
\end{definition}

By Schur's Lemma, we already know that if $V$ and $W$ are irreps, then
\[
	\Homrep(V,W) \simeq
	\begin{cases}
		k & \text{if $V \not\cong W$} \\
		0 & \text{if $V \not\cong W$}.
	\end{cases}
\]
Can we say anything more?
For example, it also tells us that 
\[ \Homrep(V, V^{\oplus 2}) = k^{\oplus 2}. \]
The possible maps are $v \mapsto (c_1v_1, c_2v_2)$ for some choice of $c_1, c_2 \in k$.

More generally, suppose $V$ is an irrep and consider
\[ \Homrep(V^{\oplus m}, V^{\oplus n}). \]
Intertwining operators are determined completely
$T : V^{\oplus m} \to V^{\oplus n}$ by the $mn$ choices of compositions
\begin{diagram}
	V & \rInj & V^{\oplus m} & \rTo^T & V^{\oplus n} & \rSurj & V
\end{diagram}
where the first arrow is inclusion to the $i$th component of $V^{\oplus m}$
(for $1 \le i \le m$) and the second arrow is inclusion to the $j$th
component of $V^{\oplus n}$ (for $1 \le j \le n$).
However, by Schur's Lemma on each of these compositions,
we know they must be constant.

Thus, $\Homrep(V^{\oplus n}, V^{\oplus m})$ consist of $n \times m$ ``matrices''
of constants, and the map is provided by
\[
	\begin{pmatrix}
		c_{11} & c_{12} & \dots & c_{1(n-1)} & c_{1n} \\
		c_{21} & c_{22} & \dots & c_{2(n-1)} & c_{1n} \\
		\vdots & \vdots & \ddots & \vdots & \vdots \\
		c_{m1} & c_{m2} & \dots & c_{m(n-1)} & c_{mn}
	\end{pmatrix}
	\begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}
	\in V^{\oplus n}
\]
where the $c_{ij} \in k$ but $v_i \in V$; note the type mismatch!
This is \emph{not} just a linear map $V^{\oplus n_i} \to V^{\oplus m_i}$;
rather, the outputs are $m$ \emph{linear combinations} of the inputs.

More generally, we have the following result.
\begin{theorem}
	[Schur's Lemma for Completely Reducible Representations]
	\label{thm:compred_schur}
	Let $V$ and $W$ be completely reducible representations,
	and set $V = \bigoplus V_i^{\oplus n_i}$, $W = \bigoplus V_i^{\oplus m_i}$
	for integers $n_i, m_i \ge 0$, where each $V_i$ is an irrep.
	Then
	\[ \Homrep(V, W)
		\simeq \bigoplus_i \Mat_{n_i \times m_i}(k) \]
	meaning that an intertwining operator $T : V \to W$
	amounts to, for each $i$, an $n_i \times m_i$ matrix of constants
	which gives a map $V_i^{\oplus n_i} \to V_i^{\oplus m_i}$.
\end{theorem}

\begin{corollary}
	[Subrepresentations of Completely Reducible Representations]
	\label{cor:subrep_schur}
	Let $V = \bigoplus V_i^{\oplus n_i}$ be completely reducible.
	Then any subrepresentation $W$ of $V$ is isomorphic
	to $\bigoplus V_i^{\oplus m_i}$ where $m_i \le n_i$ for each $i$,
	and the inclusion $W \injto V$ is given
	by the direct sum of inclusion $V_i^{\oplus m_i} \injto V_i^{\oplus n_i}$,
	which are $n_i \times m_i$ matrices.
\end{corollary}
\begin{proof}
	Apply Schur's Lemma to the inclusion $W \injto V$.
\end{proof}



\section{Density Theorem}
We are going to take advantage of the previous result to prove that
finite-dimensional algebras have finitely many irreps.

\begin{theorem}
	[Jacobson Density Theorem]
	Let $(V_1, \rho_1)$, \dots, $(V_r, \rho_r)$ be pairwise nonisomorphic
	finite-dimensional representations of $A$.
	Then the map of vector spaces
	\[ \bigoplus_{i=1}^r \rho_i : A \surjto \bigoplus_{i=1}^r \Mat(V_i) \]
	is surjective.
\end{theorem}
The right way to think about this theorem is that
\begin{moral}
	The Density Theorem is the ``Chinese Remainder Theorem''
	for irreps of $A$.
\end{moral}
Recall that in number theory, the Chinese Remainder Theorem tells us
that given lots of ``unrelated'' congruences, we can find a single $N$
which simultaneously satisfies them all.
Similarly, given lots of different nonisomorphic representations of $A$,
this means that we can select a single $a \in A$ which induces any tuple
$(\rho_1(a), \dots, \rho_r(a))$ of actions we want --- a surprising result,
since even the $r=1$ case is not obvious at all!

\begin{diagram}
	&&& \rho_1(a) &= M_1 \in \Mat(V_1) \\
	&& \ruTo(3,2) & \rho_2(a) &= M_2 \in \Mat(V_2) \\
	\boxed{a \in A} && \ruTo(3,1) && \vdots \\
	& \rdTo(3,1) && \rho_r(a) &= M_r \in \Mat(V_r) \\
\end{diagram}

This also gives us the non-obvious corollary
\begin{corollary}
	[Finiteness of Number of Representations]
	Any finite-dimensional algebra $A$ has at most $\dim A$ irreps.
	\label{cor:finiteness}
\end{corollary}
\begin{proof}
	If $V_i$ are such irreps then 
	$A \surjto \bigoplus_i V_i^{\oplus \dim V_i}$,
	hence we have the inequality $\sum (\dim V_i)^2 \le \dim A$.
\end{proof}

\begin{proof}[Proof of Density Theorem]
	Let $V = V_1 \oplus \dots \oplus V_r$, so $A$
	acts on $V = (V, \rho)$ by $\rho = \bigoplus_i \rho_i$.
	Thus by \Cref{prob:reg_mat}, we can instead consider $\rho$
	as an \emph{intertwining operator}
	\[ \rho : \Reg(A) \to \bigoplus_{i=1}^r \Mat(V_i)
		\cong \bigoplus_{i=1}^r V_i^{\oplus d_i}. \]
	We will use this instead as it will be easier to work with.

	First, we handle the case $r = 1$.
	Fix a basis $e_1$, \dots, $e_n$ of $V = V_1$.
	Assuming for contradiction that the map is not surjective.
	Then there is a map of representations (by $\rho$ and the isomorphism)
	$\Reg(A) \to V^{\oplus n}$ given by $a \mapsto (a \cdot e_1, \dots, a \cdot e_n)$.
	By hypothesis is not surjective:
	its image is a \emph{proper} subrepresentation of $V^{\oplus n}$.
	Assume its image is isomorphic to $V^{\oplus m}$ for $m < n$,
	so by \Cref{thm:compred_schur} there is a matrix of constants $X$ with
	\begin{diagram}
		\Reg(A) & \rTo & V^{\oplus n} & \lInj^{X \cdot -} & V^{\oplus r} \\
		a & \rMapsto & (a \cdot e_1, \dots, a \cdot e_n) && \\
		1_A & \rMapsto & (e_1, \dots, e_n) & \lMapsto & (v_1, \dots, v_m)
	\end{diagram}
	where the two arrows in the top row have the same image;
	hence the pre-image $(v_1, \dots, v_m)$ of $(e_1, \dots, e_n)$ can be found.
	But since $r < n$ we can find constants $c_1, \dots, c_n$ not all zero
	such that $X$ applied to the column vector $(c_1, \dots, c_n)$ is zero:
	\[
		\sum_{i=1}^n c_ie_i
		=
		\begin{pmatrix} c_1 & \dots & c_n \end{pmatrix}
		\begin{pmatrix} e_1 \\ \vdots \\ e_n \end{pmatrix}
		=
		\begin{pmatrix} c_1 & \dots & c_n \end{pmatrix}
		X
		\begin{pmatrix} v_1 \\ \vdots \\ v_m \end{pmatrix}
		= 0
	\]
	contradicting the fact that $e_i$ are linearly independent.
	Hence we conclude the theorem for $r=1$.

	As for $r \ge 2$, the image of $\rho$ is necessarily of the form
	$\bigoplus_i V_i^{\oplus r_i}$ (by \Cref{cor:subrep_schur})
	and by the above $r_i = \dim V_i$ for each $i$.
\end{proof}

\section{Semisimple Algebras}

\begin{definition}
	A finite-dimensional algebra $A$ is a \vocab{semisimple}
	if every finite-dimensional representation of $A$ is completely reducible.
\end{definition}

\begin{theorem}
	[Semisimple Algebras]
	Let $A$ be a finite-dimensional algebra.
	Then the following are equivalent:
	\begin{enumerate}[(i)]
		\ii $A \cong \bigoplus_i \Mat_{d_i}(k)$ for some $d_i$.
		\ii $A$ is semisimple.
		\ii $\Reg(A)$ is completely reducible.
	\end{enumerate}
\end{theorem}
\begin{proof}
	(i) $\implies$ (ii) follows
	from \Cref{thm:rep_1mat} and \Cref{prop:rep_direct_sum}.
	(ii) $\implies$ (iii) is tautological.

	To see (iii) $\implies$ (i), we use the following clever trick.
	Consider
	\[ \Homrep(\Reg(A), \Reg(A)). \]
	On one hand, by \Cref{prob:regA_intertwine},
	it is isomorphic to $A\op$ ($A$ with opposite multiplication),
	because the only intertwining operators $\Reg(A) \to \Reg(A)$
	are those of the form $- \cdot a$.
	On the other hand, suppose that we have set
	$ \Reg(A) = \bigoplus_i V_i^{\oplus n_i} $.
	By \Cref{thm:compred_schur}, we have
	\[ A\op \simeq \Homrep(\Reg(A), \Reg(A))
		= \bigoplus_i \Mat_{n_i \times n_i}(k). \]
	But $\Mat_n(k)\op \simeq \Mat_n(k)$ (just by transposing),
	so we recover the desired conclusion.
\end{proof}

In fact, if we combine the above result with
the Density Theorem (and \Cref{cor:finiteness}), we also have the following.
\begin{theorem}
	[Sum of Squares Formula]
	For a finite-dimensional algebra $A$ we have
	\[ \sum_{i} \dim(V_i)^2 \le \dim A \]
	where the $V_i$ are the irreps of $A$;
	equality holds exactly when $A$ is semisimple,
	in which case 
	\[ \Reg(A) \cong \bigoplus_i \Mat(V_i)
		\cong \bigoplus_I V_i^{\oplus \dim V_i}. \]
\end{theorem}
\begin{proof}
	The inequality was already mentioned in \Cref{cor:finiteness}.
	It is equality if and only if the map $\rho : A \to \bigoplus_i \Mat(V_i)$
	is an isomorphism; this means all $V_i$ are present.
\end{proof}

\begin{remark}
	[Digression]
	For any finite-dimensional $A$, the kernel of the map
	$\rho : A \to \bigoplus_i \Mat(V_i)$ is denoted $\opname{Rad}(A)$
	and is the so-called \vocab{Jacobson radical} of $A$;
	it's the set of all $a \in A$ which act by zero in all irreps of $A$.
	The usual definition of ``semisimple'' given in books is that
	this Jacobson radical is trivial.
\end{remark}

\section{Maschke's Theorem}
We now prove that the representation theory of groups is as nice as possible.
\begin{theorem}
	[Maschke's Theorem]
	Let $G$ be a finite group, and $k$ an algebraically closed
	field whose characteristic does not divide $|G|$.
	Then $k[G]$ is semisimple.
\end{theorem}
This tells us that when studying representations of groups,
all representations are completely reducible.
\begin{proof}
	Consider any finite-dimensional representation $(V, \rho)$ of $k[G]$.
	Given a proper subrepresentation $W \subseteq V$,
	our goal is to construct a supplementary $G$-invariant subspace $W'$
	which satisfies \[ V = W \oplus W'. \]
	This will show that indecomposable $\iff$ irreducible,
	which is enough to show $k[G]$ is semisimple.

	Let $\pi : V \to W$ be any projection of $V$ onto $W$,
	meaning $\pi(v) = v \iff v \in W$.
	We consider the \emph{averaging} map $P : V \to V$ by
	\[ 
		P(v) = \frac{1}{\left\lvert G \right\rvert}
		\sum_{g \in G} \rho(g\inv) \circ \pi \circ \rho(g).
	\]
	We'll use the following properties of the map:
	\begin{exercise}
		Show that the map $P$ has the following three properties.
		\begin{itemize}
			\ii For any $w \in W$, $P(w) = w$.
			\ii For any $v \in V$, $P(w) \in W$.
			\ii The map $P : V \to V$ is an intertwining operator.
		\end{itemize}
	\end{exercise}
	Thus $P$ is idempotent (it is the identity on its image $W$),
	so by \Cref{prob:idempotent} we have $V = \ker P \oplus \img P$,
	but both $\ker P$ and $\img P$ are subrepresentations as desired.
\end{proof}

\section{Example: The Representations of $\CC[S_3]$}
We compute all irreps of $\CC[S_3]$.
I'll take for granted right now there are exactly three such representations
(which will be immediate by the first theorem in the next chapter:
we'll in fact see that the number of representations of $G$
is exactly equal to the number of conjugacy classes of $G$).

Given that, if the three representations of have dimension $d_1$, $d_2$, $d_3$ ,
then we ought to have
\[ d_1^2 + d_2^2 + d_3^2 = |G| = 6. \]
From this, combined with some deep arithmetic,
we deduce that we should have $d_1 = d_2 = 1$ and $d_3 = 2$
or some permutation.

In fact, we can describe these representations explicitly:
\begin{itemize}
	\ii There is a trivial one-dimensional representation $\Ctriv$
	where the map $S_3 \to \CC^\times$ is the identity.
	Thus in $\Ctriv$ every $\sigma \in S_3$ acts as the identity.

	\ii There is a nontrivial one-dimensional representation
	$\Csign$ where the map $S_3 \to \CC^\times$ is given
	by sending $\sigma$ to the sign of $\sigma$.
	Thus in $\Csign$ every $\sigma \in S_3$ acts as $\pm 1$.
	Of course, $\Ctriv$ and $\Csign$ are not isomorphic
	(as one-dimensional representations are never isomorphic
	unless the constants they act on coincide for all $a$,
	as we saw in \Cref{prob:one_dim}).

	\ii Finally, we have already seen the two-dimensional representation,
	but now we give it a name.
	Define $\refl_0$ to be the representation whose vector space is
	$\{ (x,y,z) \mid x+y+z = 0 \}$,
	and whose action of $S_3$ on it is permutation of coordinates.
	\begin{exercise}
		Show that $\refl_0$ is irreducible, for example by showing directly
		that no subspace is invariant under the action of $S_3$.
	\end{exercise}
	Thus $V$ is also not isomorphic to the previous two representations.
\end{itemize}
This implies that these are all the irreps of $S_3$.
Note that, if we take the representation $V$ of $S_3$ on $k^{\oplus 3}$,
we just get that $V = \refl_0 \oplus \CC_{\text{triv}}$.

\section\problemhead
\begin{problem}
	\gim
	Let $G = \Zc n$ and let $k = \CC$.
	Show that up to isomorphism there are exactly $n$ irreps of $k[G]$.
	What if $k = \RR$?
\end{problem}

\begin{problem}
	Determine all the irreps of $D_{10}$,
	given that there are exactly four of them.
\end{problem}

\chapter{Characters}
Characters are basically the best thing ever.
To every representation $V$ of $A$ we will attach a
so-called character $\chi_V : A \to k$.
It will turn out that the characters of irreps of $V$
will determine the representation $V$ completely.
Thus an irrep is just specified by a set of $\dim A$ numbers.

\section{Definitions}
\begin{definition}
	Let $V = (V, \rho)$ be a finite-dimensional representation of $A$.
	The \vocab{character} $\chi_V : A \to k$ attached to
	$A$ is defined $\chi_V = \Tr \circ \rho$, i.e.\
	\[ \chi_V(a) \defeq \Tr\left( \rho(a) : V \to V \right). \]
\end{definition}
Since $\Tr$ and $\rho$ are additive, this is a $k$-linear map
(but it is not multiplicative).
Note also that $\chi_{V \oplus W} = \chi_V + \chi_W$
for any representations $V$ and $W$.

We are especially interested in the case $A = k[G]$, of course.
As usual, we just have to specify $\chi_V(g)$ for each
$g \in S_3$ to get the whole map $k[G] \to k$.
Thus we often think of $\chi_V$ as a function $G \to k$,
called a character of the group $G$.
Here is the case $G = S_3$:
\begin{example}
	[Character Table of $S_3$]
	Let's consider the three irreps of $G = S_3$ from before.
	For $\CC_{\text{triv}}$ all traces are $1$;
	for $\CC_{\text{sign}}$ the traces are $\pm 1$ depending on sign
	(obviously, for one-dimensional maps $k \to k$ the trace ``is''
	just the map itself).
	For $\refl_0$ we take a basis $(1,0,-1)$ and $(0,1,-1)$, say,
	and compute the traces directly in this basis.
	\[
		\begin{array}{|r|rrr|}
			\hline
			\chi_V(g) & \CC_{\text{triv}} & \CC_{\text{sign}} & \refl_0 \\ \hline
			\id & 1 & 1 & 2 \\
			(1 \; 2) & 1 & -1 & 0 \\
			(2 \; 3) & 1 & -1 & 0 \\
			(3 \; 1) & 1 & -1 & 0 \\
			(1 \; 2 \; 3) & 1 & 1 & -1 \\
			(3 \; 2 \; 1) & 1 & 1 & -1 \\ \hline
		\end{array}
	\]
\end{example}
The above table is called the \vocab{character table} of the group $G$.
The table above has certain mysterious properties,
which we will prove as the chapter progresses.
\begin{enumerate}[(I)]
	\ii The value of $\chi_V(g)$ only depends on the conjugacy class of $g$.
	\ii The number of columns equals the number of conjugacy classes.
	\ii The sum of the squares are any column is $6$ again!
	\ii The ``dot product'' of any two columns is zero.
\end{enumerate}

\begin{abuse}
	The name ``character'' for $\chi_V : G \to k$ is a bit of a misnomer.
	This $\chi_V$ is not multiplicative in any way,
	as the above example shows: one can almost think of it as
	an element of $k^{\oplus |G|}$.
\end{abuse}

\begin{ques}
	Show that $\chi_V(1_A) = \dim V$.
\end{ques}

\section{The Dual Space Modulo the Commutator}
For any algebra, we first observe that since $\Tr(TS) = \Tr(ST)$,
we have for any $V$ that
\[ \chi_V(ab) = \chi_V(ba). \]
This explains observation (I) from earlier:
\begin{ques}
	Deduce that if $g$ and $h$ are in the same conjugacy class of a 
	group $G$, and $V$ is a representation of $\CC[G]$,
	then $\chi(g) = \chi(h)$.
\end{ques}
Now, given our algebra $A$ we define the \vocab{commutator} $[A,A]$
to be the (two-sided) ideal\footnote{%
	This means the ideal consists of sums elements of the form
	$a(xy-yx)b$ for $a,b \in A$.
}
generated by elements of the form $xy-yx$.
Thus $[A,A]$ is contained in the kernel of each $\chi_V$.
\begin{definition}
	The space $A / [A,A]$ is called the \vocab{abelianization} of $A$;
	for brevity we denote it as $A\ab$.
	We think of this as ``$A$ modulo the relation $ab=ba$ for each $a,b \in A$.''
\end{definition}
So we can think of each character $\chi_V$ as an element of $(A\ab)^\vee$.

\begin{example}
	[Examples of Abelianizations]
	\listhack
	\begin{enumerate}[(a)]
		\ii If $A$ is commutative, then $[A,A] = \{0\}$
		and $A\ab = A$.
		\ii If $A = \Mat_k(d)$, then $[A,A]$ consists exactly
		of the $d \times d$ matrices of trace zero.
		(Proof: harmless exercise.)
		Consequently, $A\ab$ is one-dimensional.
		\ii Suppose $A = k[G]$.  We claim that $\dim A\ab$ is equal to the
		number of conjugacy classes of $A$.
		Indeed, an element of $A$ can be thought of as just 
		an arbitrary function $\xi : G \to k$.
		So an element of $A\ab$ is a function $\xi: G \to k$ such that
		$\xi(gh) = \xi(hg)$ for every $g,h \in G$.
		This is equivalent to functions from conjugacy classes of $G$ to $k$.
	\end{enumerate}
\end{example}

\begin{theorem}
	[Character of Representations of Algebras]
	Let $A$ be an algebra over an algebraically closed field. Then
	\begin{enumerate}[(a)]
		\ii Characters of pairwise non-isomorphic irreps are
		linearly independent as elements of $A\ab$.
		\ii If $A$ is finite-dimensional and semisimple,
		then the characters attached to irreps
		form a basis of $A\ab$.
	\end{enumerate}
	In particular, in (b) the number of irreps of $A$ equals $\dim A\ab$.
\end{theorem}
\begin{proof}
	Part (a) is more or less obvious by the Density Theorem.
	Suppose there is a linear dependence, so that for every $a$ we have
	\[ c_1 \chi_{V_1}(a) + c_2 \chi_{V_2}(a) + \dots + c_r \chi_{V_r} (a) = 0\]
	for some integer $r$.
	\begin{ques}
		Deduce that $c_1 = \dots = c_r = 0$ from Density.
	\end{ques}
	For part (b), assume there are $r$ irreps
	we may assume that \[ A = \bigoplus_{i=1}^r \Mat(V_i) \]
	where $V_1$, \dots, $V_r$ are the irreps of $A$.
	Since we have already showed the characters are linearly independent
	we need only show that $\dim ( A / [A,A] ) = r$,
	which follows from the observation earlier that each $\Mat(V_i)$
	has a one-dimensional abelianization.
\end{proof}
Since $\dim \CC[G]\ab$ is the number of conjugacy classes of $G$,
this completes the proof of (II).

\section{Orthogonality of Characters}
Now we specialize to the case of finite groups $G$, represented over $\CC$.
\begin{definition}
	Let $\Classes(G)$ denote the set conjugacy classes of $G$.
\end{definition}
If $G$ has $r$ conjugacy classes, then it has $r$ irreps.
Each (finite-dimensional) representation $V$, irreducible or not, gives a
character $\chi_V$.
\begin{abuse}
	From now on, we will often regard $\chi_V$ as a function $G \to \CC$
	or as a function $\Classes(G) \to \CC$.
	So for example, we will write both $\chi_V(g)$ (for $g \in G$)
	and $\chi_V(C)$ (for a conjugacy class $C$);
	the latter just means $\chi_V(g_C)$ for any representative $g_C \in C$.
\end{abuse}
\begin{definition}
	Let $\FunCl(G)$ denote the set of functions $\Classes(G) \to \CC$
	viewed as a vector space over $\CC$.
	We endow it with the inner form
	\[
		\left< f_1, f_2 \right> = 
		\frac{1}{|G|}
		\sum_{g \in G} f_1(g) \ol{f_2(g)}.
	\]
\end{definition}
This is the same ``dot product'' that we mentioned at the beginning,
when we looked at the character table of $S_3$.
We now aim to prove the following orthogonality theorem,
which will imply (III) and (IV) from earlier.
\begin{theorem}[Orthogonality]
	For any finite-dimenisonal complex representations $V$ and $W$
	of $G$ we have
	\[ \left< \chi_V, \chi_W \right> = \dim \Homrep(W, V). \]
	In particular, if $V$ and $W$ are irreps then
	\[ \left< \chi_V, \chi_W \right> 
		=
		\begin{cases}
			1 & V  \cong W \\
			0 & \text{otherwise}.
		\end{cases}
	\]
\end{theorem}
\begin{corollary}[Irreps Give an Orthonormal Basis]
	The characters associated to irreps
	form an \emph{orthonormal} basis of $\FunCl(G)$.
\end{corollary}

In order to prove this theorem, we have to define
the dual representation and the tensor representation,
which give a natural way to deal with the quantity $\chi_V(g)\ol{\chi_W(g)}$.
\begin{definition}
	Let $V = (V, \rho)$ be a representation of $G$.
	The \vocab{dual representation} $V^\vee$ is the representation on $V^\vee$
	with the action of $G$ given as follows:
	for each $\xi \in V^\vee$, the image of the action $g \cdot \xi \in V^\vee$
	is specified by
	\[ v \xmapsto{g \cdot \xi} \xi\left( \rho(g\inv)(x) \right). \]
\end{definition}
\begin{definition}
	Let $V = (V, \rho_V)$ and $W = (W, \rho_W)$ be representations of $G$.
	The \vocab{tensor product} of $V$ and $W$ is the representation
	on $V \otimes W$ with the action of $G$ given on pure tensors by
	\[
		g \cdot (v \otimes w)
		= 
		(\rho_V(g)(v)) \otimes (\rho_W(g)(w)) \]
	which extends linearly to define the action of $G$ on all of $V \otimes W$.
\end{definition}

\begin{theorem}
	[Character Traces]
	If $V$ and $W$ are finite-dimensional representations of $G$,
	then for any $g \in G$.
	\begin{enumerate}[(a)]
		\ii $\chi_{V \oplus W}(g) = \chi_V(g) + \chi_W(g)$.
		\ii $\chi_{V \otimes W}(g) = \chi_V(g) \cdot \chi_W$.
		\ii $\chi_{V}(g) = \ol{\chi_V(g)}$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Parts (a) and (b) follow from the identities
	$\Tr(S \oplus T) = \Tr(S) + \Tr(T)$
	and $\Tr(S \otimes T) = \Tr(S) \Tr(T)$.
	However, part (c) is trickier.
	As $(\rho(g))^{|G|} = \rho(g^{|G|}) = \rho(1_G) = \id_V$
	by Lagrange's Theorem, we can diagonalize $\rho(g)$,
	say with eigenvalues $\lambda_1$, \dots, $\lambda_n$
	which are $|G|$th roots of unity,
	corresponding to eigenvectors $e_1$, \dots, $e_n$.
	Then we see that in the basis $e_1^\vee$, \dots, $e_n^\vee$,
	the action of $g$ on $V^\vee$ has eigenvalues
	$\lambda_1\inv$, $\lambda_2\inv$, \dots, $\lambda_n\inv$.
	So
	\begin{align*}
		\chi_V(g) &= \sum_{i=1}^n \lambda_i \\
		\chi_{V^\vee}(g) &= \sum_{i=1}^n \lambda_i\inv
		= \sum_{i=1}^n \ol{\lambda_i}
	\end{align*}
	where the last step follows from the identity $|z|=1 \iff z\inv = \ol z$.
\end{proof}
\begin{remark}
	[Warning]
	The identities (b) and (c) do not extend linearly to $\CC[G]$,
	i.e.\ it is not true for example that $\chi_V(a) = \ol{\chi_V(a)}$
	if we think of $\chi_V$ as a map $\CC[G] \to \CC$.
\end{remark}

With this, we can now prove the orthogonality relation.
\begin{proof}
	[Proof of Orthogonality Relation]
	The key point is that we can now reduce
	the sums of products to just a single character by
	\[ \chi_V(g) \ol{\chi_W(g)} = \chi_{V \otimes W^\vee} (g). \]
	So we can rewrite the sum in question as just
	\[
		\left< \chi_V, \chi_W \right>
		= \frac{1}{|G|} \sum_{g \in G} \chi_{V \otimes W^\vee} (g)
		= \chi_{V \otimes W^\vee}
		\left( \frac{1}{|G|} \sum_{g \in G} g \right).
	\]
	Let $P : V \otimes W^\vee \to V \otimes W^\vee$ be the
	action of $\frac{1}{|G|} \sum_{g \in G}$,
	so we wish to find $\Tr P$.
	\begin{exercise}
		Show that $P$ is idempotent.
		(Compute $P \circ P$ directly.)
	\end{exercise}
	Hence $V \otimes W^\vee = \ker P \oplus \img P$ (by \Cref{prob:idempotent})
	and $\img P$ is subspace of elements which are fixed under $G$.
	From this we deduce that
	\[ \Tr P = \dim \img P =
		\dim \left\{ x \in V \otimes W^\vee
		\mid g \cdot x = x \; \forall g \in G  \right\}.
		\]
	Now, consider the usual isomorphism $V \otimes W^\vee \to \Hom(W, V)$.
	\begin{exercise}
		Let $g \in G$.
		Show that under this isomorphism, $T \in \Hom(W, V)$
		satisfies $g \cdot T = T$ if and only if
		$T(g \cdot w) = g \cdot T(w)$ for each $w \in W$.
		(This is just unwinding three or four definitions.)
	\end{exercise}
	Consequently, $\chi_{V \otimes W^\vee}(P) = \Tr P = \dim \Homrep(W,V)$
	as desired.
\end{proof}


The orthogonality relation gives us a fast and mechanical way to check
whether a finite-dimensional representation $V$ is irreducible.
Namely, compute the traces $\chi_V(g)$ for each $g \in G$,
and then check whether $\left< \chi_V, \chi_V \right> = 1$.
So, for example, we could have seen the three representations of
$S_3$ that we found were irreps directly from the character table.

In other words, we can now efficiently verify any time we have
a complete set of irreps.
We give another example with $D_{10}$ again.
\begin{example}
	[Dihedral Group on $10$ Elements]
	Let $D_{10} = \left< r,s \mid r^5 = s^2 = 1, rs = sr\inv \right>$.
	Let $\omega = \exp(\frac{2\pi i}{5})$.
	We write the following four representations of $D_{10}$.
	\begin{itemize}
		\ii $\Ctriv$, all elements of $D_{10}$ act as the identity.
		\ii $\Csign$, all elements of $D_{10}$ act as the identity.
		\ii $V_1$, which is two-dimensional and given by
		$r \mapsto \begin{pmatrix} \omega & 0 \\ 0 & \omega^4 \end{pmatrix}$
		and $s \mapsto \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$.
		\ii $V_2$, which is two-dimensional and given by
		$r \mapsto \begin{pmatrix} \omega^2 & 0 \\ 0 & \omega^3 \end{pmatrix}$
		and $s \mapsto \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$.
	\end{itemize}
	We claim that these four representations are irreducible
	and pairwise non-isomorphic.
	We do so by writing the character table:
	\[
		\begin{array}{|c|rrcc|}
			\hline
			& \Ctriv & \Csign & V_1 & V_2 \\\hline
			1 & 1 & 1 & 2 & 2 \\
			r, r^4 & 1 & 1 & \omega + \omega^4 & \omega^2 + \omega^3 \\
			r^2, r^3 & 1 & 1 & \omega^2 + \omega^3 & \omega + \omega^4 \\
			sr^k & 1 & -1 & 0 & 0 \\ \hline
		\end{array}
	\]
	Then a direct computation shows the orthogonality relations,
	hence we indeed have an orthonormal basis.
	For example, $\left< \Ctriv, \Csign \right> = 1 + 2 \cdot 1 + 2 \cdot 1 + 5 \cdot (-1) = 0$.
\end{example}

\section\problemhead
\begin{sproblem}
	[Second Orthogonality Formula]
	\label{prob:second_orthog}
	Let $g$ and $h$ be elements of a finite group $G$,
	and let $V_1$, \dots, $V_r$ be the irreps of $G$.
	Prove that
	\[
		\sum_{i = 1}^r \chi_{V_i}(g) \ol{\chi_{V_i}(h)}
		=
		\begin{cases}
			|Z(g)| & \text{if $g$ and $h$ are conjugates} \\
			0 & \text{otherwise}.
		\end{cases}
	\]
	Here, $Z(g) = \left\{ x \in G : xg = gx \right\}$.
\end{sproblem}

\todo{Heisenberg group?}



\chapter{Some Applications}
With all this setup, we now take the time to develop some nice
results which are of independent interest.

\section{Frobenius Divisibility}
\begin{theorem}
	[Frobenius Divisibility]
	Let $V$ be a complex irrep of a finite group $G$.
	Then $\dim V$ divides $|G|$.
\end{theorem}
The proof of this will require algebraic integers
(developed in the algebraic number theory chapter).
Recall that an \emph{algebraic integer} is a complex number
which is the root of a polynomial with integer coefficients,
and that these algebraic integers form a ring $\ol\ZZ$
under addition and multiplication, and that $\ol\ZZ \cap \QQ = \ZZ$.

First, we prove the following lemma.
\newcommand{\tempuuujbhtkx}{$\ZZ[G]$}
\begin{lemma}[Elements of \tempuuujbhtkx\ are Integral]
	\label{lem:group_ring_integral}
	Let $\alpha \in \ZZ[G]$.
	Then there exists a monic polynomial $P$ with integer coefficients
	such that $P(\alpha) = 0$.
\end{lemma}
\begin{proof}
	Let $A_k$ be the $\ZZ$-span of $1, \alpha^1, \dots, \alpha^k$.
	Since $\ZZ[G]$ is Noetherian,
	the inclusions $A_0 \subseteq A_1 \subseteq A_2 \subseteq \dots$
	cannot all be strict, hence $A_k = A_{k+1}$ for some $k$,
	which means $\alpha^{k+1}$ can be expressed in terms of
	lower powers of $\alpha$.
\end{proof}

\begin{proof}
	[Proof of Frobenius Divisibility]
	Let $C_1$, \dots, $C_m$ denote the conjugacy classes of $G$.
	Then consider the rational number \[ \frac{|G|}{\dim V}; \]
	we will show it is an algebraic integer, which will prove the theorem.
	Observe that we can rewrite it as
	\[
		\frac{|G|}{\dim V}
		= \frac{|G| \left< \chi_V, \chi_V \right>}{\dim V}
		= \sum_{g \in G} \frac{\chi_V(g) \ol{\chi_V(g)}}{\dim V}.
	\]
	We split the sum over conjugacy classes, so
	\[
		\frac{|G|}{\dim V}
		=
		\sum_{i=1}^m \ol{\chi_V(C_i)} \cdot \frac{|C_i| \chi_V(C_i)}{\dim V}.
	\]
	We claim that for every $i$,
	\[ \frac{|C_i| \chi_V(C_i)}{\dim V}
		= \frac{1}{\dim V} \Tr T_i \]
	is an algebraic integer,
	where \[ T_i \defeq \rho\left(\sum_{h \in C_i} h\right). \]
	To see this, note that $T_i$ commutes with elements of $G$,
	and hence is an intertwining operator $T_i : V \to V$.
	Thus by Schur's Lemma, $T_i = \lambda_i \cdot \id_V$
	and $\Tr T = \lambda_i \dim V$.
	By \Cref{lem:group_ring_integral}, $\lambda_i \in \ol\ZZ$, as desired.
	
	Now we are done, since $\ol{\chi_V(C_i)} in \ol\ZZ$ too
	(it is the sum of conjugates of roots of unity),
	so $\frac{|G|}{\dim V}$ is the sum of products of algebraic integers,
	hence itself an algebraic integer.
\end{proof}

\section{Burnside's Theorem}
We now prove a group-theoretic result.
This is the famous poster child for representation theory
(in the same way that RSA is the poster child of number theory)
because the result is purely group theoretic.

Recall that a group is \emph{simple} if it has no normal subgroups.
In fact, we will prove the following theorem.
\begin{theorem}[Burnside]
	Let $G$ be a nonabelian group of order $p^a q^b$ (where $a,b \ge 0$).
	Then $G$ is not simple.
\end{theorem}
In what follows $p$ and $q$ will always denote prime numbers.

\begin{lemma}[On $\gcd(|C|, \dim V) = 1$]
	Let $V = (V, \rho)$ be an complex irrep of $G$.
	Assume $C$ is a conjugacy class of $G$ with $\gcd(|C|, \dim V) = 1$.
	Then for any $g \in C$, either
	\begin{itemize}
		\ii $\rho(g)$ is multiplication by a scalar, or
		\ii $\chi_V(g) = \Tr \rho(g) = 0$.
	\end{itemize}
\end{lemma}
\begin{proof}
	If $\eps_i$ are the $n$ eigenvalues of $\rho(g)$ (which are roots of unity),
	then from the proof of Frobenius divisibility we know
	$\frac{|C|}{n} \chi_V(g) \in \ol\ZZ$,
	thus from $\gcd(|C|, n) = 1$ we get 
	\[ \frac1n \chi_V(g) = \frac1n(\eps_1 + \dots + \eps_n) \in \ol\ZZ. \]
	So this follows readily from a fact from algebraic number theory,
	namely \Cref{prob:rep_lemma}:
	either $\eps_1 = \dots = \eps_n$ (first case) or
	$\eps_1 + \dots + \eps_n = 0$ (second case).
\end{proof}

\begin{lemma}
	[Simple Groups Don't Have Prime Power Conjuagcy Classes]
	Let $G$ be a finite simple group.
	Then $G$ cannot have a conjugacy class of order $p^k$ (where $k > 0$).
\end{lemma}
\begin{proof}
	By contradiction.
	Assume $C$ is such a conjugacy class, and fix any $g \in C$.
	By the second orthogonality formula (\Cref{prob:second_orthog})
	applied $g$ and $1_G$ (which are not conjugate since $g \neq 1_G$) we have
	\[ \sum_{i=1}^r \dim V_i \chi_{V_i}(g) = 0 \]
	where $V_i$ are as usual all irreps of $G$.
	\begin{exercise}
		Show that there exists a nontrivial irrep $V$
		such that $p \nmid \dim V$ and $\chi_V(g) \neq 0$.
		(Proceed by contradiction to show that $-\frac1p \in \ol\ZZ$ if not.)
	\end{exercise}
	Let $V = (V, \rho)$ be the irrep mentioned.
	By the previous lemma, we now know that $\rho(g)$ acts as a scalar in $V$.

	Now consider the subgroup
	\[ H = \left< ab\inv \mid a,b \in C \right> \subset G. \]
	We claim this is a nontrivial normal subgroup of $G$.
	It is easy to check $H$ is normal,
	and since $|C| > 1$ we have that $H$ is nontrivial.
	Each element of $H$ acts trivially in $G$,
	so since $V$ is nontrivial and irreducible $H \neq G$.
	This contradicts the assumption that $G$ was simple.
\end{proof}

With this lemma, Burnside's Theorem follows by partitioning
the $|G|$ elements of our group into conjugacy classes.
Assume for contradiction $G$ is simple.
Each conjugacy class must have order either $1$ (of which there are $|Z(G)|$)
or divisible by $pq$, but on the other hand the sum equals $|G| = p^aq^b$.
Consequently, we must have $|Z(G)| > 1$.
But $G$ is not abelian, hence $Z(G) \neq G$,
thus the center $Z(G)$ is a nontrivial normal subgroup,
contradicting the assumption that $G$ was simple.



\section{Frobenius Determinant}

\end{document}
TODO: "finite-dimensional irreducible representation" $\to$ irrep
